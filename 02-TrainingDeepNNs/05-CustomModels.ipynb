{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faced-manchester",
   "metadata": {},
   "source": [
    "# Custom Models and Training with TensorFlow\n",
    "\n",
    "Up until now we've used only TensorFlow high-level API, `tf.keras`. In most cases you will only use `tf.keras`. But when you want want extra control over your model, cost function, optimization, etc then you have to know about TensorFlow's low-level API.\n",
    "\n",
    "# A Quick tour of TensorFlow\n",
    "\n",
    "TensorFlow is the most popular Machine Learning framework.\n",
    "\n",
    "- Its core is similar to NumPy but with GPU.\n",
    "- It supports distributed computing.\n",
    "- It uses computational graphs which can be exported in any format. (with this you can use it with python or javascript)\n",
    "\n",
    "TensorFlow offers many features such as preprocessing, model building, visualization, deployment, mathematics, etc.\n",
    "\n",
    "TensorFlow's Python API overview:\n",
    "\n",
    "```python\n",
    "# High-level\n",
    "tf.keras\n",
    "tf.estimators\n",
    "\n",
    "# Low-level\n",
    "tf.nn\n",
    "tf.losses\n",
    "tf.metrices\n",
    "tf.optimizers\n",
    "tf.train\n",
    "tf.initializers\n",
    "\n",
    "# Autodiff\n",
    "tf.GradientTape\n",
    "tf.gradients()\n",
    "\n",
    "# I/O and prerpcessing\n",
    "tf.data\n",
    "tf.image\n",
    "tf.audio\n",
    "tf.feature_column\n",
    "tf.io\n",
    "tf.queue\n",
    "```\n",
    "\n",
    "# Using TensorFlow like NumPy\n",
    "\n",
    "TensorFlow uses *tensors*, which flow from operation to operation hence the name TensorFlow. A tensor is basically a multi-dimensional array. \n",
    "\n",
    "## Tensors and Operations\n",
    "\n",
    "Creating tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "offshore-range",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pacific-cartridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-sector",
   "metadata": {},
   "source": [
    "Shape and type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "increased-thousand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infectious-compiler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-metro",
   "metadata": {},
   "source": [
    "Slicing like NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pursuant-officer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "numerical-retro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-chester",
   "metadata": {},
   "source": [
    "Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "periodic-unemployment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "racial-revelation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "finished-identifier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-lancaster",
   "metadata": {},
   "source": [
    "## Tensors and NumPy\n",
    "\n",
    "Tensors are friendly to NumPy arrays, they can be converted into each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "induced-shooting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([2., 4., 6.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "meaningful-frederick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-hello",
   "metadata": {},
   "source": [
    "You can even apply TensorFlow operations to NumPy array and vice-versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "elect-lover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 36.])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "trained-publicity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-lesson",
   "metadata": {},
   "source": [
    "> Type conversion is sensitive in TensorFlow so be careful when using different types.\n",
    "\n",
    "## Variables\n",
    "\n",
    "The `tf.Tensor` are immutable, you can't modify them. So, for weights we need `tf.Variable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "political-platform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-ridge",
   "metadata": {},
   "source": [
    "They are pretty much same as `tf.Tensor`. Now, you can modify them using `assign()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "developed-inquiry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[ 8., 84.,  0.],\n",
      "       [32., 40.,  2.]], dtype=float32)>\n",
      "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[ 8., 42.,  0.],\n",
      "       [32., 40.,  2.]], dtype=float32)>\n",
      "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[ 8., 42.,  0.],\n",
      "       [32., 40.,  1.]], dtype=float32)>\n",
      "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[100.,  42.,   0.],\n",
      "       [ 32.,  40., 200.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(v.assign(2 * v))\n",
    "print(v[0, 1].assign(42))\n",
    "print(v[:, 2].assign([0., 1.]))\n",
    "print(v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-dictionary",
   "metadata": {},
   "source": [
    "# Customizing Models and Training Algorithms\n",
    "\n",
    "Let's start by customizing loss function.\n",
    "\n",
    "## Custom Loss Functions\n",
    "\n",
    "Let's make a huber cost function (which is a combination of both MSE and MAE):\n",
    "\n",
    "```python\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "```\n",
    "\n",
    "Now you can use this when you compile the Keras model:\n",
    "\n",
    "```python\n",
    "model.compile(loss=huber_fn, optimizer='nadam')\n",
    "model.fit(X_train, y_train, [...])\n",
    "```\n",
    "\n",
    "## Saving and Loading Models That contain Custom Components\n",
    "\n",
    "Saving a model containing custom components is easy, as Keras saves the name of functions. Morever, when you load it you need to pass custom functions as a dictionary with name:\n",
    "\n",
    "```python\n",
    "model = keras.models.load_model('my_model.h5',\n",
    "                               custom_objects={'huber_fn': huber_fn})\n",
    "```\n",
    "\n",
    "In this any error between -1 and 1 is considered to be small. But, if you want a different threshold and pass it to `huber_fn()`, then Keras will not save the threshold value. This can be solved by subclassing `keras.losses.Loss` class and by accessing `get_config()` function:\n",
    "\n",
    "```python\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__()\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return (**base_config, 'threshold': self.threshold)\n",
    "```\n",
    "\n",
    "Now, you can use it during compiling with threshold. Now Keras saves it with configs, so when loading you don't need to give threshold value.\n",
    "\n",
    "## Custom Activation Functions, Initializers, Regularizers, and Constraints\n",
    "\n",
    "Most of them are customized in a same way as we did with loss function:\n",
    "\n",
    "```python\n",
    "def my_softplus(z): # Activation function\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_init(shape, dtype=tf.float32): # Initializer\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights): # Regularizer\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # Constraint\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "```\n",
    "\n",
    "Then apply them when defining the layers:\n",
    "\n",
    "```python\n",
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                          kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=my_l1_regularizer,\n",
    "                          kernel_constraint=my_positive_weights)\n",
    "```\n",
    "\n",
    "If you want to add a hyperparameter and want to save it, then use subclassing of appropriate class, example:\n",
    "\n",
    "```python\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        super().__init__()\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {'factor': self.factor}\n",
    "```\n",
    "\n",
    "Make sure you implement `call()` method for losses, activation functions, layers, and models, or the `__call__()` method for regularizers, initializers, and constraints.\n",
    "\n",
    "## Custom Metrics\n",
    "\n",
    "Losses and Metrics are not same thing: losses are used to train the model and might not be human understandable on the other hand Metrics are used to show losses and human understandable.\n",
    "\n",
    "We can customize Metrics like we did as loss function:\n",
    "\n",
    "```python\n",
    "model.compile(loss='mse', optimizer='nadam', metrics=[create_huber(2.0)])\n",
    "```\n",
    "\n",
    "This computes mean of loss after every epoch. But its not we always want like in calculating precision, here we use `keras.metrics.Precision()`. There is a technique called *streaming metric*, which takes batch-wise precision with keeping track of previous precisions. For creating this you need to subclass `keras.metrics.Metric` class:\n",
    "\n",
    "```python\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold': self.threshold}\n",
    "```\n",
    "\n",
    "This way it keeps track of total loss and count of instances seen.\n",
    "\n",
    "## Custom Layers\n",
    "\n",
    "Sometime you want a different layer than the Keras ones or you may simply create block of layers. So let's build custom layers.\n",
    "\n",
    "First, if you want layers that have no weights like `keras.layers.Flatten` you can simply wrap it up in `keras.layers.Lambda`:\n",
    "\n",
    "```python\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "```\n",
    "\n",
    "For creating custom stateful layer use subclassing of `keras.layers.Layer`, example a simplified version of `keras.layers.Dense`:\n",
    "\n",
    "```python\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "        \tname='kernel', shape=[batch_input_shape[-1], self.units],\n",
    "            initializer='glorot_normal')\n",
    "        self.bias = self.add_weight(\n",
    "        \tname='bias', shape=[self.units], initializer='zeros')\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'units': self.units, \n",
    "               'activation': keras.activations.serialize(self.activation)}\n",
    "```\n",
    "\n",
    "To create a layer with multiple inputs:\n",
    "\n",
    "```python\n",
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1]\n",
    "```\n",
    "\n",
    "> You can omit `compute_output_shape()` method as Keras automatically computes output shape, except when the layer is dynamic. \n",
    "\n",
    "When your layer's behavior is different during training and testing, then you should pass `training` argument to the `call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-stream",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
