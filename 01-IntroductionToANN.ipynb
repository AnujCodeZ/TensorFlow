{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suitable-japan",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Network with Keras\n",
    "\n",
    "## Introduction\n",
    "\n",
    "*Artificial Neural Networks* (ANNs) is a Machine Learning Model inspired by the networks of biological neurons found in our brains.\n",
    "\n",
    "ANNs are the very core of Deep Learning. They are versatile, powerful, and scalable. They are able to tackle complex problems like classifying large number of images (Google Images), speech recognition services (\"OK Google\"), recommending movies (Netflix), etc.\n",
    "\n",
    "We'll use popular Keras API: this is a beautifully designed and simple high-level API for building, training, evaluating, and running neural networks. \n",
    "\n",
    "## History of ANNs\n",
    "\n",
    "ANNs first introduced back in 1943 by Neurophysiologist Warren McCulloch and the mathematician Walter Pitts in their paper \"A Logical Calculus of Ideas Immanent in Nervous Activity\".\n",
    "\n",
    "In 1960, idea of making intelligent machines seems impossible (for quite a while), ANNs entered a long winter.\n",
    "\n",
    "In early 1980, new architectures were build, interest rises in *connectionism* (the study of neural networks), progress was slow. And around 1990, more powerful Machine Learning Algorithms was built like SVMs. So they put on hold again.\n",
    "\n",
    "But now we see that ANNs are rising and this time they keep rising. Here are few reasons why:\n",
    "\n",
    "- Huge quantity of data available.\n",
    "- Increase in Computing powers, and thanks to Gaming industry to give us GPU.\n",
    "- Training algorithms have been improved.\n",
    "- More funding in building amazing products and research.\n",
    "\n",
    "## Logical Computations with Neurons\n",
    "\n",
    "McCulloch and Pitts proposed a very simple model of the biological neuron, which later became known as *artificial neuron*: it has one or more binary inputs and one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active. \n",
    "\n",
    "You can compute complex logical expressions by changing the inputs or combining them.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "The *Perceptron* is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on slightly different artificial neuron called a *threshold logic unit* (TLU), or sometimes a *linear threshold unit* (LTU).\n",
    "\n",
    "The inputs and output are numbers, and each connection holds some value called *weights*. The TLU computes a weighted sum of its inputs:\n",
    "$$\n",
    "z = w_1x_1 + w_2x_2 + ... + w_nx_n = X^TW\n",
    "$$\n",
    " Then applies a *step function* and results an output:\n",
    "$$\n",
    "h_w(X) = step(z), where\\ z = X^TW\n",
    "$$\n",
    "The most common step function used in Perceptron is the *Heaviside step function*:\n",
    "$$\n",
    "heaviside(z) = \\{{0, if\\ z < 0 \\\\ 1, if z >= 0}\n",
    "$$\n",
    "Sometimes a sign function:\n",
    "$$\n",
    "sign(z) = \\{{-1, if\\ z < 0 \\\\ 0, if\\ z = 0 \\\\ 1, if\\ z > 0}\n",
    "$$\n",
    "A single TLU can be use for simple linear binary classification. Training a TLU means finding the right values of weights.\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs.\n",
    "\n",
    "When all the neurons in a layer are connected to every neuron in previous layer, the layer is called *fully connected layer* or *dense layer*.\n",
    "\n",
    "The inputs of Perceptron are simply passthrough neurons called *input neurons*: they output whatever they are fed. All the input neurons form the *input layer*. Moreover, an extra bias feature is added: represented as a *bias neuron*, always outputs 1. \n",
    "\n",
    "Computing the outputs of a fully connected layer:\n",
    "$$\n",
    "h_{W, b} = \\phi(XW + b)\n",
    "$$\n",
    " In this equation,\n",
    "\n",
    "- **X**: inputs matrix\n",
    "- **W**: weights matrix\n",
    "- **b**: bias\n",
    "- $\\phi$: it represents *activation function*\n",
    "\n",
    "The training of Perceptron was largely inspired by *Hebb's rule*: which is summarized as \"cells that fire together, wire together\"; that is connection weights between two neurons tends to increase when they activates together.\n",
    "\n",
    "The Perceptron trains with slight different variant that takes into account of error made by the network. It reinforces the connection when it makes an error. The rule is:\n",
    "$$\n",
    "w_{i,j}^{(next\\ step)} = w_{i, j} + \\eta(y_j - \\hat y_j)x_i\n",
    "$$\n",
    "In this equation:\n",
    "\n",
    "- $w_{i,j}$ is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron.\n",
    "- $x_i$ is the $i^{th}$ input value of current training instance.\n",
    "- $y_j$ is the target output of the $j^{th}$ output neuron.\n",
    "- $\\hat y_j$ is the output of the $j^{th}$ output neuron.\n",
    "- $\\eta$ is the learning rate.\n",
    "\n",
    "The decision boundary of the TLUs are linear, so Perceptron are incapable of learning complex patterns. Overall this algorithm is called *Perceptron convergence theorem*.\n",
    "\n",
    "You can use Sklearn's `Perceptron` to do this. However, it is same as the SGD classifier of Sklearn with hyperparameters `loss=\"perceptron\", learning rate=\"constant\"`.\n",
    "\n",
    "Perceptrons do not output a class probability as Logistic regression. Rather, they output a hard threshold.\n",
    "\n",
    "In 1969 monograph *Perceptrons*, there are weakness of Perceptrons like it cannot solve XOR problem. \n",
    "\n",
    "It turns out limitations of Perceptron can be replaced by stacking multiple Perceptrons. The resulting ANN is called *Multilayer Perceptron* (MLP). An MLP can solve XOR problem easily.\n",
    "\n",
    "\n",
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "canadian-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ignored-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length and width\n",
    "y = (iris.target == 0).astype(np.int) # Iris setosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fifth-lease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "delayed-chase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-european",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron and Backpropagation\n",
    "\n",
    "A MLP is composed of one (passthrough) *input layer*, one or more layers of TLUs, called *hidden layers*, and one final layer of TLUs called the *output layer*. \n",
    "\n",
    "The layers close to input layer are usually called *lower layers*, and one close to output layer called *upper layers*.\n",
    "\n",
    "Every layer except output layer includes the bias neuron and is fully connected to the next layer.\n",
    "\n",
    "> The signal flows only in one direction (from the inputs to the outputs), so this architecture called *feedforward neural network* (FNN).\n",
    "\n",
    "When an ANN contains deep stack of hidden layers, it is called *deep neural networks* (DNN). \n",
    "\n",
    "In 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a paper that introduces the *backpropagation*. Which is ground breaking and still in use. In short it is Gradient Descent using an efficient way to calculate gradients automatically.\n",
    "\n",
    "> Automatically computing gradients is called *automatic differentiation*, or *autodiff*. The autodiff technique used in backpropagation is called *reverse-mode autodiff*. It is fast and precise, and well suited for when function to differentiate has many variables (e.g. connection weights) and few outputs (e.g. loss).\n",
    "\n",
    "Backpropagation Algorithm:\n",
    "\n",
    "- It handles one mini-batch at a time, and it goes through full training set multiple times. Each pass though full training set is called an *epoch*.\n",
    "- Each mini-batch is passed through the network from input layer through hidden layers to the output layer, this is called *forward pass*. The results for every layer is preserved in order to compute gradients.\n",
    "- Next, algorithm measures the network's output error (by loss function).\n",
    "- Then it computes how much each output connection contributed to the error analytically by applying chain rule.\n",
    "- Then it measures how much of these error contributions came from each connection in the layer below, again using chain rule, working backward until the algo reaches to the input layer. This is called *backward pass*.\n",
    "- Finally, the algorithm performs Gradient Descent step to tweak all the connection weights in the network, using error gradients it just computed.\n",
    "\n",
    "> It is important to initialize all the hidden layers' connection weights randomly, or else training will fall. For example, if you initialize all weights and bias to zero, then all neurons in a given layer will be perfectly identical, and backpropagation will affect them in a same way, so they will remain identical. Network treat a layer as only one neuron. If you initialize the weights randomly, you *break the symmetry* and allow backpropagation to train a diverse team of neurons.\n",
    "\n",
    "In order to work this algorithm properly, there is a key change in MLP: they replaced the step function (non-differentiable) to the logistic (sigmoid) function,\n",
    "$$\n",
    "\\sigma(z) = 1 / (1 + \\exp(-z))\n",
    "$$\n",
    "Sigmoid is differentiable and ranges from 0 to 1. There are others we can use:\n",
    "\n",
    "- *Hyperbolic tangent function*: $\\tanh(z) = 2\\sigma(2z)-1$\n",
    "  - It is like sigmoid, but ranges from -1 to 1. Which make layer's output more centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "- *The Rectified Linear Unit function*: $ReLU(z) = \\max(0, z)$\n",
    "  - It is continuous but not differentiable at 0. But in practice it works well and computed very fast, which makes it default for today's architectures. It's derivative is 0 for z < 0 and has no upper limit which makes it more usable.\n",
    "\n",
    "We need activation functions because if we chain linear transformations we get a linear transformation. So if you don't have non-linearity between layers, then deep networks is equivalent to single layer network.\n",
    "\n",
    "## Regression MLPs\n",
    "\n",
    "We can use MLPs for regression tasks. To predict single values, you just need an output neuron, and for multivariate regression, you need one output neuron for every dimension (value).\n",
    "\n",
    "In general, you don't need any activation function for the output layer. If you want positive output then you can use ReLU or *softplus function*, which is smooth variant of ReLU: $softplus(z) = \\log(1 + \\exp(z))$.\n",
    "\n",
    "Finally, if you want output in a specific range, you could use sigmoid or tanh and scale it up to your specific range.\n",
    "\n",
    "The loss function is generally mean squared error. But if you have lots of outliers you could use mean absolute error. Alternatively, you can use the *Huber loss*, which is combination of both.\n",
    "\n",
    "> Huber loss is quadratic when error is smaller than a threshold $\\delta$ (typically 1) but linear when greater.\n",
    "\n",
    "## Classification MLPs\n",
    "\n",
    "They mostly used in classification tasks. For a binary classification, you can use an output neuron with logistic function. The output will be in range 0 and 1, also gives you probability of 1 (positive class).\n",
    "\n",
    "MLPs can also easily handle multilabel binary classification. You can use two neurons with logistic function if you have 2 labels. The probability of both does not add up to 1, means label are independent.\n",
    "\n",
    "If your labels are dependent (e.g. MNIST), then you use output neuron for each label and apply *softmax function* to whole output layer, which produce a probabilities which add up to 1. This is called multiclass classification.\n",
    "\n",
    "For loss, since we are pretending the output is probability distribution, the *cross-entropy loss* (also called the log loss) is generally a good choice.\n",
    "\n",
    "## Implementing MLPs with Keras\n",
    "We'll be using Keras, which is a Deep Learning API that allows you to easily build, train, evaluate, and execute all types of neural networks. It was developed Francois Chollet as part of a research project, and it made open source in March 2015.\n",
    "\n",
    "Tensorflow is a framework that multi functionality to perform Deep Learning, Tensorflow 2 has made Keras as its official API.\n",
    "### Buildind an Image Classifier using the Sequential API\n",
    "Install tensorflow and check the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "centered-console",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elder-moldova",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-geometry",
   "metadata": {},
   "source": [
    "#### Using Keras to load the dataset\n",
    "Keras provides some utility functions to fetch and load common datasets. Let's use Fashion MNIST, it is similar to MNIST but contain cloth images instead of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "official-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-style",
   "metadata": {},
   "source": [
    "Keras' Fashion MNIST gives 28x28 array for each image, type as intergers, and ranges from 0 to 255 as pixel intensities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "undefined-belly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intellectual-snake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-lesson",
   "metadata": {},
   "source": [
    "It already splits dataset into train and test, but not validation set. We'll use Gradient Descent so we must scale input features by dividing them by 255.0. Now, numbers are in range 0-1 and become float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "western-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-fireplace",
   "metadata": {},
   "source": [
    "For Fashion MNIST, we need the list of class names to know the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "maritime-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "              \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "behavioral-artwork",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-theater",
   "metadata": {},
   "source": [
    "#### Creating the Model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "herbal-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-memorial",
   "metadata": {},
   "source": [
    "Let's go though code:\n",
    "\n",
    "- `Sequential` model is the simplest Keras model to compose a single stack of layers connected sequentially.\n",
    "- Next, the first layer added to the model. It is a `Flatten` layer, whose role is to convert input shape as 1D array. It does not accept any parameters, since it is first layer you need to pass `input_shape`. Alternatively, you could add `keras.layers.InputLayer` with passing `input_shape=[28, 28]`.\n",
    "- Next, we add a `Dense` layer with 300 neuron number passing to it. It has an activation function ReLU. It manages all weights and bias when recieves inputs.\n",
    "\n",
    "- Then we add another `Dense` layer with 100 neurons and activation as ReLU.\n",
    "- Finally, we add a `Dense` output layer with 10 neurons (one per class), using the softmax activation function.\n",
    "\n",
    "> Specifying `activation='relu'` is equivalent to `activation=keras.activations.relu`.\n",
    "\n",
    "Instead of adding layers one by one, you could pass a list of layers when creating th `Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "funky-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-covering",
   "metadata": {},
   "source": [
    "The model's `summary()` method displays all the model's layers, including their name (which automatically created if not mentioned), output shape and number of parameters with total parameters, trainable and non-trainable parameter numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "organizational-gasoline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-emission",
   "metadata": {},
   "source": [
    "However, you can use `keras.utils.plot_model()` to generate an image of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "starting-dimension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-treasure",
   "metadata": {},
   "source": [
    "Note that, `Dense` layer has lot of parameters. This gives the model a lot of flexibility to fit the data, but it also means that model could overfit the data. Specially when you don't have lots of data.\n",
    "\n",
    "You can easily get a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "soviet-individual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x7ff1406443a0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7ff13fd6e730>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7ff1242158e0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7ff124215cd0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "focused-wound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "knowing-ethnic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-lemon",
   "metadata": {},
   "source": [
    "All the parameters can be accessed by `get_weights()` and `set_weights()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tender-permit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.0050991 ,  0.01367374,  0.06867222, ..., -0.00842841,\n",
       "          0.04613265,  0.04490034],\n",
       "        [ 0.00351208, -0.05015696, -0.04875442, ...,  0.00047144,\n",
       "         -0.0282766 , -0.02230942],\n",
       "        [-0.07322014, -0.01896045, -0.07388081, ...,  0.00279492,\n",
       "         -0.05858865, -0.03133649],\n",
       "        ...,\n",
       "        [-0.03691772,  0.0655247 ,  0.01245636, ...,  0.01111966,\n",
       "         -0.01662078, -0.03823334],\n",
       "        [-0.0726272 , -0.03864296, -0.06224412, ..., -0.06387234,\n",
       "          0.02304252, -0.0673283 ],\n",
       "        [-0.04849637, -0.06575351,  0.04961916, ..., -0.01769502,\n",
       "          0.06493831,  0.02017646]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias = hidden1.get_weights()\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "embedded-retailer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 300), (300,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-stockholm",
   "metadata": {},
   "source": [
    "`Dense` layer initialize the connection weights randomly, and biases to zeros. If you want to change initialization, you can set `kernel_initializer` (kernel is another name of matrix of connection weights) or `bias_initializer` when creating the layer.\n",
    "\n",
    "> The shape of the weigth matrix depends on the input shape. That's why we need to specify input shape at start. But if you don't that's okay Keras will wait until you pass it the data or you can call its `build()` mehod. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-incidence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
