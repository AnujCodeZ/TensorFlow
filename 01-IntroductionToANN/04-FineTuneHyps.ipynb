{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "saving-notification",
   "metadata": {},
   "source": [
    "# Fine-Tuning Neural Network Hyperparameters\n",
    "\n",
    "There are many hyperparameters to tune in neural networks like number of layers, number of units in each layer, activation function in each layer, etc.\n",
    "\n",
    "One way to find a combination is to use Sklearn's `GridSearchCV` or `RandomizedSearchCV` to explore the hyperparameter space.\n",
    "\n",
    "So we need to wrap Keras model to Sklearn model.So let's create a function that build and compile a model, given a set of hyperparameters. And then convert this model using `KerasRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parallel-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sized-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "norman-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-surge",
   "metadata": {},
   "source": [
    "Now we can use it like Sklearn regressor also it works as Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "extended-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "antique-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fancy-discretion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.9551 - val_loss: 0.7994\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7938 - val_loss: 0.6934\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6655 - val_loss: 0.6336\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6336 - val_loss: 0.5937\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5824 - val_loss: 0.5666\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5831 - val_loss: 0.5708\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5299\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5180 - val_loss: 0.5185\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5104 - val_loss: 0.5069\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4914 - val_loss: 0.4950\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4925 - val_loss: 0.4879\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4834 - val_loss: 0.4820\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4873 - val_loss: 0.4757\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4701 - val_loss: 0.4707\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4698 - val_loss: 0.4653\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4560 - val_loss: 0.4606\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4516 - val_loss: 0.4582\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4500 - val_loss: 0.4567\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4455 - val_loss: 0.4517\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4438 - val_loss: 0.4486\n",
      "162/162 [==============================] - 0s 780us/step - loss: 0.4297\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=20,\n",
    "             validation_data=(X_val, y_val))\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-opening",
   "metadata": {},
   "source": [
    "Note that any extra parameter you pass to `fit()` method will get passed to the underlying Keras model. \n",
    "\n",
    "Now, we want to train it on many variants to see which combination of hyperparameters works best for validation set. We use randomized search because there are many hyperparameters:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    'n_hidden': [0, 1, 2, 3],\n",
    "    'n_neurons': np.arange(1, 100),\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RanomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Note that `RandomizedSearchCV` use K-fold cross-validation, so it does not use `X_val` and `y_val`. Since, Sklearn regression `score()` is opposite to the Keras `evaluate()`.\n",
    "\n",
    "This training will take several hours, depending on the hardware, size of dataset, complexity of model, and values of `n_iter` and `cv`. After this done you can check best hyperparameters by `rnd_search_cv.best_params_`, score by `rnd_search_cv.best_score_`, and model by `rnd_search_cv.best_estimator_.model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-powell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
