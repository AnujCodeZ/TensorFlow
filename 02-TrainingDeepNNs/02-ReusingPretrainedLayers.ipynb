{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spatial-tattoo",
   "metadata": {},
   "source": [
    "# Reusing Pretrained Layers\n",
    "\n",
    "It is not good to train a DNN from scratch, instead use lower layers pretrained architectures from a similar task. This technique is called *transfer learning*. It requires less data and less time to train.\n",
    "\n",
    "The input shape must be same as original model input shape, if yours is different then you can add preprocessing step to resize inputs.\n",
    "\n",
    "The output layer is replaced as it is most likely not be useful. Similarly, the upper hidden layers are less likely to be useful, since the high level features are most important for the new task. You need to find right number of layers to reuse. \n",
    "\n",
    "> The more similar the tasks are, the more layers from original model you should reuse.\n",
    "\n",
    "First, you start with freezing (make non-trainable) changing output layer. Then you can start unfreezing upper layers and see model's performance. The more training data you have, more layers you can unfreeze. It is also useful to reduce learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.\n",
    "\n",
    "You can add your layers or replace top layers if you have enough training data.\n",
    "\n",
    "## Transfer Learning with Keras\n",
    "\n",
    "Let's say we use a pretrained model A that is trained on image classification on 10 classes and we want a binary image classification model B.\n",
    "\n",
    "First, you need to load original saved model and create a new model by changing output layer:\n",
    "\n",
    "```python\n",
    "model_A = keras.models.load_model('my_model_A.h5')\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "Now when you train model_B_on_A, it will also affect model_A. if you want to avoid that you have to clone model_A and load pretrained weights since `clone_model` only clones architecture:\n",
    "\n",
    "```python\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "```\n",
    "\n",
    "Now for freezing layers:\n",
    "\n",
    "```python\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "```\n",
    "\n",
    "> You must always compile your model after you freeze or unfreeze layers.\n",
    "\n",
    "One fact, if you want to use all layers with pretrained weights, the output layer's weights are randomly initialized. Hence, at start the error is pretty huge which may wreck pretrained weights. So for few epochs you must freeze pretrained layers, then unfreeze all.\n",
    "\n",
    "In fact transfer learning does not work well on small dense networks because they learn few patterns. It will be useful in CNNs where they learn the features of images in lower hidden layers.\n",
    "\n",
    "## Unsupervised Pretraining\n",
    "\n",
    "In case you don't have much labeled training data, and no similar task was previously trained. In this case you can use *unsupervised pretraining*. As it is easy to gather unlabeled training data, you can collect them and then train an unsupervised model like autoencoder or GAN. Then you could use their lower hidden layers and then add your layers then train them on labeled training data.\n",
    "\n",
    "## Pretraining on an Auxiliary Task\n",
    "\n",
    " One other option if you don't have much labeled data is to train a neural network on auxiliary task for which you can easily get labeled data. Then use lower layer of that network and train on your main task.\n",
    "\n",
    "For example: If you want to recognize faces, but you have only few faces of an individual. One thing you can do is collect images of faces from the web and train first neural network that tells whether two faces are of same person or different. Then use lower layers of this network to classify your faces data.\n",
    "\n",
    "> *Self-supervised learning* is when you automatically generate the labels from the data itself, then you train a model on the resulting \"labeled\" dataset using supervised learning. Since no human is required to do labeling, it is classified as unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-gospel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
