{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sixth-incident",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras\n",
    "We'll be using Keras, which is a Deep Learning API that allows you to easily build, train, evaluate, and execute all types of neural networks. It was developed Francois Chollet as part of a research project, and it made open source in March 2015.\n",
    "\n",
    "Tensorflow is a framework that multi functionality to perform Deep Learning, Tensorflow 2 has made Keras as its official API.\n",
    "## Buildind an Image Classifier using the Sequential API\n",
    "Install tensorflow and check the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "biological-travel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "encouraging-sword",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-deployment",
   "metadata": {},
   "source": [
    "### Using Keras to load the dataset\n",
    "Keras provides some utility functions to fetch and load common datasets. Let's use Fashion MNIST, it is similar to MNIST but contain cloth images instead of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "detected-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-blogger",
   "metadata": {},
   "source": [
    "Keras' Fashion MNIST gives 28x28 array for each image, type as integers, and ranges from 0 to 255 as pixel intensities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incident-woman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incorrect-hacker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-empire",
   "metadata": {},
   "source": [
    "It already splits dataset into train and test, but not validation set. We'll use Gradient Descent so we must scale input features by dividing them by 255.0. Now, numbers are in range 0-1 and become float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bigger-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-smooth",
   "metadata": {},
   "source": [
    "For Fashion MNIST, we need the list of class names to know the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intimate-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "              \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "preliminary-superintendent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-sigma",
   "metadata": {},
   "source": [
    "### Creating the Model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "isolated-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-european",
   "metadata": {},
   "source": [
    "Let's go though code:\n",
    "\n",
    "- `Sequential` model is the simplest Keras model to compose a single stack of layers connected sequentially.\n",
    "- Next, the first layer added to the model. It is a `Flatten` layer, whose role is to convert input shape as 1D array. It does not accept any parameters, since it is first layer you need to pass `input_shape`. Alternatively, you could add `keras.layers.InputLayer` with passing `input_shape=[28, 28]`.\n",
    "- Next, we add a `Dense` layer with 300 neuron number passing to it. It has an activation function ReLU. It manages all weights and bias when receives inputs.\n",
    "\n",
    "- Then we add another `Dense` layer with 100 neurons and activation as ReLU.\n",
    "- Finally, we add a `Dense` output layer with 10 neurons (one per class), using the softmax activation function.\n",
    "\n",
    "> Specifying `activation='relu'` is equivalent to `activation=keras.activations.relu`.\n",
    "\n",
    "Instead of adding layers one by one, you could pass a list of layers when creating th `Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "headed-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-disaster",
   "metadata": {},
   "source": [
    "The model's `summary()` method displays all the model's layers, including their name (which automatically created if not mentioned), output shape and number of parameters with total parameters, trainable and non-trainable parameter numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fixed-rhythm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-sculpture",
   "metadata": {},
   "source": [
    "> However, you can use `keras.utils.plot_model()` to generate an image of your model.\n",
    "\n",
    "Note that, `Dense` layer has lot of parameters. This gives the model a lot of flexibility to fit the data, but it also means that model could overfit the data. Specially when you don't have lots of data.\n",
    "\n",
    "You can easily get a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "intensive-grass",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x7f414d5b5c40>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f414d5b5310>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f41302d28b0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f41302d89d0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "general-cabinet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "assigned-inspection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-recipe",
   "metadata": {},
   "source": [
    "All the parameters can be accessed by `get_weights()` and `set_weights()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "informative-salad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00206982,  0.03493267,  0.0091569 , ...,  0.06525832,\n",
       "         -0.05459186,  0.05065851],\n",
       "        [-0.04126098,  0.0593053 ,  0.00368364, ...,  0.013064  ,\n",
       "          0.05187111,  0.00774974],\n",
       "        [ 0.02220994,  0.00214276,  0.02497701, ..., -0.05574257,\n",
       "         -0.01936175, -0.0181095 ],\n",
       "        ...,\n",
       "        [ 0.05952045,  0.05416371, -0.07029738, ...,  0.00786551,\n",
       "          0.01186923, -0.06122747],\n",
       "        [-0.04533949,  0.04343931,  0.02093161, ..., -0.04334065,\n",
       "         -0.0294878 , -0.01689502],\n",
       "        [-0.05332336, -0.05843108,  0.02125969, ..., -0.00840024,\n",
       "          0.03568808,  0.00501072]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias = hidden1.get_weights()\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "obvious-induction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 300), (300,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-kennedy",
   "metadata": {},
   "source": [
    "`Dense` layer initialize the connection weights randomly, and biases to zeros. If you want to change initialization, you can set `kernel_initializer` (kernel is another name of matrix of connection weights) or `bias_initializer` when creating the layer.\n",
    "\n",
    "> The shape of the weigthts matrix depends on the input shape. That's why we need to specify input shape at start. But if you don't that's okay Keras will wait until you pass it the data or you can call its `build()` mehod. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-nepal",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "After a model is created, you must call its `compile()` method to specify loss and optimizer, optionally you can specify some metrics to compute during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "integral-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-possible",
   "metadata": {},
   "source": [
    "> Using `loss='sparse_categorical_crossentropy'` is equivalent to `loss=keras.losses.sparse_categorical_crossentropy`, same goes for optimizers and metrics.\n",
    "\n",
    "We use `sparse_categorical_crossentropy` loss because we have sparse labels (i.e. for each instances, there is just a target class index, from 0 to 9). If we had one-hot vectors then we'll use `categorical_crossentropy` loss instead. If we were doing binary classification binary classification, then we should use `sigmoid` activation function in the output layer, and we would use the `binary_crossentropy` loss.\n",
    "\n",
    "> If you want to convert sparse labels to one-hot vector labels, use `keras.utils.to_categorical()` function. To go other way around, use the `np.argmax()` function with `axis=1`.\n",
    "\n",
    "For optimizer `\"sgd\"` means that we will train model using simple Stochastic Gradient Descent.\n",
    "\n",
    "> When using SGD optimizer, it is important to tune the learning rate. So, you will generally want to use `optimizer=keras.optimizers.SGD(lr=???)` to set the learning rate, which defaults to `lr=0.01`\n",
    "\n",
    "Finally, metrics is `\"accuracy\"` which is calculated during training and evaluation.\n",
    "\n",
    "### Training and evaluating the Model\n",
    "\n",
    "For this we simply call its `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "requested-record",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.0076 - accuracy: 0.6797 - val_loss: 0.5114 - val_accuracy: 0.8266\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5032 - accuracy: 0.8266 - val_loss: 0.4391 - val_accuracy: 0.8524\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4538 - accuracy: 0.8426 - val_loss: 0.4249 - val_accuracy: 0.8580\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4209 - accuracy: 0.8525 - val_loss: 0.4007 - val_accuracy: 0.8662\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4000 - accuracy: 0.8582 - val_loss: 0.3759 - val_accuracy: 0.8688\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3869 - accuracy: 0.8636 - val_loss: 0.3831 - val_accuracy: 0.8662\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3732 - accuracy: 0.8683 - val_loss: 0.3652 - val_accuracy: 0.8678\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3544 - accuracy: 0.8751 - val_loss: 0.3567 - val_accuracy: 0.8768\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3418 - accuracy: 0.8798 - val_loss: 0.3484 - val_accuracy: 0.8764\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3359 - accuracy: 0.8800 - val_loss: 0.3464 - val_accuracy: 0.8762\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3239 - accuracy: 0.8853 - val_loss: 0.3605 - val_accuracy: 0.8712\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3219 - accuracy: 0.8859 - val_loss: 0.3328 - val_accuracy: 0.8798\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3190 - accuracy: 0.8866 - val_loss: 0.3334 - val_accuracy: 0.8822\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3089 - accuracy: 0.8901 - val_loss: 0.3237 - val_accuracy: 0.8820\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3085 - accuracy: 0.8914 - val_loss: 0.3321 - val_accuracy: 0.8846\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2977 - accuracy: 0.8934 - val_loss: 0.3197 - val_accuracy: 0.8840\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2860 - accuracy: 0.8975 - val_loss: 0.3189 - val_accuracy: 0.8894\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2809 - accuracy: 0.8983 - val_loss: 0.3079 - val_accuracy: 0.8898\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2750 - accuracy: 0.9004 - val_loss: 0.3136 - val_accuracy: 0.8890\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2755 - accuracy: 0.9029 - val_loss: 0.3050 - val_accuracy: 0.8926\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2645 - accuracy: 0.9036 - val_loss: 0.3176 - val_accuracy: 0.8866\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2649 - accuracy: 0.9049 - val_loss: 0.3142 - val_accuracy: 0.8884\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2553 - accuracy: 0.9077 - val_loss: 0.3078 - val_accuracy: 0.8906\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2578 - accuracy: 0.9067 - val_loss: 0.3041 - val_accuracy: 0.8904\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2526 - accuracy: 0.9080 - val_loss: 0.3282 - val_accuracy: 0.8818\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2499 - accuracy: 0.9083 - val_loss: 0.2996 - val_accuracy: 0.8936\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2378 - accuracy: 0.9138 - val_loss: 0.2954 - val_accuracy: 0.8950\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2386 - accuracy: 0.9127 - val_loss: 0.3023 - val_accuracy: 0.8902\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2357 - accuracy: 0.9147 - val_loss: 0.2993 - val_accuracy: 0.8940\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2313 - accuracy: 0.9155 - val_loss: 0.3021 - val_accuracy: 0.8904\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                   validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-cleveland",
   "metadata": {},
   "source": [
    "We pass it the input features (X_train) and the target classes (y_train), as well as number of epochs to train (defaults to 1).  We also pass it validation set (which is optional), Keras will measure the loss and other metrics on this set at the end of each epoch, which is very helpful to see. \n",
    "\n",
    "If performance on training set is much better than on the validation set, your model is probably overfitting the training set.\n",
    "\n",
    "> Instead of passing a validation set, you could set `validation_split` to the ratio you want. For example, `validation_split=0.1` last 10% of training data will be used for validation.\n",
    "\n",
    "If training set was very skewed, with some classes being over-represented than other, you could use `class_weight` argument in `fit()`, this would give extra weight to under-represented class while calculating the loss. If you need per-instance weights, use `sample_weight` argument, it would be useful if some instances were labeled by experts while other labeled by crowdsourcing platform: it gives more weight to the expert ones. You can also pass sample weight (not class weight) in `validation_data` tuple as third argument.\n",
    "\n",
    "The `fit()` method returns a `history` object which contains the training parameters (`history.params`), the list of epochs (`history.epoch`), and most important dictionary (`history.history`) containing loss and extra metrics it measured. You could plot history using pandas `DataFrame` to get learning curves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "killing-georgia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-surgeon",
   "metadata": {},
   "source": [
    "Here you would see that training and validation curves are close which is good. At first, it seems validation is good because validation error is computed at the end of epoch and training loss is computed during each epoch. So training curve should be shifted half the epoch to the left. Then you see overlapping curves at the start.\n",
    "\n",
    "> When plotting training curves, it should be shifted by half an epoch to the left.\n",
    "\n",
    "If you want to train longer you can call `fit()` method and Keras will continue the training.\n",
    "\n",
    "If you didn't satisfy with the training try tuning the hyperparameters. The first one to check is learning rate, then optimizer, then model's hyperparameters such as number of layers, number of neurons, and activation functions. You could also tune batch size (which can be set in `fit()` method as `batch_size` argument, defaults to 32) and number of epochs.\n",
    "\n",
    "Now, it's time to evaluate model on the test set to see the generalization error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "better-importance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 59.8660 - accuracy: 0.8518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[59.86600112915039, 0.8518000245094299]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-third",
   "metadata": {},
   "source": [
    "Generally, test set performance is slightly lower than validation set, because the hyperparameters tuned on validation set. And do not try to tune hyperparameters on the test set.\n",
    "\n",
    "#### Using the Model to make predictions\n",
    "\n",
    "Next, we can use model `predict()` method for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "green-prison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_probs = model.predict(X_new)\n",
    "y_probs.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-dylan",
   "metadata": {},
   "source": [
    "This will give you probabilities for each class. If you only want class then you can use `np.argmax()` method with `axis=-1`.\n",
    "\n",
    "> Use  `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification (e.g. if it uses a `softmax` last-layer activation).`(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "limited-necessity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "variable-kinase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-country",
   "metadata": {},
   "source": [
    "## Building a Regression MLP Using the Sequential API\n",
    "\n",
    "Let's do some linear regression using neural networks. We'll use Sklearn's `fetch_california_housing()` dataset, and do some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "genuine-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hollow-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-starter",
   "metadata": {},
   "source": [
    "Now, we use Sequential API to build, train, evaluate, and use a MLP regression model for predictions. The output layer has a single neuron to predict a value and has no activation function, and the mean squared error as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "lasting-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.2898 - val_loss: 0.5643\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5901 - val_loss: 0.5281\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5003 - val_loss: 0.4902\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4852 - val_loss: 0.4690\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4791 - val_loss: 0.4571\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4408 - val_loss: 0.4514\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4499 - val_loss: 0.4392\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4090 - val_loss: 0.4347\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4294\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4177 - val_loss: 0.4197\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4146\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.4197\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.4093\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 982us/step - loss: 0.4046 - val_loss: 0.4043\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3798 - val_loss: 0.4066\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 999us/step - loss: 0.3823 - val_loss: 0.3988\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.3955\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.3973\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3958\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.3941\n",
      "162/162 [==============================] - 0s 630us/step - loss: 0.3994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.9176114 ],\n",
       "       [0.57594985],\n",
       "       [4.932582  ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                   validation_data=(X_val, y_val))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-adult",
   "metadata": {},
   "source": [
    "Sequential API is quit easy and common, but sometimes we need to build more complex architectures like with multiple inputs or outputs. For this purpose, Keras offers a *Functional API*.\n",
    "\n",
    "## Building Complex Models using Functional API\n",
    "\n",
    "One example of non-sequential neural network is *Wide & Deep* neural networks. It connects all or part of the inputs to the outputs, which helps to learn both deep patterns (using deep path) and simple patterns (using short path).\n",
    "\n",
    "Let's build such a neural network:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "removable-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-harvey",
   "metadata": {},
   "source": [
    "Let's go through this code:\n",
    "\n",
    "- First, we need to create an `Input` object. Which specifies the input kind to the model.\n",
    "- Next, we create a normal `Dense` layer. Notice, we call it like a function, passing it the input. This is why this is called Functional API. \n",
    "- Then we pass output of first hidden to another.\n",
    "- Next, we create a `Concatenate` layer, we pass it the input and the output of second hidden layer. You may prefer `keras.layers.concatenate()` function.\n",
    "- Then we create an output layer, passing in result of the concatenation.\n",
    "- Lastly, we create a Keras `Model`, specifying which inputs and outputs to use.\n",
    "\n",
    "After that all steps are same.\n",
    "\n",
    "But what if you want to send a subset of the features through the wide path and different (possibly overlapping) to the deep path? Solution is to use multiple inputs. For example, suppose we want to pass five features through wide path (0 to 4), and six features to deep path (2 to 7):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "experienced-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-basketball",
   "metadata": {},
   "source": [
    "The code is self-explanatory. You should name the important layers when model get complex. Now to call `fit()` you need to pass it a pair of input matrices `(X_train_A, X_train_B)`, one for each. This is true for each `X_val`, `X_test`, and `X_new` when you call `evaluate()` or `predict()`.\n",
    "\n",
    "> Alternatively, you could pass a dictionary mapping the input names to the input matrices, like `{'wide_input': X_train_A, 'deep_input': X_train_B}`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "basic-element",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.7520 - val_loss: 0.9681\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8430 - val_loss: 0.7589\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7138 - val_loss: 0.6940\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6706 - val_loss: 0.6567\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6324 - val_loss: 0.6290\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5831 - val_loss: 0.6045\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5566 - val_loss: 0.5854\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5683 - val_loss: 0.5703\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5501 - val_loss: 0.5565\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5615 - val_loss: 0.5468\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5216 - val_loss: 0.5366\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5138 - val_loss: 0.5271\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5100 - val_loss: 0.5234\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5068 - val_loss: 0.5156\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4739 - val_loss: 0.5111\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4734 - val_loss: 0.5080\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5030 - val_loss: 0.5020\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5019 - val_loss: 0.5011\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4780 - val_loss: 0.4962\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5046 - val_loss: 0.4933\n",
      "162/162 [==============================] - 0s 690us/step - loss: 0.5053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.9436855],\n",
       "       [1.2431078],\n",
       "       [5.2231183]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_val_A, X_val_B = X_val[:, :5], X_val[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                   validation_data=((X_val_A, X_val_B), y_val))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-strip",
   "metadata": {},
   "source": [
    "There are many use cases when you may want multiple outputs:\n",
    "\n",
    "- For object detection, you may want to locate (regression) and classify (classification) an object in a picture.\n",
    "- If you have multiple independent tasks on the same data. Sure, you could train multiple neural networks for each task but training a single might get you better performance by learning across features. For example, you can perform *multitask classification*.\n",
    "- One use case is used as regularization technique by adding some auxiliary outputs in a neural network architecture to ensure that the underlying part of the networks learn something useful.\n",
    "\n",
    "Adding extra outputs is quite easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "statutory-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-growth",
   "metadata": {},
   "source": [
    "Each output will need its own loss function. Therefor, when we compile the model, we should pass a list of losses (or a dictionary mapping output names to losses). If we pass a single loss, Keras will assume same loss for all outputs. Keras will compute all these losses and add them up at last. Since, main outputs are more important than auxiliary ones, we can add weight to main output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "muslim-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-helen",
   "metadata": {},
   "source": [
    "Now when we train the model it should expect labels for each output. In this example the labels are same, so instead of passing `y_train`, we need to pass `(y_train, y_train)` (and same goes for `y_val` and `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "taken-buying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.7457 - main_output_loss: 1.5220 - aux_output_loss: 3.7588 - val_loss: 0.6087 - val_main_output_loss: 0.5422 - val_aux_output_loss: 1.2075\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5623 - main_output_loss: 0.5097 - aux_output_loss: 1.0352 - val_loss: 0.5396 - val_main_output_loss: 0.4950 - val_aux_output_loss: 0.9411\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4946 - main_output_loss: 0.4559 - aux_output_loss: 0.8429 - val_loss: 0.5207 - val_main_output_loss: 0.4890 - val_aux_output_loss: 0.8059\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4949 - main_output_loss: 0.4662 - aux_output_loss: 0.7528 - val_loss: 0.4773 - val_main_output_loss: 0.4515 - val_aux_output_loss: 0.7089\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4621 - main_output_loss: 0.4393 - aux_output_loss: 0.6676 - val_loss: 0.4654 - val_main_output_loss: 0.4434 - val_aux_output_loss: 0.6641\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4636 - main_output_loss: 0.4420 - aux_output_loss: 0.6575 - val_loss: 0.4515 - val_main_output_loss: 0.4313 - val_aux_output_loss: 0.6333\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4367 - main_output_loss: 0.4192 - aux_output_loss: 0.5946 - val_loss: 0.4397 - val_main_output_loss: 0.4218 - val_aux_output_loss: 0.6006\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4499 - main_output_loss: 0.4334 - aux_output_loss: 0.5982 - val_loss: 0.4461 - val_main_output_loss: 0.4288 - val_aux_output_loss: 0.6020\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4312 - main_output_loss: 0.4169 - aux_output_loss: 0.5602 - val_loss: 0.4223 - val_main_output_loss: 0.4079 - val_aux_output_loss: 0.5521\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4416 - main_output_loss: 0.4271 - aux_output_loss: 0.5720 - val_loss: 0.4166 - val_main_output_loss: 0.4014 - val_aux_output_loss: 0.5535\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4018 - main_output_loss: 0.3863 - aux_output_loss: 0.5414 - val_loss: 0.4050 - val_main_output_loss: 0.3911 - val_aux_output_loss: 0.5303\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4073 - main_output_loss: 0.3933 - aux_output_loss: 0.5335 - val_loss: 0.3989 - val_main_output_loss: 0.3848 - val_aux_output_loss: 0.5264\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3890 - main_output_loss: 0.3754 - aux_output_loss: 0.5105 - val_loss: 0.3991 - val_main_output_loss: 0.3853 - val_aux_output_loss: 0.5234\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3938 - main_output_loss: 0.3798 - aux_output_loss: 0.5203 - val_loss: 0.3882 - val_main_output_loss: 0.3760 - val_aux_output_loss: 0.4984\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3706 - main_output_loss: 0.3578 - aux_output_loss: 0.4863 - val_loss: 0.3787 - val_main_output_loss: 0.3665 - val_aux_output_loss: 0.4880\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3701 - main_output_loss: 0.3573 - aux_output_loss: 0.4856 - val_loss: 0.3784 - val_main_output_loss: 0.3663 - val_aux_output_loss: 0.4872\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3665 - main_output_loss: 0.3543 - aux_output_loss: 0.4766 - val_loss: 0.3736 - val_main_output_loss: 0.3623 - val_aux_output_loss: 0.4756\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3647 - main_output_loss: 0.3523 - aux_output_loss: 0.4755 - val_loss: 0.3654 - val_main_output_loss: 0.3545 - val_aux_output_loss: 0.4638\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3436 - main_output_loss: 0.3320 - aux_output_loss: 0.4476 - val_loss: 0.3697 - val_main_output_loss: 0.3581 - val_aux_output_loss: 0.4741\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3552 - main_output_loss: 0.3432 - aux_output_loss: 0.4631 - val_loss: 0.3680 - val_main_output_loss: 0.3567 - val_aux_output_loss: 0.4700\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-microwave",
   "metadata": {},
   "source": [
    "When we evaluate the model, Keras will return total loss, as well as all the individual losses. Similarly, `predict()` will return predictions for each output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sound-migration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 831us/step - loss: 0.3684 - main_output_loss: 0.3574 - aux_output_loss: 0.4676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[3.143724 ],\n",
       "        [1.1296512],\n",
       "        [5.0402637]], dtype=float32),\n",
       " array([[2.5922377],\n",
       "        [1.009677 ],\n",
       "        [4.452898 ]], dtype=float32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test]\n",
    ")\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
    "y_pred_main, y_pred_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-civilian",
   "metadata": {},
   "source": [
    "## Using the Subclassing API to Build Dynamic Models\n",
    "\n",
    "Both Sequential API and Functional API are declarative: you start by declaring layers and how should they connect to each other. But they are static; you cannot involve loops, varying shapes, conditional branching, and other dynamic behaviors. For this you'll use Subclassing API.\n",
    "\n",
    "Simply subclass the `Model` class, create the layers you need in the constructor, and use them to perform the computations you want in the `call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "processed-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-apache",
   "metadata": {},
   "source": [
    "The main difference here is you can do anything in `call()` method: for loops, if else, low-level TensorFlow operations. This makes it a great API for researchers experimenting with new ideas.\n",
    "\n",
    "> Keras models have an `output` attribute, that's why we use `main_output` name.\n",
    "\n",
    "This extra flexibility comes with a cost: your model's architecture is hidden within `call()` method, so Keras cannot easily inspect it; it cannot save it or clone it; and when you call `summary()` method it only give list of layers not how they are connected.\n",
    "\n",
    "## Saving and Restoring a Model\n",
    "\n",
    "Saving trained models made by Sequential API or Functional API is as simple as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "united-correction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.5807 - main_output_loss: 1.4653 - aux_output_loss: 2.6198 - val_loss: 0.6128 - val_main_output_loss: 0.5410 - val_aux_output_loss: 1.2594\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7597 - main_output_loss: 0.7154 - aux_output_loss: 1.1592 - val_loss: 0.5601 - val_main_output_loss: 0.5084 - val_aux_output_loss: 1.0256\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5519 - main_output_loss: 0.5062 - aux_output_loss: 0.9628 - val_loss: 0.5152 - val_main_output_loss: 0.4765 - val_aux_output_loss: 0.8630\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4997 - main_output_loss: 0.4648 - aux_output_loss: 0.8133 - val_loss: 0.4878 - val_main_output_loss: 0.4570 - val_aux_output_loss: 0.7647\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4861 - main_output_loss: 0.4578 - aux_output_loss: 0.7409 - val_loss: 0.4765 - val_main_output_loss: 0.4506 - val_aux_output_loss: 0.7099\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4827 - main_output_loss: 0.4594 - aux_output_loss: 0.6921 - val_loss: 0.4615 - val_main_output_loss: 0.4374 - val_aux_output_loss: 0.6786\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4608 - main_output_loss: 0.4374 - aux_output_loss: 0.6709 - val_loss: 0.4603 - val_main_output_loss: 0.4383 - val_aux_output_loss: 0.6583\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4404 - main_output_loss: 0.4193 - aux_output_loss: 0.6307 - val_loss: 0.4489 - val_main_output_loss: 0.4277 - val_aux_output_loss: 0.6395\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4336 - main_output_loss: 0.4133 - aux_output_loss: 0.6160 - val_loss: 0.4470 - val_main_output_loss: 0.4261 - val_aux_output_loss: 0.6353\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4258 - main_output_loss: 0.4063 - aux_output_loss: 0.6007 - val_loss: 0.4270 - val_main_output_loss: 0.4070 - val_aux_output_loss: 0.6073\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4044 - main_output_loss: 0.3846 - aux_output_loss: 0.5825 - val_loss: 0.4209 - val_main_output_loss: 0.4013 - val_aux_output_loss: 0.5972\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4211 - main_output_loss: 0.4031 - aux_output_loss: 0.5830 - val_loss: 0.4106 - val_main_output_loss: 0.3921 - val_aux_output_loss: 0.5774\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4123 - main_output_loss: 0.3941 - aux_output_loss: 0.5760 - val_loss: 0.4053 - val_main_output_loss: 0.3870 - val_aux_output_loss: 0.5705\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3895 - main_output_loss: 0.3720 - aux_output_loss: 0.5472 - val_loss: 0.4061 - val_main_output_loss: 0.3888 - val_aux_output_loss: 0.5623\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4087 - main_output_loss: 0.3925 - aux_output_loss: 0.5542 - val_loss: 0.4002 - val_main_output_loss: 0.3830 - val_aux_output_loss: 0.5546\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3804 - main_output_loss: 0.3635 - aux_output_loss: 0.5323 - val_loss: 0.3899 - val_main_output_loss: 0.3731 - val_aux_output_loss: 0.5417\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3850 - main_output_loss: 0.3689 - aux_output_loss: 0.5307 - val_loss: 0.3835 - val_main_output_loss: 0.3669 - val_aux_output_loss: 0.5325\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3782 - main_output_loss: 0.3617 - aux_output_loss: 0.5261 - val_loss: 0.3891 - val_main_output_loss: 0.3727 - val_aux_output_loss: 0.5369\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3823 - main_output_loss: 0.3671 - aux_output_loss: 0.5190 - val_loss: 0.3768 - val_main_output_loss: 0.3610 - val_aux_output_loss: 0.5195\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3725 - main_output_loss: 0.3579 - aux_output_loss: 0.5043 - val_loss: 0.3777 - val_main_output_loss: 0.3620 - val_aux_output_loss: 0.5190\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')\n",
    "\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]))\n",
    "\n",
    "model.save('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-knife",
   "metadata": {},
   "source": [
    "Keras will use `HDF5` format to save both model's architecture (including every layer's hyperparameters) and the values of all the parameters for every layer. It also saves the optimizer.\n",
    "\n",
    "And loading the model is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "arabic-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-pastor",
   "metadata": {},
   "source": [
    "> If you use Subclassing API, then you can save model's parameters by `save_weights()` and load them by `load_weights()`. But you have to save everything else by yourself as a Dictionary maybe.\n",
    "\n",
    "But what if your training lasts several hours? Then you should save checkpoints for every epoch, in case you system crashes this will save everything your model learned so far. This can be done using callbacks.\n",
    "\n",
    "## Using Callbacks\n",
    "\n",
    "The `fit()` method accepts a `callbacks` argument that lets you specify when to save model. For example, `ModelCheckpoint` callback saves checkpoint at regular intervals (by default after each epoch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "sexual-differential",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.6606 - main_output_loss: 1.5197 - aux_output_loss: 2.9291 - val_loss: 0.6142 - val_main_output_loss: 0.5526 - val_aux_output_loss: 1.1684\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6099 - main_output_loss: 0.5594 - aux_output_loss: 1.0646 - val_loss: 0.5501 - val_main_output_loss: 0.5042 - val_aux_output_loss: 0.9626\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5255 - main_output_loss: 0.4835 - aux_output_loss: 0.9035 - val_loss: 0.5055 - val_main_output_loss: 0.4696 - val_aux_output_loss: 0.8284\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5015 - main_output_loss: 0.4712 - aux_output_loss: 0.7749 - val_loss: 0.4776 - val_main_output_loss: 0.4482 - val_aux_output_loss: 0.7423\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5013 - main_output_loss: 0.4737 - aux_output_loss: 0.7496 - val_loss: 0.4779 - val_main_output_loss: 0.4511 - val_aux_output_loss: 0.7194\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5071 - main_output_loss: 0.4827 - aux_output_loss: 0.7265 - val_loss: 0.4601 - val_main_output_loss: 0.4367 - val_aux_output_loss: 0.6714\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4610 - main_output_loss: 0.4385 - aux_output_loss: 0.6632 - val_loss: 0.4553 - val_main_output_loss: 0.4341 - val_aux_output_loss: 0.6454\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4512 - main_output_loss: 0.4299 - aux_output_loss: 0.6424 - val_loss: 0.4429 - val_main_output_loss: 0.4228 - val_aux_output_loss: 0.6239\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4211 - main_output_loss: 0.4016 - aux_output_loss: 0.5960 - val_loss: 0.4358 - val_main_output_loss: 0.4167 - val_aux_output_loss: 0.6074\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4171 - main_output_loss: 0.3980 - aux_output_loss: 0.5883 - val_loss: 0.4245 - val_main_output_loss: 0.4060 - val_aux_output_loss: 0.5909\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4310 - main_output_loss: 0.4132 - aux_output_loss: 0.5920 - val_loss: 0.4320 - val_main_output_loss: 0.4142 - val_aux_output_loss: 0.5921\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4164 - main_output_loss: 0.3969 - aux_output_loss: 0.5912 - val_loss: 0.4101 - val_main_output_loss: 0.3930 - val_aux_output_loss: 0.5643\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4056 - main_output_loss: 0.3879 - aux_output_loss: 0.5648 - val_loss: 0.4022 - val_main_output_loss: 0.3847 - val_aux_output_loss: 0.5602\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3782 - main_output_loss: 0.3601 - aux_output_loss: 0.5409 - val_loss: 0.3945 - val_main_output_loss: 0.3775 - val_aux_output_loss: 0.5476\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3968 - main_output_loss: 0.3798 - aux_output_loss: 0.5496 - val_loss: 0.3892 - val_main_output_loss: 0.3719 - val_aux_output_loss: 0.5441\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3887 - main_output_loss: 0.3704 - aux_output_loss: 0.5535 - val_loss: 0.3944 - val_main_output_loss: 0.3778 - val_aux_output_loss: 0.5438\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3868 - main_output_loss: 0.3707 - aux_output_loss: 0.5319 - val_loss: 0.3828 - val_main_output_loss: 0.3671 - val_aux_output_loss: 0.5242\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3722 - main_output_loss: 0.3554 - aux_output_loss: 0.5236 - val_loss: 0.3785 - val_main_output_loss: 0.3629 - val_aux_output_loss: 0.5196\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3672 - main_output_loss: 0.3501 - aux_output_loss: 0.5211 - val_loss: 0.3791 - val_main_output_loss: 0.3637 - val_aux_output_loss: 0.5173\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3605 - main_output_loss: 0.3450 - aux_output_loss: 0.5006 - val_loss: 0.3733 - val_main_output_loss: 0.3582 - val_aux_output_loss: 0.5090\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model.h5', save_best_only=True)\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]),\n",
    "                   callbacks=[checkpoint_cb])\n",
    "\n",
    "model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-leader",
   "metadata": {},
   "source": [
    "If you use validation data then set `save_best_only=True` will save only best performance one so far. This will be best on validation set.\n",
    "\n",
    "Another way is use the `EarlyStopping` callback: it will interrupt training when there is no progress on the validation set for a number of epochs (defined by `patience` argument), and it will optionally roll back to the best model. You can combine both; this will useful in both ways if you computer crashes or stop early when there is no progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "centered-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3787 - main_output_loss: 0.3644 - aux_output_loss: 0.5073 - val_loss: 0.3697 - val_main_output_loss: 0.3557 - val_aux_output_loss: 0.4954\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3731 - main_output_loss: 0.3590 - aux_output_loss: 0.5000 - val_loss: 0.3756 - val_main_output_loss: 0.3613 - val_aux_output_loss: 0.5047\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3598 - main_output_loss: 0.3449 - aux_output_loss: 0.4932 - val_loss: 0.3595 - val_main_output_loss: 0.3458 - val_aux_output_loss: 0.4827\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3575 - main_output_loss: 0.3430 - aux_output_loss: 0.4873 - val_loss: 0.3631 - val_main_output_loss: 0.3493 - val_aux_output_loss: 0.4877\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3548 - main_output_loss: 0.3408 - aux_output_loss: 0.4812 - val_loss: 0.3546 - val_main_output_loss: 0.3414 - val_aux_output_loss: 0.4734\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3527 - main_output_loss: 0.3389 - aux_output_loss: 0.4770 - val_loss: 0.3534 - val_main_output_loss: 0.3405 - val_aux_output_loss: 0.4689\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3499 - main_output_loss: 0.3365 - aux_output_loss: 0.4708 - val_loss: 0.3532 - val_main_output_loss: 0.3408 - val_aux_output_loss: 0.4647\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3476 - main_output_loss: 0.3344 - aux_output_loss: 0.4664 - val_loss: 0.3514 - val_main_output_loss: 0.3398 - val_aux_output_loss: 0.4556\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3474 - main_output_loss: 0.3347 - aux_output_loss: 0.4624 - val_loss: 0.3537 - val_main_output_loss: 0.3425 - val_aux_output_loss: 0.4543\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3446 - main_output_loss: 0.3320 - aux_output_loss: 0.4585 - val_loss: 0.3641 - val_main_output_loss: 0.3533 - val_aux_output_loss: 0.4615\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3427 - main_output_loss: 0.3305 - aux_output_loss: 0.4527 - val_loss: 0.3418 - val_main_output_loss: 0.3301 - val_aux_output_loss: 0.4471\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3411 - main_output_loss: 0.3291 - aux_output_loss: 0.4492 - val_loss: 0.3562 - val_main_output_loss: 0.3447 - val_aux_output_loss: 0.4591\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3397 - main_output_loss: 0.3278 - aux_output_loss: 0.4460 - val_loss: 0.3507 - val_main_output_loss: 0.3402 - val_aux_output_loss: 0.4452\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3368 - main_output_loss: 0.3252 - aux_output_loss: 0.4416 - val_loss: 0.3378 - val_main_output_loss: 0.3268 - val_aux_output_loss: 0.4373\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3365 - main_output_loss: 0.3253 - aux_output_loss: 0.4375 - val_loss: 0.3372 - val_main_output_loss: 0.3266 - val_aux_output_loss: 0.4333\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3357 - main_output_loss: 0.3245 - aux_output_loss: 0.4362 - val_loss: 0.3399 - val_main_output_loss: 0.3301 - val_aux_output_loss: 0.4280\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3352 - main_output_loss: 0.3242 - aux_output_loss: 0.4345 - val_loss: 0.3368 - val_main_output_loss: 0.3268 - val_aux_output_loss: 0.4261\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3336 - main_output_loss: 0.3229 - aux_output_loss: 0.4298 - val_loss: 0.3376 - val_main_output_loss: 0.3276 - val_aux_output_loss: 0.4278\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3341 - main_output_loss: 0.3235 - aux_output_loss: 0.4301 - val_loss: 0.3525 - val_main_output_loss: 0.3430 - val_aux_output_loss: 0.4375\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3328 - main_output_loss: 0.3224 - aux_output_loss: 0.4261 - val_loss: 0.3468 - val_main_output_loss: 0.3369 - val_aux_output_loss: 0.4358\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3302 - main_output_loss: 0.3201 - aux_output_loss: 0.4215 - val_loss: 0.3480 - val_main_output_loss: 0.3385 - val_aux_output_loss: 0.4339\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3289 - main_output_loss: 0.3187 - aux_output_loss: 0.4207 - val_loss: 0.3467 - val_main_output_loss: 0.3376 - val_aux_output_loss: 0.4290\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3288 - main_output_loss: 0.3188 - aux_output_loss: 0.4192 - val_loss: 0.3464 - val_main_output_loss: 0.3374 - val_aux_output_loss: 0.4273\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3291 - main_output_loss: 0.3192 - aux_output_loss: 0.4181 - val_loss: 0.3389 - val_main_output_loss: 0.3296 - val_aux_output_loss: 0.4232\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3276 - main_output_loss: 0.3178 - aux_output_loss: 0.4156 - val_loss: 0.3349 - val_main_output_loss: 0.3260 - val_aux_output_loss: 0.4151\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3262 - main_output_loss: 0.3165 - aux_output_loss: 0.4134 - val_loss: 0.3581 - val_main_output_loss: 0.3496 - val_aux_output_loss: 0.4350\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3271 - main_output_loss: 0.3175 - aux_output_loss: 0.4129 - val_loss: 0.3405 - val_main_output_loss: 0.3323 - val_aux_output_loss: 0.4136\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3269 - main_output_loss: 0.3174 - aux_output_loss: 0.4124 - val_loss: 0.3352 - val_main_output_loss: 0.3263 - val_aux_output_loss: 0.4150\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3248 - main_output_loss: 0.3152 - aux_output_loss: 0.4108 - val_loss: 0.3327 - val_main_output_loss: 0.3241 - val_aux_output_loss: 0.4103\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3250 - main_output_loss: 0.3157 - aux_output_loss: 0.4087 - val_loss: 0.3344 - val_main_output_loss: 0.3255 - val_aux_output_loss: 0.4144\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3260 - main_output_loss: 0.3167 - aux_output_loss: 0.4097 - val_loss: 0.3308 - val_main_output_loss: 0.3223 - val_aux_output_loss: 0.4071\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3253 - main_output_loss: 0.3161 - aux_output_loss: 0.4081 - val_loss: 0.3317 - val_main_output_loss: 0.3233 - val_aux_output_loss: 0.4071\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3238 - main_output_loss: 0.3147 - aux_output_loss: 0.4059 - val_loss: 0.3320 - val_main_output_loss: 0.3239 - val_aux_output_loss: 0.4052\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3229 - main_output_loss: 0.3138 - aux_output_loss: 0.4040 - val_loss: 0.3345 - val_main_output_loss: 0.3261 - val_aux_output_loss: 0.4103\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3231 - main_output_loss: 0.3142 - aux_output_loss: 0.4040 - val_loss: 0.3326 - val_main_output_loss: 0.3244 - val_aux_output_loss: 0.4066\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3212 - main_output_loss: 0.3122 - aux_output_loss: 0.4015 - val_loss: 0.3228 - val_main_output_loss: 0.3144 - val_aux_output_loss: 0.3984\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3216 - main_output_loss: 0.3128 - aux_output_loss: 0.4011 - val_loss: 0.3286 - val_main_output_loss: 0.3204 - val_aux_output_loss: 0.4024\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3221 - main_output_loss: 0.3134 - aux_output_loss: 0.4008 - val_loss: 0.3233 - val_main_output_loss: 0.3153 - val_aux_output_loss: 0.3952\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3197 - main_output_loss: 0.3108 - aux_output_loss: 0.3991 - val_loss: 0.3258 - val_main_output_loss: 0.3180 - val_aux_output_loss: 0.3957\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3202 - main_output_loss: 0.3115 - aux_output_loss: 0.3979 - val_loss: 0.3257 - val_main_output_loss: 0.3175 - val_aux_output_loss: 0.3997\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3195 - main_output_loss: 0.3109 - aux_output_loss: 0.3977 - val_loss: 0.3268 - val_main_output_loss: 0.3185 - val_aux_output_loss: 0.4019\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3193 - main_output_loss: 0.3107 - aux_output_loss: 0.3968 - val_loss: 0.3402 - val_main_output_loss: 0.3324 - val_aux_output_loss: 0.4102\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3184 - main_output_loss: 0.3099 - aux_output_loss: 0.3943 - val_loss: 0.3235 - val_main_output_loss: 0.3153 - val_aux_output_loss: 0.3969\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3173 - main_output_loss: 0.3089 - aux_output_loss: 0.3929 - val_loss: 0.3261 - val_main_output_loss: 0.3178 - val_aux_output_loss: 0.4002\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3168 - main_output_loss: 0.3084 - aux_output_loss: 0.3928 - val_loss: 0.3291 - val_main_output_loss: 0.3214 - val_aux_output_loss: 0.3988\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3187 - main_output_loss: 0.3104 - aux_output_loss: 0.3935 - val_loss: 0.3231 - val_main_output_loss: 0.3157 - val_aux_output_loss: 0.3896\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=100,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]),\n",
    "                   callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-weekly",
   "metadata": {},
   "source": [
    "You can set large number of epochs since it will stop automatically after no progress.\n",
    "\n",
    "If you need extra control, you can easily build your custom callback. There is an example of displaying ratio of validation loss and training loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "grateful-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3199 - main_output_loss: 0.3109 - aux_output_loss: 0.4006 - val_loss: 0.3275 - val_main_output_loss: 0.3196 - val_aux_output_loss: 0.3986\n",
      "\n",
      "val/train: 1.02\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3192 - main_output_loss: 0.3105 - aux_output_loss: 0.3981 - val_loss: 0.3271 - val_main_output_loss: 0.3186 - val_aux_output_loss: 0.4031\n",
      "\n",
      "val/train: 1.02\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3210 - main_output_loss: 0.3122 - aux_output_loss: 0.3997 - val_loss: 0.3412 - val_main_output_loss: 0.3335 - val_aux_output_loss: 0.4108\n",
      "\n",
      "val/train: 1.06\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3204 - main_output_loss: 0.3117 - aux_output_loss: 0.3983 - val_loss: 0.3240 - val_main_output_loss: 0.3158 - val_aux_output_loss: 0.3980\n",
      "\n",
      "val/train: 1.01\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3185 - main_output_loss: 0.3100 - aux_output_loss: 0.3949 - val_loss: 0.3222 - val_main_output_loss: 0.3142 - val_aux_output_loss: 0.3939\n",
      "\n",
      "val/train: 1.01\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3185 - main_output_loss: 0.3099 - aux_output_loss: 0.3961 - val_loss: 0.3320 - val_main_output_loss: 0.3243 - val_aux_output_loss: 0.4013\n",
      "\n",
      "val/train: 1.04\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3201 - main_output_loss: 0.3115 - aux_output_loss: 0.3973 - val_loss: 0.3254 - val_main_output_loss: 0.3169 - val_aux_output_loss: 0.4017\n",
      "\n",
      "val/train: 1.02\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3191 - main_output_loss: 0.3104 - aux_output_loss: 0.3972 - val_loss: 0.3258 - val_main_output_loss: 0.3178 - val_aux_output_loss: 0.3980\n",
      "\n",
      "val/train: 1.02\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3189 - main_output_loss: 0.3106 - aux_output_loss: 0.3941 - val_loss: 0.3577 - val_main_output_loss: 0.3504 - val_aux_output_loss: 0.4231\n",
      "\n",
      "val/train: 1.12\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3175 - main_output_loss: 0.3091 - aux_output_loss: 0.3934 - val_loss: 0.3362 - val_main_output_loss: 0.3283 - val_aux_output_loss: 0.4068\n",
      "\n",
      "val/train: 1.06\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3192 - main_output_loss: 0.3111 - aux_output_loss: 0.3927 - val_loss: 0.3241 - val_main_output_loss: 0.3158 - val_aux_output_loss: 0.3991\n",
      "\n",
      "val/train: 1.02\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3186 - main_output_loss: 0.3103 - aux_output_loss: 0.3936 - val_loss: 0.3240 - val_main_output_loss: 0.3163 - val_aux_output_loss: 0.3931\n",
      "\n",
      "val/train: 1.02\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3161 - main_output_loss: 0.3079 - aux_output_loss: 0.3901 - val_loss: 0.3214 - val_main_output_loss: 0.3136 - val_aux_output_loss: 0.3910\n",
      "\n",
      "val/train: 1.02\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3146 - main_output_loss: 0.3065 - aux_output_loss: 0.3883 - val_loss: 0.3216 - val_main_output_loss: 0.3138 - val_aux_output_loss: 0.3914\n",
      "\n",
      "val/train: 1.02\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3161 - main_output_loss: 0.3080 - aux_output_loss: 0.3892 - val_loss: 0.3287 - val_main_output_loss: 0.3215 - val_aux_output_loss: 0.3927\n",
      "\n",
      "val/train: 1.04\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3127 - main_output_loss: 0.3047 - aux_output_loss: 0.3852 - val_loss: 0.3184 - val_main_output_loss: 0.3105 - val_aux_output_loss: 0.3896\n",
      "\n",
      "val/train: 1.02\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3143 - main_output_loss: 0.3063 - aux_output_loss: 0.3866 - val_loss: 0.3259 - val_main_output_loss: 0.3178 - val_aux_output_loss: 0.3983\n",
      "\n",
      "val/train: 1.04\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3149 - main_output_loss: 0.3068 - aux_output_loss: 0.3876 - val_loss: 0.3398 - val_main_output_loss: 0.3330 - val_aux_output_loss: 0.4008\n",
      "\n",
      "val/train: 1.08\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3125 - main_output_loss: 0.3046 - aux_output_loss: 0.3837 - val_loss: 0.3325 - val_main_output_loss: 0.3249 - val_aux_output_loss: 0.4006\n",
      "\n",
      "val/train: 1.06\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3141 - main_output_loss: 0.3062 - aux_output_loss: 0.3854 - val_loss: 0.3234 - val_main_output_loss: 0.3162 - val_aux_output_loss: 0.3875\n",
      "\n",
      "val/train: 1.03\n"
     ]
    }
   ],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(f\"\\nval/train: {logs['val_loss']/logs['loss']:.2f}\")\n",
    "\n",
    "ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]),\n",
    "                   callbacks=[ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-lounge",
   "metadata": {},
   "source": [
    ">`on_epoch_end()` is predefined method, there are others which you can use like `on_epoch_begin()`. There are such methods for test and predict also, if you need debugging in those cases.\n",
    "\n",
    "## Using TensorBoard for Visualization\n",
    "\n",
    "TensorBoard is a great interactive visualization tool that you can use to \n",
    "\n",
    "- view the learning curves during multiple runs,\n",
    "- visualize the computation graph,\n",
    "- analyze training statistics,\n",
    "- view images generated by your model,\n",
    "- visualize complex multidimensional data projected down to 3D, and\n",
    "- automatically clustered for you and more!\n",
    "\n",
    "To use it, you need to modify your program including a special binary file called *event files*. Each binary data is called a *summary*. TensorBoard server will monitor the log directory. We have to point TensorBoard to the root directory and configure your program so that it writes in different directory every time it runs. This way your logs won't mixed up.\n",
    "\n",
    "Let's make it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "happy-style",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.0155 - val_loss: 0.7296\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5633 - val_loss: 0.4930\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4670 - val_loss: 0.4504\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.4122\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.4107\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.3967\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3842 - val_loss: 0.3937\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3779 - val_loss: 0.3968\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3618 - val_loss: 0.3804\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3791\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3582 - val_loss: 0.4079\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3643 - val_loss: 0.3840\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3617 - val_loss: 0.3813\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.3725\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.3728\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3645\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3843\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3415 - val_loss: 0.3714\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3699\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3419 - val_loss: 0.3631\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3717\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.3604\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3571\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3597\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3414 - val_loss: 0.3589\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 0.3588\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3326 - val_loss: 0.3644\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3234 - val_loss: 0.3606\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3349 - val_loss: 0.3714\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3368 - val_loss: 0.3553\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                   validation_data=(X_val, y_val),\n",
    "                   callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-crazy",
   "metadata": {},
   "source": [
    "It will automatically create needed folders and events as well as profile traces: that tells you much time your model takes on each part, and across all your devices, which is great for locating bottlenecks.\n",
    "\n",
    "Next you need to start the TensorBoard server, you can do this by command line:\n",
    "\n",
    "`$ tensorboard --logdir=./my_logs --port=6006`\n",
    "\n",
    "Once the server is up go to web browser and visit *https://localhost:6006*.\n",
    "\n",
    "There you can see TensorBoard web interface. Click the SCALARS tab to view learning curves.\n",
    "\n",
    "Additionally, TensorFlow offers a lower-level API in the `tf.summary` package. Which creates a `SummaryWriter` using the `create_file_writer()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-numbers",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}