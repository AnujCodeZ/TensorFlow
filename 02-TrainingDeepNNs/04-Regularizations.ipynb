{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thousand-constant",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting through Regularization\n",
    "\n",
    "Deep neural networks have many parameters to fit that give them incredible freedom and flexibility. But, it tends to overfit dataset because of this, so we need regularization techniques.\n",
    "\n",
    "We already studied early stopping and Batch Normalization (which is not for overfitting).\n",
    "\n",
    "## $l_1$ and $l_2$ Regularization\n",
    "\n",
    "You can use $l_2$ regularization to constrain connection weights or use $l_1$ if you want sparse model.\n",
    "\n",
    "```python\n",
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                          kernel_initialization='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "```\n",
    "\n",
    "This computes regularization loss at every steps and then add it to the final loss.\n",
    "\n",
    "`keras.regularizers.l1()` for $l1$ regularization. `keras.regularizers.l1_l2()` if you want to use both.\n",
    "\n",
    "If you want to use same activation, regularization, and initialization to every layer, then you can do it with Python's `functools.partial()` function:\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense, \n",
    "                           activation='elu',\n",
    "                           kernel_initialization='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax',\n",
    "                    kernel_initializer='glorot_uniform')\n",
    "])\n",
    "```\n",
    "\n",
    "## Dropout\n",
    "\n",
    "*Dropout* is one of the most popular regularization and default in recent deep neural networks.\n",
    "\n",
    "It is a fairly simple algorithm: at every training step, every neurons (excluding output neurons) has a probability $p$ of being temporarily \"dropped out\", meaning it will be ignored during this training step, but it may be active during the next step. $p$ is a hyperparameter called *dropout rate*, usually between 10% to 50%: closer to 40-50% in CNNs, and 20-30% in RNNs. After training it will be removed.\n",
    "\n",
    "It works because neurons now not depend on the neighbor neurons, it will learn on their own and it will focus on every input neurons makes it less sensitive to slight changes in input neurons. Thus performing a great generalization.\n",
    "\n",
    "Another power of dropout is that a new network is generated at each step. So if you train for 10,000 epochs, you have 10,000 different networks. These networks are not independent as they share many of their weights. This results in ensemble of many neural networks.\n",
    "\n",
    "One technical detail is when applying dropout, remaining neurons have extra input weights (in average) as it should be, which leads to total much more weights than we want. To solve this multiply each input connection weights by $(1 - p)$ *keep probability* after training or alternatively divide each neuron's output by keep probability during training.\n",
    "\n",
    "To implement dropout in Keras, you can use `keras.layers.Dropout`:\n",
    "\n",
    "```python\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(100, activation='elu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "> Since dropout is active only during training, comparing training loss with validation loss can be misleading. So make sure to evaluate training loss after training without dropout.\n",
    "\n",
    "Dropout does tends to significantly slow down convergence, but it results a much better model.\n",
    "\n",
    "> If you want to regularize a self-normalizing network based on SELU activation function, you can use *alpha dropout*: this preserves mean and variance of its inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-choir",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
