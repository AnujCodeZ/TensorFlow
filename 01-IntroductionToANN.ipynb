{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dressed-conjunction",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Network with Keras\n",
    "\n",
    "## Introduction\n",
    "\n",
    "*Artificial Neural Networks* (ANNs) is a Machine Learning Model inspired by the networks of biological neurons found in our brains.\n",
    "\n",
    "ANNs are the very core of Deep Learning. They are versatile, powerful, and scalable. They are able to tackle complex problems like classifying large number of images (Google Images), speech recognition services (\"OK Google\"), recommending movies (Netflix), etc.\n",
    "\n",
    "We'll use popular Keras API: this is a beautifully designed and simple high-level API for building, training, evaluating, and running neural networks. \n",
    "\n",
    "## History of ANNs\n",
    "\n",
    "ANNs first introduced back in 1943 by Neurophysiologist Warren McCulloch and the mathematician Walter Pitts in their paper \"A Logical Calculus of Ideas Immanent in Nervous Activity\".\n",
    "\n",
    "In 1960, idea of making intelligent machines seems impossible (for quite a while), ANNs entered a long winter.\n",
    "\n",
    "In early 1980, new architectures were build, interest rises in *connectionism* (the study of neural networks), progress was slow. And around 1990, more powerful Machine Learning Algorithms was built like SVMs. So they put on hold again.\n",
    "\n",
    "But now we see that ANNs are rising and this time they keep rising. Here are few reasons why:\n",
    "\n",
    "- Huge quantity of data available.\n",
    "- Increase in Computing powers, and thanks to Gaming industry to give us GPU.\n",
    "- Training algorithms have been improved.\n",
    "- More funding in building amazing products and research.\n",
    "\n",
    "## Logical Computations with Neurons\n",
    "\n",
    "McCulloch and Pitts proposed a very simple model of the biological neuron, which later became known as *artificial neuron*: it has one or more binary inputs and one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active. \n",
    "\n",
    "You can compute complex logical expressions by changing the inputs or combining them.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "The *Perceptron* is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on slightly different artificial neuron called a *threshold logic unit* (TLU), or sometimes a *linear threshold unit* (LTU).\n",
    "\n",
    "The inputs and output are numbers, and each connection holds some value called *weights*. The TLU computes a weighted sum of its inputs:\n",
    "$$\n",
    "z = w_1x_1 + w_2x_2 + ... + w_nx_n = X^TW\n",
    "$$\n",
    " Then applies a *step function* and results an output:\n",
    "$$\n",
    "h_w(X) = step(z), where\\ z = X^TW\n",
    "$$\n",
    "The most common step function used in Perceptron is the *Heaviside step function*:\n",
    "$$\n",
    "heaviside(z) = \\{{0, if\\ z < 0 \\\\ 1, if z >= 0}\n",
    "$$\n",
    "Sometimes a sign function:\n",
    "$$\n",
    "sign(z) = \\{{-1, if\\ z < 0 \\\\ 0, if\\ z = 0 \\\\ 1, if\\ z > 0}\n",
    "$$\n",
    "A single TLU can be use for simple linear binary classification. Training a TLU means finding the right values of weights.\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs.\n",
    "\n",
    "When all the neurons in a layer are connected to every neuron in previous layer, the layer is called *fully connected layer* or *dense layer*.\n",
    "\n",
    "The inputs of Perceptron are simply passthrough neurons called *input neurons*: they output whatever they are fed. All the input neurons form the *input layer*. Moreover, an extra bias feature is added: represented as a *bias neuron*, always outputs 1. \n",
    "\n",
    "Computing the outputs of a fully connected layer:\n",
    "$$\n",
    "h_{W, b} = \\phi(XW + b)\n",
    "$$\n",
    " In this equation,\n",
    "\n",
    "- **X**: inputs matrix\n",
    "- **W**: weights matrix\n",
    "- **b**: bias\n",
    "- $\\phi$: it represents *activation function*\n",
    "\n",
    "The training of Perceptron was largely inspired by *Hebb's rule*: which is summarized as \"cells that fire together, wire together\"; that is connection weights between two neurons tends to increase when they activates together.\n",
    "\n",
    "The Perceptron trains with slight different variant that takes into account of error made by the network. It reinforces the connection when it makes an error. The rule is:\n",
    "$$\n",
    "w_{i,j}^{(next\\ step)} = w_{i, j} + \\eta(y_j - \\hat y_j)x_i\n",
    "$$\n",
    "In this equation:\n",
    "\n",
    "- $w_{i,j}$ is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron.\n",
    "- $x_i$ is the $i^{th}$ input value of current training instance.\n",
    "- $y_j$ is the target output of the $j^{th}$ output neuron.\n",
    "- $\\hat y_j$ is the output of the $j^{th}$ output neuron.\n",
    "- $\\eta$ is the learning rate.\n",
    "\n",
    "The decision boundary of the TLUs are linear, so Perceptron are incapable of learning complex patterns. Overall this algorithm is called *Perceptron convergence theorem*.\n",
    "\n",
    "You can use Sklearn's `Perceptron` to do this. However, it is same as the SGD classifier of Sklearn with hyperparameters `loss=\"perceptron\", learning rate=\"constant\"`.\n",
    "\n",
    "Perceptrons do not output a class probability as Logistic regression. Rather, they output a hard threshold.\n",
    "\n",
    "In 1969 monograph *Perceptrons*, there are weakness of Perceptrons like it cannot solve XOR problem. \n",
    "\n",
    "It turns out limitations of Perceptron can be replaced by stacking multiple Perceptrons. The resulting ANN is called *Multilayer Perceptron* (MLP). An MLP can solve XOR problem easily.\n",
    "\n",
    "\n",
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sought-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "close-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length and width\n",
    "y = (iris.target == 0).astype(np.int) # Iris setosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "substantial-wiring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "electronic-liechtenstein",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-footwear",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron and Backpropagation\n",
    "\n",
    "A MLP is composed of one (passthrough) *input layer*, one or more layers of TLUs, called *hidden layers*, and one final layer of TLUs called the *output layer*. \n",
    "\n",
    "The layers close to input layer are usually called *lower layers*, and one close to output layer called *upper layers*.\n",
    "\n",
    "Every layer except output layer includes the bias neuron and is fully connected to the next layer.\n",
    "\n",
    "> The signal flows only in one direction (from the inputs to the outputs), so this architecture called *feedforward neural network* (FNN).\n",
    "\n",
    "When an ANN contains deep stack of hidden layers, it is called *deep neural networks* (DNN). \n",
    "\n",
    "In 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a paper that introduces the *backpropagation*. Which is ground breaking and still in use. In short it is Gradient Descent using an efficient way to calculate gradients automatically.\n",
    "\n",
    "> Automatically computing gradients is called *automatic differentiation*, or *autodiff*. The autodiff technique used in backpropagation is called *reverse-mode autodiff*. It is fast and precise, and well suited for when function to differentiate has many variables (e.g. connection weights) and few outputs (e.g. loss).\n",
    "\n",
    "Backpropagation Algorithm:\n",
    "\n",
    "- It handles one mini-batch at a time, and it goes through full training set multiple times. Each pass though full training set is called an *epoch*.\n",
    "- Each mini-batch is passed through the network from input layer through hidden layers to the output layer, this is called *forward pass*. The results for every layer is preserved in order to compute gradients.\n",
    "- Next, algorithm measures the network's output error (by loss function).\n",
    "- Then it computes how much each output connection contributed to the error analytically by applying chain rule.\n",
    "- Then it measures how much of these error contributions came from each connection in the layer below, again using chain rule, working backward until the algo reaches to the input layer. This is called *backward pass*.\n",
    "- Finally, the algorithm performs Gradient Descent step to tweak all the connection weights in the network, using error gradients it just computed.\n",
    "\n",
    "> It is important to initialize all the hidden layers' connection weights randomly, or else training will fall. For example, if you initialize all weights and bias to zero, then all neurons in a given layer will be perfectly identical, and backpropagation will affect them in a same way, so they will remain identical. Network treat a layer as only one neuron. If you initialize the weights randomly, you *break the symmetry* and allow backpropagation to train a diverse team of neurons.\n",
    "\n",
    "In order to work this algorithm properly, there is a key change in MLP: they replaced the step function (non-differentiable) to the logistic (sigmoid) function,\n",
    "$$\n",
    "\\sigma(z) = 1 / (1 + \\exp(-z))\n",
    "$$\n",
    "Sigmoid is differentiable and ranges from 0 to 1. There are others we can use:\n",
    "\n",
    "- *Hyperbolic tangent function*: $\\tanh(z) = 2\\sigma(2z)-1$\n",
    "  - It is like sigmoid, but ranges from -1 to 1. Which make layer's output more centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "- *The Rectified Linear Unit function*: $ReLU(z) = \\max(0, z)$\n",
    "  - It is continuous but not differentiable at 0. But in practice it works well and computed very fast, which makes it default for today's architectures. It's derivative is 0 for z < 0 and has no upper limit which makes it more usable.\n",
    "\n",
    "We need activation functions because if we chain linear transformations we get a linear transformation. So if you don't have non-linearity between layers, then deep networks is equivalent to single layer network.\n",
    "\n",
    "## Regression MLPs\n",
    "\n",
    "We can use MLPs for regression tasks. To predict single values, you just need an output neuron, and for multivariate regression, you need one output neuron for every dimension (value).\n",
    "\n",
    "In general, you don't need any activation function for the output layer. If you want positive output then you can use ReLU or *softplus function*, which is smooth variant of ReLU: $softplus(z) = \\log(1 + \\exp(z))$.\n",
    "\n",
    "Finally, if you want output in a specific range, you could use sigmoid or tanh and scale it up to your specific range.\n",
    "\n",
    "The loss function is generally mean squared error. But if you have lots of outliers you could use mean absolute error. Alternatively, you can use the *Huber loss*, which is combination of both.\n",
    "\n",
    "> Huber loss is quadratic when error is smaller than a threshold $\\delta$ (typically 1) but linear when greater.\n",
    "\n",
    "## Classification MLPs\n",
    "\n",
    "They mostly used in classification tasks. For a binary classification, you can use an output neuron with logistic function. The output will be in range 0 and 1, also gives you probability of 1 (positive class).\n",
    "\n",
    "MLPs can also easily handle multilabel binary classification. You can use two neurons with logistic function if you have 2 labels. The probability of both does not add up to 1, means label are independent.\n",
    "\n",
    "If your labels are dependent (e.g. MNIST), then you use output neuron for each label and apply *softmax function* to whole output layer, which produce a probabilities which add up to 1. This is called multiclass classification.\n",
    "\n",
    "For loss, since we are pretending the output is probability distribution, the *cross-entropy loss* (also called the log loss) is generally a good choice.\n",
    "\n",
    "## Implementing MLPs with Keras\n",
    "We'll be using Keras, which is a Deep Learning API that allows you to easily build, train, evaluate, and execute all types of neural networks. It was developed Francois Chollet as part of a research project, and it made open source in March 2015.\n",
    "\n",
    "Tensorflow is a framework that multi functionality to perform Deep Learning, Tensorflow 2 has made Keras as its official API.\n",
    "### Buildind an Image Classifier using the Sequential API\n",
    "Install tensorflow and check the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fantastic-astronomy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "annoying-collective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-making",
   "metadata": {},
   "source": [
    "#### Using Keras to load the dataset\n",
    "Keras provides some utility functions to fetch and load common datasets. Let's use Fashion MNIST, it is similar to MNIST but contain cloth images instead of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "about-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-heating",
   "metadata": {},
   "source": [
    "Keras' Fashion MNIST gives 28x28 array for each image, type as integers, and ranges from 0 to 255 as pixel intensities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "appreciated-great",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anticipated-contact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-lying",
   "metadata": {},
   "source": [
    "It already splits dataset into train and test, but not validation set. We'll use Gradient Descent so we must scale input features by dividing them by 255.0. Now, numbers are in range 0-1 and become float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "disturbed-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-census",
   "metadata": {},
   "source": [
    "For Fashion MNIST, we need the list of class names to know the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "satellite-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "              \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "studied-bidder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-republican",
   "metadata": {},
   "source": [
    "#### Creating the Model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "promising-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-extent",
   "metadata": {},
   "source": [
    "Let's go though code:\n",
    "\n",
    "- `Sequential` model is the simplest Keras model to compose a single stack of layers connected sequentially.\n",
    "- Next, the first layer added to the model. It is a `Flatten` layer, whose role is to convert input shape as 1D array. It does not accept any parameters, since it is first layer you need to pass `input_shape`. Alternatively, you could add `keras.layers.InputLayer` with passing `input_shape=[28, 28]`.\n",
    "- Next, we add a `Dense` layer with 300 neuron number passing to it. It has an activation function ReLU. It manages all weights and bias when receives inputs.\n",
    "\n",
    "- Then we add another `Dense` layer with 100 neurons and activation as ReLU.\n",
    "- Finally, we add a `Dense` output layer with 10 neurons (one per class), using the softmax activation function.\n",
    "\n",
    "> Specifying `activation='relu'` is equivalent to `activation=keras.activations.relu`.\n",
    "\n",
    "Instead of adding layers one by one, you could pass a list of layers when creating th `Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vertical-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-butterfly",
   "metadata": {},
   "source": [
    "The model's `summary()` method displays all the model's layers, including their name (which automatically created if not mentioned), output shape and number of parameters with total parameters, trainable and non-trainable parameter numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "characteristic-attendance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-feeding",
   "metadata": {},
   "source": [
    "However, you can use `keras.utils.plot_model()` to generate an image of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "italian-morning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-string",
   "metadata": {},
   "source": [
    "Note that, `Dense` layer has lot of parameters. This gives the model a lot of flexibility to fit the data, but it also means that model could overfit the data. Specially when you don't have lots of data.\n",
    "\n",
    "You can easily get a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "combined-wyoming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x7fcbb95e46d0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fcbb95e4250>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fcbb94c2280>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fcbbb9bc250>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "solved-renaissance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "english-animal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-superintendent",
   "metadata": {},
   "source": [
    "All the parameters can be accessed by `get_weights()` and `set_weights()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spoken-kitty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00754258, -0.04366939,  0.01029805, ...,  0.00372275,\n",
       "          0.00243229, -0.06694706],\n",
       "        [ 0.07192007,  0.00896271, -0.0303936 , ..., -0.05347449,\n",
       "          0.05146894,  0.07180741],\n",
       "        [-0.0474909 ,  0.06719105,  0.01919688, ...,  0.06146406,\n",
       "         -0.05107195, -0.05320762],\n",
       "        ...,\n",
       "        [ 0.01187094, -0.05321714, -0.03532935, ..., -0.02335224,\n",
       "          0.02986118, -0.02064291],\n",
       "        [-0.01759655,  0.02915387, -0.05630223, ...,  0.04567022,\n",
       "         -0.01391109,  0.06617334],\n",
       "        [ 0.02651229,  0.03345647, -0.06416623, ...,  0.0065814 ,\n",
       "         -0.0402001 , -0.02786959]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias = hidden1.get_weights()\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "proved-butterfly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 300), (300,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-saturn",
   "metadata": {},
   "source": [
    "`Dense` layer initialize the connection weights randomly, and biases to zeros. If you want to change initialization, you can set `kernel_initializer` (kernel is another name of matrix of connection weights) or `bias_initializer` when creating the layer.\n",
    "\n",
    "> The shape of the weigthts matrix depends on the input shape. That's why we need to specify input shape at start. But if you don't that's okay Keras will wait until you pass it the data or you can call its `build()` mehod. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-bobby",
   "metadata": {},
   "source": [
    "#### Compiling the Model\n",
    "After a model is created, you must call its `compile()` method to specify loss and optimizer, optionally you can specify some metrics to compute during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "genetic-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-being",
   "metadata": {},
   "source": [
    "> Using `loss='sparse_categorical_crossentropy'` is equivalent to `loss=keras.losses.sparse_categorical_crossentropy`, same goes for optimizers and metrics.\n",
    "\n",
    "We use `sparse_categorical_crossentropy` loss because we have sparse labels (i.e. for each instances, there is just a target class index, from 0 to 9). If we had one-hot vectors then we'll use `categorical_crossentropy` loss instead. If we were doing binary classification binary classification, then we should use `sigmoid` activation function in the output layer, and we would use the `binary_crossentropy` loss.\n",
    "\n",
    "> If you want to convert sparse labels to one-hot vector labels, use `keras.utils.to_categorical()` function. To go other way around, use the `np.argmax()` function with `axis=1`.\n",
    "\n",
    "For optimizer `\"sgd\"` means that we will train model using simple Stochastic Gradient Descent.\n",
    "\n",
    "> When using SGD optimizer, it is important to tune the learning rate. So, you will generally want to use `optimizer=keras.optimizers.SGD(lr=???)` to set the learning rate, which defaults to `lr=0.01`\n",
    "\n",
    "Finally, metrics is `\"accuracy\"` which is calculated during training and evaluation.\n",
    "\n",
    "#### Training and evaluating the Model\n",
    "\n",
    "For this we simply call its `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "stock-tobacco",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.0196 - accuracy: 0.6644 - val_loss: 0.5172 - val_accuracy: 0.8292\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5175 - accuracy: 0.8211 - val_loss: 0.4554 - val_accuracy: 0.8428\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4538 - accuracy: 0.8386 - val_loss: 0.4163 - val_accuracy: 0.8576\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4305 - accuracy: 0.8490 - val_loss: 0.4065 - val_accuracy: 0.8588\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4076 - accuracy: 0.8562 - val_loss: 0.3821 - val_accuracy: 0.8654\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3819 - accuracy: 0.8648 - val_loss: 0.3863 - val_accuracy: 0.8632\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3787 - accuracy: 0.8675 - val_loss: 0.3724 - val_accuracy: 0.8700\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3578 - accuracy: 0.8728 - val_loss: 0.3587 - val_accuracy: 0.8718\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3468 - accuracy: 0.8772 - val_loss: 0.3467 - val_accuracy: 0.8776\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3308 - accuracy: 0.8809 - val_loss: 0.3436 - val_accuracy: 0.8774\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3296 - accuracy: 0.8840 - val_loss: 0.3517 - val_accuracy: 0.8716\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3195 - accuracy: 0.8872 - val_loss: 0.3281 - val_accuracy: 0.8794\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3103 - accuracy: 0.8887 - val_loss: 0.3304 - val_accuracy: 0.8796\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3015 - accuracy: 0.8891 - val_loss: 0.3198 - val_accuracy: 0.8840\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2995 - accuracy: 0.8933 - val_loss: 0.3187 - val_accuracy: 0.8808\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2891 - accuracy: 0.8957 - val_loss: 0.3138 - val_accuracy: 0.8854\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2845 - accuracy: 0.8992 - val_loss: 0.3190 - val_accuracy: 0.8828\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2792 - accuracy: 0.8999 - val_loss: 0.3301 - val_accuracy: 0.8788\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2710 - accuracy: 0.9017 - val_loss: 0.3113 - val_accuracy: 0.8858\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2692 - accuracy: 0.9031 - val_loss: 0.3112 - val_accuracy: 0.8858\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2607 - accuracy: 0.9069 - val_loss: 0.3134 - val_accuracy: 0.8848\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2541 - accuracy: 0.9081 - val_loss: 0.3000 - val_accuracy: 0.8900\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2510 - accuracy: 0.9082 - val_loss: 0.2979 - val_accuracy: 0.8926\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2510 - accuracy: 0.9091 - val_loss: 0.3007 - val_accuracy: 0.8916\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2431 - accuracy: 0.9129 - val_loss: 0.3111 - val_accuracy: 0.8794\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2407 - accuracy: 0.9126 - val_loss: 0.2924 - val_accuracy: 0.8916\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2313 - accuracy: 0.9175 - val_loss: 0.3050 - val_accuracy: 0.8862\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2308 - accuracy: 0.9157 - val_loss: 0.2947 - val_accuracy: 0.8938\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2286 - accuracy: 0.9187 - val_loss: 0.2990 - val_accuracy: 0.8900\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2231 - accuracy: 0.9199 - val_loss: 0.2909 - val_accuracy: 0.8962\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                   validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-sigma",
   "metadata": {},
   "source": [
    "We pass it the input features (X_train) and the target classes (y_train), as well as number of epochs to train (defaults to 1).  We also pass it validation set (which is optional), Keras will measure the loss and other metrics on this set at the end of each epoch, which is very helpful to see. \n",
    "\n",
    "If performance on training set is much better than on the validation set, your model is probably overfitting the training set.\n",
    "\n",
    "> Instead of passing a validation set, you could set `validation_split` to the ratio you want. For example, `validation_split=0.1` last 10% of training data will be used for validation.\n",
    "\n",
    "If training set was very skewed, with some classes being over-represented than other, you could use `class_weight` argument in `fit()`, this would give extra weight to under-represented class while calculating the loss. If you need per-instance weights, use `sample_weight` argument, it would be useful if some instances were labeled by experts while other labeled by crowdsourcing platform: it gives more weight to the expert ones. You can also pass sample weight (not class weight) in `validation_data` tuple as third argument.\n",
    "\n",
    "The `fit()` method returns a `history` object which contains the training parameters (`history.params`), the list of epochs (`history.epoch`), and most important dictionary (`history.history`) containing loss and extra metrics it measured. You could plot history using pandas `DataFrame` to get learning curves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "enclosed-provision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABNT0lEQVR4nO3deXxU9b3/8dd3tsxMtsm+kISEfQtrQHAFrTturUipWsTtarW02mpta1tva3t7tXa591qrtdalKlqXutSl+pOAVFBW2XcCJED2fZLM9v39cSaTBCYQIDDJ5PN8PM7jLHPmzHe+DLz5fs/3nKO01gghhBAickyRLoAQQggx0EkYCyGEEBEmYSyEEEJEmISxEEIIEWESxkIIIUSESRgLIYQQEXbMMFZKPaOUqlBKbezmdaWU+h+l1E6l1Hql1OTeL6YQQggRvXrSMn4WuOQor18KDA9OtwNPnHyxhBBCiIHjmGGstV4K1Bxll6uA57VhBeBSSmX1VgGFEEKIaNcb54wHAfs7rZcGtwkhhBCiByyn88OUUrdjdGXjcDim5Obm9tqxA4EAJpOMRzuc1Et4Ui/hSb2EJ/USntRLeN3Vy/bt26u01mnh3tMbYVwGdE7VnOC2I2itnwKeAigqKtKrVq3qhY83FBcXM3PmzF47XrSQeglP6iU8qZfwpF7Ck3oJr7t6UUrt7e49vfFfmreBbwZHVU8H6rXWB3vhuEIIIcSAcMyWsVLqZWAmkKqUKgV+BlgBtNZ/At4DLgN2Am5gwakqrBBCCBGNjhnGWut5x3hdA3f1WomEEEKIAUbOvAshhBARJmEshBBCRJiEsRBCCBFhEsZCCCFEhEkYCyGEEBEmYSyEEEJEmISxEEIIEWESxkIIIUSESRgLIYQQESZhLIQQQkSYhLEQQggRYRLGQgghRIRJGAshhBARJmEshBBCRJiEsRBCCBFhEsZCCCFEhFkiXQAhhBDihPm90NZoTJ6m4HITtDV0rHvc4Gs1Jr8nuNwWnB+23vl1qxPu/uK0fA0JYyGEECdG605h1toRYt6WYLi1hF/3tRmhF/AZc7/XmALe4Lqv03Kn13xtRsi2NXWEr6+1Z2VVJrDYwRLTMTfHdF13pgTXg9tiEk5t/XUiYSyEEP1ZIABeN3iajXDyNHda77R8xDa3sb/XDd5WIxgD3uDcHwzKzuvty8F1v5fz/F5YEji58isTmKxgtoHZYsxNVjAHp87LFjsk5EBMPMTEGXNbfMe6LbitfWpft8Ua7+/DJIyFEOJY2luAnVtu/vb19qmbll7ADzoA2t+xHPB3Xdd+I1TbtwW8RusvFLDB5bZOy6F5M6B7/l0sdqP71RZrTFYnWB1Ga9AUCyZLMATNxrLJGpybO71mrO/bf4DBw0aCJfh+qyPYygxOVnv4dbMtOAU/R0gYCyH6Ma2NAOxyDrCto8vU23rktvYAC7US25fdh7Usm0LbZ/paYMlp/m7KZLTsbLEd85h4SMjutK19e3BuDQaszWm8dnjo2mJ7Nfz2FBcz+NyZvXa8gUzCWAjR+7Q2Wobe4MAZb0vH3NtinDv0tq+7OyZPN8velo6QbH9Pe8Dqk+gm7RJYcUaI2RMgPrNTEDopKasgf+jwI7tQw65bOrX6gi1KZQ52xwaXTabg/PDXgnOT1WhlKtV7fyaiT5MwFmKg8bVBa0NwIExjp3kj2WVrYfmmblqabZ2m7lqiwfOPvpYTC0llCrbugl2nnZcTBhlzm9PoFg11gcZ06gqNOfbcFt9xDFPPru4sKS4mX1qAUU0HAvgqq/AeKMN74ADeAwfQLa2kLfz2afl8CWMh+hOtje7T1npoqYPWuvDztoZg4LaHbUfg4vd0e/gRADs6bThasJljwJ7Ysa39fKHVEWa5PTyDc6uz0+vOYOA6jRZllLYGtceDv7mZQLObgLuZQPtyc/MRk/Z4wGJGWawoiwVltaAsFrBYut9mtWB2ubCPGYPJbj+130VrvPv2Yd22jba8PCzp6Zjj4nrt+P6mJjwle/GUlHSZ/PX1mBMTMbtcHfNwU5IxN8XGooK/p4DHg+/gwWDQHgwFbmg6dAi83i7lsKSlkfrtu0PHOJUkjIU4Hu3dr35Px+RrC24LXq7ha38tuP2EX/cYXbKdg7a13jhH2i1lBKQ90ehujUkwWpQxo4zzjfaE4EjT4Gvto06D2/+9aj1nnXt+MGytURuMvUFrjb+uDl9FJb6KCmOqNObeigp8FZX4q6s7Avawf+i7ZbFgstnQfj/a5wO//7jKpaxW7OPG4SyagmPyZJyTJ2NOTDyBb9hB+/20bt1Ky+rVuFevwb1mNf7KKpKB3b/7PQAmpxNLRgaWjAysGelY0o1lS3oa1uB2S2qq8R8IINDWhnffPtraw3Zve/juxV9V1ekLKaxZWdjy87ENHoy/oQF/XR2evXvx19URaGzsvuAWi/HdTQp/VbXx97fTcS1paVizs3EUFpJwycVYs7OxZGVhzc7Gmj0Ic1zsSdXb8ZAwFgOXxw0tNeCu6Zi7q6Gltuu2zvPW+t4vR2hkqTFpsxW/z4a/zUogYMWUkIg5MRtTXgqmuGRwuMDu6pjbEzuWYxKO2fWq/X78NTX4qqvxVVfjr67GV7UTX3UV9h07KF+7HVNsbHByhpbNoW0dk7LbT2mroT3wvPv349m3H2+pMfdVVZ7YAZVCmY0WZXvrsktrM9w2qwXnjh0cWvbvjtCtqMBXWRk2YM2JiVjS07Gkp2PLH4w5Ls6oL6fziPoLO9lsXesgEACfD9158vrA5z1im6+inJY1a3CvWk31s8/Bn58GpYgZPhzHlMk4pxThnDIZa1bWUasp0NpKy/r1RviuWk3LunUEmpsBsGZnEztjBs7JU9hcV8u4QTn4KsrxlpfjK6/AV16Oe+UqvJWVR7Q0MZmwpKSA1YLv4KEu4WhOTcWWP5i4medhGzwYW34+Mfn5WPPyMMXEdFtW7fOFArpjqu+yrv0+rFnZwZANhm1mJuqwuo4kCWPRf/m9Rldsa11Hq7GH0znN1VDcfXcttnhwJoEjGZzJkFRgzO2JwZsF2IxuWrM1tK5N1k7XS9q6DOYJBBT+hhb89c34Gprw1TXgr2vCX1uHr6YGf001vuoafDXV+Gtqwd8CtAQLUx0qlrLbMcXHYY5PwBwfjymhfR6POT4BU3w85oR4lNWKr7oGf3UVvqpq47hVwfCtre3aQmg/ts2G3Waj9ouV6NYe3kjBZDICJC4Oc5ILiysJc1L75MKclITF5eq0LcnoPuz0j6v2+/EePBQM2n2h4PXs34d3f+kRLR9LWhqW9PQen+/tIhAItji94D0s4Hw+8HYEHIGOc97xQH18fDBk03BOLTKW09JDwWtJT8OSlnbU4DgRymQCm62HwTGW+PPPN75qSwstGzaEArXhrbepe3kRYASqo2gKzslTcBZNwZySQsvadbSsCYbvpk1GkAaDPOHKK8IGube4mMSZM8OWRAcC+Gtr8ZUHg7qi0liuKEe3eUKBa0yDT7ibW1ksWJKTsSQnn9D7+woJYxER/oYGvPt24du/B2/pPvzVlR2DgfzBydeK8rWB/7C7+/haUMEuXWXWWBx+rE4/Fqcfs0139Kwqc0eXrcNlzOMzwZ5IWWUDeSMmGAHbHrjBubYn4aupx7NnD56SPXj27KFtzx48JevxHTpktFS07jqdIFNsLOaUFCzJyVhzcnCMH485ORlLSjLm5BRMTieB5ib8DQ0EGhvxNzQa88ZGAsHWgHffPvyNjfgbGsDXtQtbOZ1YUlKwpKRgHZyHY/JkLCkpmFNTsKSkGp+TkoIlNRVTXBxLlixh5syZaJ+PgNttTGHOafq7rLuNMtXV4a+txVNWir+2jkBDQ/ff2+nEnJQEFjPeAwe7tqCsVmyDBmHNzcU5cRLWvFxseXnYcnOx5uRgcjhOuL6Phw4EQgG99N//ZuZFF52Wz+0tJoeD2GnTiJ02DTBakK3bttGyeg3u1atp/mw5DW+/0/VNViuOceNIuWn+SXdxq2Ar2JKSgn3MmJP9OlFPwlicOJ/nsO7dGmipJVBXhffQIbwVVfgqa/FW1+OtbcZX14a3wYuvWRPwnkzXpglwBKeuVIzNOF+VmYk1OweLKxNrZhbWrExjW1YWprg4di9ZQs7kqcb5qt0leEq+NMJ3zx48JSUE3O6OYzoc2PLzcYwbi/WiC4OXoChQGN2eSgEquO2w7cHXlNXaJWSNeXKvtqK01ujWVvwNjWivF0tyEian84SOpSwWzAkJmBNO/HaA2uvFX19vtI5qa/HXGmHtr6sNbcPnI+Gii43AzTUmS2Ymyhz5G0Eok8lojbZP/ZyyWHCMHYtj7FiSv3ljaBCWe9VqfDXVOCdOxF5YeMoHf4nwJIyF0bJra8RftR/vnh14SnbjO3gA3VRPwN2IdjcTaGlGt7jRbW0E2trQbR4C3gDarwj4VfDGQYqA10TAe2T3oTnWjDXRRkx2ArEp8VhTk7Cmp2LJTMeamY05MwtlT0LHJARvYZdgdP8GW55G41Mf0SINtLUZXV8HD+E7dNDo7jx0CN/BgzSvWIGvoqJLdyMYrbJUi4VtnVtuSmEdNAhbfj6JU6ZgK8gnpqAAW0EBlvR0o6uwj1NKoRyO09ZyPBZltWJJTcWSmkrvdtyK3qCUMrqKBw+OdFEEEsZ9ntaaQEMDvuoaTDE2lNOJyeFAxcR0P3BGa2gqh9q9pFV8Cit3grsW3VRttFgPVeKpqMVb1YS3phVPfQBvswl/W/jWiDJplEWhrCZMVjPK5sAUk4iKt2Gy2zE7nChHLCZnHKYEF5asHKw5eVizsoyRienpPR4ocbztZTNgzcjAMX58+Krw+fBVVnaE9aFyvIcOcmDnLvKmTcNWUICtwBil2dvn+oQQoqckjPuAgNuNp7QUb2kZ3rIyvKWleMqC66WlBJqajnyTyWSEc4wFk9WEyaIxmXwo5cGkWzCZfSiLJhnY12zB02TG6zaD7hR3JoU1KR5begL2zGRsWZlYc3KwDs7HmluAKTkblZiBsvXfkFIWC9asrOCgk0mh7VuLi0ntZuCJEEKcbhLGJyjgdhvdtV6vMQKzh1OgudkI3LIyPMGw9dfUdDm2stuxZmdiS0/COWQ81gQTFksLuqGSQH0VgYZaAm0eo4vYF5y0DY2DAA68/jh0myLQpPF4fTjz8nBMHkxC3mBsuTlYc3Kx5eZgycgIXfMnhBAicuRf4h7SHg/uNWtoWrKUpk+X4tm568QPZrFgzUzHlu7CPikfa3wBVqcHm60Bq6kSs+cAyre763uUEwYPhqRh4BoMSYPBldexbA8/4rG4uJiZ0gIUQog+TcL4KLwHD9K09FOaPl2K+7PlBNxulNWKc2oRibNnY4qNQ1mtxmSzdixbLB3LgVao3YWq2Y6q3oypbjsWXylK7ev6YY40SMyBxDGQeDG4ciEx19jmygNHktwNSQghopSEcSfa68W9di3NS5fStPRT2rZvB8CSnUXClVcQd+65xJ5xBqbYbm6R5vNA+QYoXQ1lq6FsFVTv7Hg9dQSMOQuS8oMh2ylwrX1jBKwQQojTb8CHsbe8guZln9K0ZCnNn31mDJayWHBOmUL6ffcRd+452IYNO3LkstZQszsYuquhdBUcWt9xE/7YdMgpggnzYNAUGDS5265kIYQQA9uAC2N/UzPulV/QvHw57uUraNthPKLGkpFBwqWXEnvuOcTOmHHkrdm0hsptsGcJ7FkKez8zbnIBxtNmsibCGf8RDN4io7Ur3cpCCCF6IOrDWHs8tKxfT/Nny2levpyWDRvA50PFxOCcMiXY/XweMSOGH9n6rdljBG/71FxhbHflwcjLjJZvThGkjQZz1FelEEKIUyTqEkQHArRt326E74rluFetRrvdYDJhHzeOlFtuIXbGDByTJh55k4eGg1DyaUfrty44yCouA4acBwXnGlNS/mn/XkIIIaJXVISxt7wCx7JllL39Ns0rPg9dt2sbMgTX1VcTe+YMnNOmHXmf3YAftr0Hu4uN8K0yBmxhd0H+2XDmQiN8U0dIl7MQQohTJirCuGnpEhL+9iLutDTizjkb5/QZxM6YjjUz8+hv/NeDsOKPYI2FwWfCpBuN8M0sNB4GIIQQQpwGURHGCRdeyEafj7O//vWeP+h8z1IjiKcsgEsfAUv/fyqLEEKI/qnvP4qmB8wuF/6srJ4HcWs9vHknpAyDi38lQSyEECKioqJlfNzefwAaD8AtH4HtxJ73KoQQQvSWqGgZH5ct78CXL8E53zMuSxJCCCEirEdhrJS6RCm1TSm1Uyn1QJjX85RSi5VSa5VS65VSl/V+UXtBUwW88x3ImgDn3h/p0gghhBBAD8JYKWUGHgcuBcYA85RSYw7b7UHgVa31JODrwB97u6AnTWt4eyG0NcE1T8p5YiGEEH1GT1rG04CdWuvdWmsPsAi46rB9NNB+EW8icKD3ithL1r0I29+HC34K6aMjXRohhBAiRGmtj76DUtcCl2itbw2u3wicobW+u9M+WcC/gCQgFviK1np1mGPdDtwOkJGRMWXRokW99T1oamoi7vD7SQfZW8opWvUdmuKGsm7iL0ANnFPlR6uXgUzqJTypl/CkXsKTegmvu3qZNWvWaq112MFKvTWaeh7wrNb6MaXUDOAFpdQ4rXWg805a66eApwCKiop0bz70vri4mLDHCwTgudlgtuC66SVmJg3utc/sD7qtlwFO6iU8qZfwpF7Ck3oJ70TqpSdNxDIgt9N6TnBbZ7cArwJorZcDdiD1uEpyqqz4I+z9N1z6axhgQSyEEKJ/6EkYrwSGK6UKlFI2jAFabx+2zz7gAgCl1GiMMK7szYKekIot8P9+DiMvh4nXR7o0QgghRFjHDGOttQ+4G/gQ2IIxanqTUurnSqkrg7t9D7hNKfUl8DJwkz7WyehTzeeBN26HmHi44g/yoAchhBB9Vo/OGWut3wPeO2zbTzstbwbO6t2inaSlj8Ch9fD1lyAuLdKlEUIIIboVncOK96+ETx8zuqZHXR7p0gghhBBHFX1h7GmGN/8DEgbBJf8V6dIIIYQQxxR9D4r46GdQswvmvwv2xEiXRgghhDim6GoZ7/x/sPLPMP0uKDgn0qURQggheiRqwtjibYK37oLUkXDBTyJdHCGEEKLHoqabeviOJ6G5Eua9DFZHpIsjhBBC9Fh0tIw3vkFGxVI47weQPSnSpRFCCCGOS3SEccZYDmRdBGffG+mSCCGEEMctOsI4bSTbR94F5qjpdRdCCDGAREcYCyGEEP2YhLEQQggRYVERxlprKt2BY+8ohBBC9EFREcYvrNjLfUtbqGhojXRRhBBCiOMWFWE8Ntu47eXa/XWRLYgQQghxAqIkjBMwK1gnYSyEEKIfioowtlvN5MWbWLuvNtJFEUIIIY5bVIQxwBCXiQ2l9fgDOtJFEUIIIY5L1ITxUJeZZo+fHRWNkS6KEEIIcVyiJoyHJBpfZd2+usgWRAghhDhOURPGGU6Fy2mVQVxCCCH6nagJY6UUE3JcrJWWsRBCiH4masIYYGKui+0VjTS1+SJdFCGEEKLHoiuM81xoDetL6yJdFCGEEKLHoiuMc1yA3PxDCCFE/xJVYZwUa6MgNVZGVAshhOhXoiqMwThvvHZ/HVrLzT+EEEL0D1EZxpWNbRyolyc4CSGE6B+iMoxBbv4hhBCi/4i6MB6dlYDNYmLdfnlohBBCiP4h6sLYZjExLjtBRlQLIYToN6IujAEm5iaxoawerz8Q6aIIIYQQxxSdYZznotUbYNsheYKTEEKIvi8qw3hScBDXWumqFkII0Q9EZRjnJDlIibXJiGohhBD9QlSGsVKKSXkuGVEthBCiX4jKMAbjeuNdlc3Ut3gjXRQhhBDiqKI4jJMA+FLOGwshhOjjojaMx+cmopQ8wUkIIUTfF7VhnGC3MjQtTsJYCCFEnxe1YQzGeeN18gQnIYQQfVxUh/GkPBc1zR7217REuihCCCFEt6I6jCeGbv4hlzgJIYTou6I6jEdmxOOwmlkrN/8QQgjRh0V1GFvMJgoHJcogLiGEEH1aVIcxGA+N2HyggTafP9JFEUIIIcLqURgrpS5RSm1TSu1USj3QzT7XKaU2K6U2KaVe6t1inriJuS48/gBbDsoTnIQQQvRNxwxjpZQZeBy4FBgDzFNKjTlsn+HAD4GztNZjge/2flFPzKQ8FwDr9skgLiGEEH1TT1rG04CdWuvdWmsPsAi46rB9bgMe11rXAmitK3q3mCcuK9FBRkKMPE5RCCFEn9WTMB4E7O+0Xhrc1tkIYIRS6t9KqRVKqUt6q4C9of3mH0IIIURfZOnF4wwHZgI5wFKlVKHWuq7zTkqp24HbATIyMiguLu6lj4empqZuj5fg9bC32ss7/1pMvE312mf2B0erl4FM6iU8qZfwpF7Ck3oJ70TqpSdhXAbkdlrPCW7rrBT4XGvtBfYopbZjhPPKzjtprZ8CngIoKirSM2fOPK7CHk1xcTHdHS8mt5q/b19BXN5YZo5K77XP7A+OVi8DmdRLeFIv4Um9hCf1Et6J1EtPuqlXAsOVUgVKKRvwdeDtw/b5B0arGKVUKka39e7jKskpND4nEZNCzhsLIYTok44ZxlprH3A38CGwBXhVa71JKfVzpdSVwd0+BKqVUpuBxcB9WuvqU1Xo4xUbY2FERjxrZUS1EEKIPqhH54y11u8B7x227aedljVwb3Dqkyblufjn+oMEAhqTaWCdNxZCCNG3Rf0duNpNzHXR0OpjT3VzpIsihBBCdDGAwjgJgHXy0AghhBB9zIAJ42HpccTazHK9sRBCiD5nwISx2aQYn+OSZxsLIYTocwZMGIMxiGvrwUZavfIEJyGEEH3HgArjibkufAHNxrL6SBdFCCGECBlYYdz+BCc5byyEEKIPGVBhnB5vZ5DLIXfiEkII0acMqDCG4BOc5PImIYQQfciAC+NJeS7K6lqoaGyNdFGEEEIIYACG8cRcFyA3/xBCCNF3DLgwHjcoEYtJySAuIYQQfcaAC2O71cyorHgJYyGEEH3GgAtjMLqq15fW4w/oSBdFCCGEGKhhnERTm4+dFU2RLooQQggxMMN4UujmH3KfaiGEEJEXFWF8oOkAr9W8hi/g69H+BSmxJNgtct5YCCFEnxAVYbzy0EqWNC7hN6t+06P9TSbFhFwXa+XyJiGEEH1AVITxVcOuYlb8LF7c8iJ/3/73Hr1nUq6L7eWNNLf1rDUthBBCnCpREcYAVyddzdmDzuZXK37FFwe/OOb+E/NcBDSsL5UnOAkhhIisqAljkzLxyLmPkJeQxz3F97CvYd9R95+Q4wLkCU5CCCEiL2rCGCDeFs//nf9/KKW4+5O7afA0dLtvSlwMg1OcMqJaCCFExEVVGAPkJuTyu5m/Y3/Dfu5fcv9RR1hPzHVJy1gIIUTERV0YA0zNnMqD0x/k3wf+zWOrHut2vymDkyhvaONvK/aextIJIYQQXVkiXYBT5WsjvsbOup38bcvfGOIawpwRc47Y57qiXIq3VfLgPzZS1dTGdy4YjlIqAqUVQggxkEVly7jd94q+x1mDzup2hLXdaubJG6fwtck5/P7jHTz4j41yv2ohhBCnXVSHscVk4dFzHyUvIY97l9wbdoS11WziN3PGc8d5Q3nx833c9eIaWr3+CJRWCCHEQBXVYQwdI6wBvv3Jt2n0NB6xj1KKBy4dxU9mj+GDTYeY/8wX1Ld4T3dRhRBCDFBRH8bQMcJ6X8M+7ltyX7cjrG85u4A/fH0ia/bVMvfJ5VQ0tJ7mkgohhBiIBkQYgzHC+sfTf3zMEdZXTRzEX+ZPZV+Nm68+8Rm7K+Uxi0IIIU6tARPGANeOuJYbRt/A37b8jde2v9btfueOSOPl26bj9vi59k/L+VKuRRZCCHEKDagwho4R1r9c8UtWHlrZ7X4Tcl28dscMnDYz8/68gqXbK09jKYUQQgwkAy6MO4+wvqf4HvY37O923yFpcbxx55kMTonl5mdX8ta6stNYUiGEEAPFgAtj6DrC+q5P7uJA04Fu901PsPPKf0xnyuAkvrNoHX9Ztud0FVMIIcQAMSDDGIwR1r+f+Xsq3ZVc9+51LC1d2u2+CXYrz908jUvHZfKLdzfz6/e3orXcHEQIIUTvGLBhDFCUWcQrs18hKzaLu/7fXfx+9e+7vezJbjXzf9+YzA3T8/jTkl1879Uv5VpkIYQQvWJAhzFAXkIeL1z6AteOuJa/bPwLt/7rVircFWH3NZsUv7hqHPdeOII31pZx3qOLeWrpLrljlxBCiJMy4MMYwG6x87MZP+NXZ/+KzdWbmfPOHJYfWB52X6UUCy8Yzj8Xns3EXBe/em8rMx8tZtEX+/D5A6e55EIIIaKBhHEnVwy9gpcvf5mkmCT+46P/4Ikvn8AfCN/qHZudyLMLprHo9ulkuew88MYGLvr9Ut7fcFDOJwshhDguEsaHGeoaykuXv8TsIbP547o/cufHd1LdUt3t/tOHpPDGnWfy1I1TMCvFnS+u4erH/82/d1adxlILIYTozySMw3Banfzy7F/y0IyHWF2+muveuY7V5au73V8pxUVjM/ngu+fy6LXjqWxs4/qnP+fGv3zOhtL601hyIYQQ/ZGEcTeUUnxtxNd48fIXsVvs3PLhLTyz8RkCuvvzwmaTYk5RLp98fyY/mT2GjWX1XPF/y7jrxTVyj2shhBDdkjA+hlHJo1g0exHn553P71b/ju988h3q247e2rVbzdxydgFL75/FwguGs3hbBRf+bik/fGMDh+rlSVBCCCG6kjDugXhbPI+d9xgPTHuAZQeWcd0717G+cv2x32e3cu+FI1hy3yxunD6Y11bv59xHFrPw5bV8tqtKBnoJIYQAwBLpAvQXSimuH309hamFfH/J97n+vetJc6QxMnkko5NHMyp5FKOSR5ETn4NJdf0/Tlp8DA9dOZZbzi7g6U938+baMt7+8gAFqbHMnZrL1ybnkBYfE6FvJoQQItJ6FMZKqUuAPwBm4Gmt9a+72e9rwGvAVK31ql4rZR8yPm08f7/i77y962221mxlS80Wlh9Yjl8bl0DFWmMZmTQyFM6jkkcxzDUMq9lKbrKT/7xqHD+8bDTvbTjIy1/s49fvb+U3H27jwjEZzJuWx9nDUjGZVIS/pRBCiNPpmGGslDIDjwMXAqXASqXU21rrzYftFw98B/j8VBS0L0mMSeTGMTeG1tv8beys28nWaiOct9Vs482db9LiawGMJ0UNcw1jZNJIxqeN5/Ihl/PVyTl8dXIOOysaWfTFfl5fU8r7Gw+Rk+RgblEuc4pyyUy0R+orCiGEOI160jKeBuzUWu8GUEotAq4CNh+23y+A/wbu69US9gMx5hjGpoxlbMrY0DZ/wM++xn1sq9kWCuhPyz7lrV1v8Yc1f+Abo7/B9aOuZ1i6iwdnj+G+S0byr03lvPzFPh77aDu/+3g754/KYN60XM4bkYbFLKf3hRAiWvUkjAcBnR/6Wwqc0XkHpdRkIFdr/U+l1IAL43DMJjMFiQUUJBZwScElAGit2VC1gb9s+At/+vJPPLfpOa4dcS3zx8wnIzaDKyZkc8WEbEqqmnll1X7+vqqUj7eUk5lgZ05RDldOyGZ4RnyEv5kQQojepo41olcpdS1widb61uD6jcAZWuu7g+sm4BPgJq11iVKqGPh+uHPGSqnbgdsBMjIypixatKjXvkhTUxNxcXG9drxT7aDnIB81fMTq5tUoFNPipvGVhK+Qbk0P7eMLaNZV+FlS6mNjlR8NDIpTTMu0MDXTQnbcsVvL/a1eThepl/CkXsKTeglP6iW87upl1qxZq7XWReHe05MwngE8pLW+OLj+QwCt9X8F1xOBXUD7XS0ygRrgyqMN4ioqKtKrVvXeGK/i4mJmzpzZa8c7XUobS3l207O8ueNNfNrHRYMv4pbCWxiVPKrLfhUNrXyw6RDvrj/IypIatIaRGfFcPj6Ly8dnMTQt/F+I/lovp5rUS3hSL+FJvYQn9RJed/WilOo2jHvSTb0SGK6UKgDKgK8D32h/UWtdD6R2+rBiumkZiyPlxOfw4PQHuWPCHbyw+QVe2fYKH5R8wDmDzuHWwluZnDEZgPQEO9+ckc83Z+RT3tDK+xsO8t6GQ/zu4+389qPtjMqM5/JCI5iHdBPMQggh+qZjhrHW2qeUuhv4EOPSpme01puUUj8HVmmt3z7VhRwIUh2p3DPlHm4pvIVFWxfxt81/Y/4H85mcPpnbxt/GWdlnoZRCa409po2Z4xQThtvZXW1m2Z4S1pWV8X9fVvH4pmbinG3EO1vB3IzH28KMJTOYkTWDGdkzyI7LjvRXFUIIcZgeXWestX4PeO+wbT/tZt+ZJ1+sgSvBlsDt42/nxjE38saON3h207Pc+fGdZMZm4gv4qGutw6d9R77RBgnpDmwqnrY2J4dqY9C+QTitmqV7v+DDkg8ByE/IZ3rWdGZkz2Ba5jTibNKKFkKISJM7cPVRDouD60dfz3UjruPd3e/yadmnJNgSSLInkRSTRJI9iWR7cmjuinFht3Rcl3ygroX3Nx7itc+2suuAxmc6hCVuB+W+Ev7e8CaLti3CrMyMTxsfajWPSx2HxSQ/CSGEON3kX94+zmq2cs3wa7hm+DXH9b5sl4Nbzi5gqG8v0886h7X76li+u5oVu6pZu6eSgK0Ea9xONvt2s7biCf745R+JtcYxLXMqM7KNbu3BCYNRSu4GJoQQp5qE8QBgt5qZMTSFGUNT4EJo8fhZs6+W5buqWb67mi/LysCxE1/cTpa1rWfx/sUAZDqzmJE9nelZ05mWNY1UR+oxPkkIIcSJkDAegBw2M2cNS+WsYUa4Nrf5WLW3lhW7q/lsVxWbKnajHDsoi9vJW80f8ubONwEY5hrOmdkzmJ41nSkZU3BanZH8GkIIETUkjAWxMRbOG5HGeSPSAGhsPYNVe2v5fHcNK/ZUsqlyC8q5g23NO9lV+xLPb34es7JQmDqeswYZ4Tw2dSxWkzXC30QIIfonCWNxhHi7lVkj05k1Mh0Yhdszg9XBcF6+5yAbq9ej7dtZ7d7Juoo/8vi6x4kxOSnKKOLsnBlMzpjMiKQRMhhMCCF6SP61FMfktFk4Z3ga5wxPA0bS6j2HNftq+WJPDf/evZ8NNavx2Lfzaesm/n1wKQBWZWdI/FimZU/mnNypTEibIN3aQgjRDQljcdzsVjNnDk3lzKGpfJcRtPlmsr60ns93V7N832521G2gXm9nc0sJW+vX8MLWP4M24bLkMyyhkDOypnDJ8Onku7JOW5m11pS7y9lTv4fd9bvZUr+Fse6xpDnTTlsZhBCiOxLG4qTFWMxMzU9man4ydzMcuJj6Fi/byxv5svQgnx9cy/b6DVS3bmOl9z1W1b7F45vB5Esl2TKC4QmFFGVN4IzBuQxLTcNpcZ7wJVVev5f9jfvZXb87FLx76vewp34Pbp+7y77vvf4es4fM5qaxNzHENaQXakIIIU6MhLE4JRId1lBA34rxnGetNSXVjXyyey2fH1zFjvoN1Pg3sLzhM5Y3wP9ua3+3IsYUS7wtnhRHAgkxCcRZ44i3xXdMVmMOsLdhbyh09zfux6/9oXJkxmZSkFDANcOvoSChgCGuIRQkFrB42WK2xW/jrZ1v8ebON5mZM5Obxt3E5PTJcm21EOK0kzAWp41SioLUBG5JPY9bOA8wAnpHzR6WlHzJlvIKdlVXUlpfS6O3kSZzKxXmVmIdtcTYKsDcgjfQTLOvuctxLcpCXkIew1zDuHDwhRQkFjAkcQj5ifnEWmPDliXNmsac6XP41sRvsWjrIl7e+jI3fXAT41PHs2DcAmblzsJsMp/yOhFCCJAwFhGmlGJEyhBGpHTtJj5U38qafbWs2VvL6n21bNrVgMcfACAnOYYJuXZGDbIyIiOWablDSHKe2OCwZHsy35r4LRaMW8BbO9/iuU3PcU/xPeTF5zF/7HyuHHpll9uMCiHEqSBhLPqkzEQ7lxVmcVmhMcir1etn04F61uytY/XeWlburuWfX9YDVcBe0uJjKEiNpSAlloK0WGM5NZa8ZCd267FbuA6Lg6+P+jpzRszh430f8+zGZ/nFil/w+LrHmTdqHl8f+XVcdtcp/c5CiIFLwlj0C3armSmDk5kyOJnbMLq3y+pa2FhWz+6qZkqqmtlT1cz/21pO1SpP6H1KwSCXIxTO7VOFO0AgoDGZup4fNpvMXJx/MRcNvohV5at4dtOzPL7ucZ7Z+AxXD7uaa4Zdg1KKFl8Lbq/bmPvctHhbOpYPf83XglKK0cmjGZcyjrGpY8lwZgzYc9PVLdVsqNrA+sr1NHgauHDwhUzNnIpJmSJdNCEiRsJY9EtKKXKSnOQkHdk93dDqDYXz7spmSqqN5TfXlNHY1vH4yZ8t/5DhGXEMT49nREYcIzLjGZERT3aiHaUUUzOnMjVzKjtrd/Lspmf5+/a/8/LWl49ZNrvZjtPqxGFx4LA4cFqctPnbePbAs6HHX6Y6UkPBPC51HGNTxpJkT+q9CuojWnwtbK3ZyvrK9Wyo2sCGyg0caD4AgFmZsZltvLLtFTKcGVw+5HKuGHIFw5KGRbjUQpx+EsYi6iTYrYzPcTE+x9Vlu9aa6mYPe6qaee/T1SjXIHZUNPLpjkpeX1Ma2i8+xsKwjDhGZsQzPCOekRkuFo5/kLsn3c2a8jXEmGNwWI2QbQ/b9nW7xd5tC6/V18q22m1srNrIpqpNbKzeyJLSJWg0AIPiBoWCeVzqOMakjOl2AFpfFNAB9tTvCYXuhqoNbK/dHhrdnhWbRWFqId8Y/Q0KUwsZnTIagOL9xbyz6x2e2/Qcz2x8htHJo7li6BVcWnCpPJxEDBgSxmLAUEqRGhdDalwMzSVWZs4cE3qtzu1he3kT28sbQ9O/NpezaOX+0D6JDisjMlwMS49jaJoxJaXFkh3nxGw6dpez3WJnQtoEJqRNCG1r8jSxuXozG6s3srFqIxsqN/BhyYdGeVFkx2VjM9uwmCxYlAWryWosd5q6bFPG3GFxkOJIIdWRSqojlTRHGqmOVJLsSSfcHRzQAera6qh0V1LhrqCypZJydzmV7kr2Ne5jU9UmmrxNAMRZ4xibOpabx91MYWohhWmF3QbrpQWXcmnBpVS1VPHBng94e9fbPLLyER5b9Rgzsmdw5dArmZU7SwbSiagmYSwE4HLamFaQzLSC5C7bq5rajHA+1Mj2iiZ2lDfyr03lVDd3hLTNYqIgJZah6bGhkB6aFseQtFhiY47+VyzOFse0rGlMy5oW2lbTWmO0nqs3sbdhL16/F1/Ah0/78AV8eANevAEvLb6W0Lov4OuyT4uvhWZv8xGfZ1ZmUuwppDhSSHOmkeZIM5aDYb2jZQf1O+upbAkGrruSihZjXtlSiS/gO+KYyfZkMmMzuazgMgrTChmfOp78xPzjDv1URyo3jLmBG8bcwK66Xbyz6x3e3f0u9y+9n1hrLBcOvpArhlxBUWbRaTm/3F7Hrb5Wmv1H1qUQvUnCWIijaG9Jnzm0a6uuttnD7qomdlU0s6uyiV2VTWw52MgHGw8R0B37ZSXaGZIWy5DUOPJTY8lPcTI4JZbcZAcxlvCjvJPtyZybcy7n5px7UmV3e91Ut1ZT1VJFpbuSqpaq0FTZUkmlu5LN1Zupaa0hoAMdb6wwZvG2eNId6aQ505iaOZU0RxppzjTSnemkOYx5qiMVm9l2UuUMZ6hrKN+d8l0WTl7IykMreWfXO/yr5F/8Y+c/yIrN4pxB5xz1czsPjlN0LAd0gBZfS2hq9bV2LPtbQwPxWvwtR/zH4+m3nmZa5jTOyDqDoswiEmwJvf69TydvwMuuul1sqtpEi6+Frwz+CpmxmZEu1oAlYSzECUiKtTEl1hjd3Vmbz8++ancwoJvZVdHErqpm/rG26+AxpSA70UF+qpP8lFjyU2IZnOIk/zguxzoWp9WJ0+okNz73qPv5A35q22qpdFfy2crPuOjMi0h1puKwOE66DCfLpEyckXUGZ2SdwY+n/5jF+xbz9u63eb/kfYKn2kPn3Nt1Xte662smZcJusYcG17Uvu+wuHGYHDmtwu9ne5fV1W9dR7azmjR1v8NLWlzApE2OSxzAtywjnSemT+kR9dccf8LOnfg+bqjexsWojm6s3s7VmK55Ax5UHj6x8hGmZ05g9dDZfyfsKcba4CJZ44JEwFqIXxVjMDA8O/OpMa02t20tJdTN7q5spqXIb82o3/9xwkDq3t8v+WYl2Bqc4yUt2MsjlJNtlZ1CSg0EuB5mJ9m5b1SfCbDKHzi2X28vJTTh6eEeKw+LgsiGXcdmQy077Z6cfTGfmzJl4/B7WV67ni0Nf8PnBz3l+0/M8s/EZrCYrE9ImMC1rGtOzpjMudVzEnu8d0AH2NexjY7UxUHBz9Wa21GyhxdcCgNPiZEzKGOaNmsfY1LGMTTFuV/vP3f/knd3v8JN//4Rfrvgls3JnMXvobM7MPvOkH4fqC/jYVrONVeWrWFW+ipqWGkYkj2BMyhjGpIxhuGv4Kelh6U8kjIU4DZRSJMfaSI61MTnvyEuY6twe9la7g2HdMf9kayVVTW1H7J8WH0O2y8Egl51BLgfZwal9OclpHbDXMZ9KNrONoswiijKL+NbEb+H2ullTsYYvDn7BioMreGLdE/xx3R9xWBxMyZjC+NTxxFpjQy3s9hH3h7fM2yerqePPTWtNm7+NJm8Tzd5mY+5pDq2HtnmbafIY80PuQ2yp3hIaSGc32xmVPIqvDv8qY1OM4O3ufP6dE+/kjgl38GXll7y7+10+KPmA90veJ9mezGUFlzF76GzGJI/p0e/K4/ewsWojq8tXs7p8NWsr1oYe1DI4YTDpznQ+LPmQ17a/BoDFZGG4a3gonMemjGV40skFtC/go66tjtrWWgI6QF5CXp/uvZAwFqIPcDltuJw2JuS6jnitzefnUH0rZbUtlNW1cKCulQN1LRyob2HroUY+2VpBqzfQ5T1Om5m8ZKMLfHCwK3xwijHPTLAfcbMTcWKcVidnDzqbswedDUB9Wz2rDq1ixcEVfHHoC5aVLTuu45mVGbvFjlmZcXvdoevSj8aiLMTaYomzxpFsT+byIZcbwZs6liGJQ46rVauUYmL6RCamT+QHU3/A0rKlvLvrXV7Z9gp/2/I3hiQO4YqhV3B5weVkxXU8AtXtdbO+an0ofNdXrqfNb/wncphrGFcMvYKijCKmZEwJPbZUa01pUymbqo3W++bqzXy09yNe3/G68b0OC+gxKWPIjM2kvq2emtYaalprqG2tDS0fvq2+rb7LKYv2qxPyE/MpSCigILFjSrGnRPw/rxLGQvRxMRYzg1NiGZwS/ppjrTU1zR4O1LUGw7qF/bVu9lW72VFhhHX7fb3BGP09ONkZPKYzNKgsPyUWf0CH/QzRM4kxiVww+AIuGHwBQGhke+eBYuGmw1/3az+x1lhirUbIts/jbB3LTquTOGscMeaYUxIkVrOVC/Iu4IK8C6hvq+fDkg95d/e7/GHNH/ifNf9DUWYRcc1xPP3e02yq2oRP+zApE6OSR3HdyOuYkjGFKelTur2NrFKK3PhccuNzuST/EqAjoNvD+fCA7k5iTCLJ9mSSYpIY6hpKUUwRyQ5jPdmRDBpKGkrYXb+bkvoS1pSvCXXbAyTYErqEc/sT3gbFDTrpLvqekjAWop9TSpESF0NKXAyFOYlHvO4PaA7Wt3Tp/t4bnC/bWdmlVW1SkPXFJwxyORiU5DDOVQfPWeckGV3gTpv8s9FTFpMl9NjP/iwxJpHrRl7HdSOvY3/jft7d/S7v7nqXNY1rKLQXMn/sfKZkTGFi+sST+q6dA/ri/IuB4K1vm8rYXL2ZypZKI3TtSSTbk0m2J+OKcR13YAZ0gPLm8q7PPG/Yw7KyZfxj5z9C+8VaY/ls3men5VI6+VslRJQzmzpuHXrWsK6XaGmtqWhso6TKCOela7dgTUymrLaFL/bUcKih9YjWsstpDZ2bHtQ+JTnISXKQm+TEJeero1pufC53TriTO8bfwSfFn3DBrAtO6ecppciJzyEnPqfXjmlSJrLissiKy+LMQWd2ea2+rZ6ShhL21O+hvq3+tN0zXcJYiAFMKUVGgp2MBDtnDEkhvXkXM2dODL3uD2jKG4xz1GXtU63RFb6v2s3yXdU0tXU9rxlrMwfD30FusjE3Jie5SU4SHBYJ6yiglMKsou+Z34kxiUfcKe90kDAWQnTLbFKhkdpFYV7XWtPQ6qO01k1ZbQv7a1sorXVTWtvC/ho3n++pOSKs42MswZa0EdRp8TGkx8eQnmA35vExJDltMshMDCgSxkKIE6aUItFhJdGRyNjsI89Xa61paPGxv9YdCun2oN5f4+bz3dVdbobSzmJSoZBOi7eTnhAM7PhgYCfEkJFgJyXWhsUsj14U/Z+EsRDilFFKkei0kuhMZNygI8MaoMXjp6KxlYrGNioa2o5YLq11s2ZfLTXNniPeqxSkxMaEAjo93gjpw0M8LT6mV2+UIkRvkzAWQkSUw3b0S7faeXwBqpragkEdDOwuy61sPtBAVVMb4a7QSnJaSYuPITnWRkpsTOgmLClxto7l4PYkp1Va3OK0kjAWQvQLNospdP76aPwBTXV7aDe2UtHQRnmwlV3Z2EZNs4ctBxuobvZQ3+INewyljEdmGgFtQ7e0UtywKTjYLYbMBDsZiXYyE+zHfDKXED0hvyIhRFQxm5QxGCzBDoTvGm/n9QeodXuoafZQ0+ShutlYNuZGcFc3edjXFGDb6tKw57fjYixGQCfaQyPTM4Pz9IQYkp02kmJtJNhlFLnonoSxEGLAsppNwUFh9qPuV1xczMyZM2lu83GooZXy+lbKG1s5VN9GeUMr5Q2tHGpoZcWuaioa2/CF6Se3mBQup9EFnhRrC4V0cqyVJKeNJKfRVd7+Wmq8TW6wMoD0qT9pr9dLaWkpra2tx/3exMREtmzZcgpK1b+dTL3Y7XZycnKwWiPz9Bkh+prYGAtD0+IYmtb94wUDAU11s4fyhlYqGlupbfaGWt+1bi+1zR5q3B52VTZRu9fY1t1tSJ02c/CZ2jZjHm88XzstPoa09m3B7bE2s7S8+7E+FcalpaXEx8eTn59/3D+qxsZG4uP79y3nToUTrRetNdXV1ZSWllJQUHAKSiZEdDIFL8tKi4/hWN3kYIR3Y5svFNJ1bg9VTR6qmtqoagzOm9ooqW5m1d5aat0edJjstltNpAZvi5oWZwxGS4mzkdIpzFPiOgapmeU67j6lT4Vxa2vrCQWx6H1KKVJSUqisrIx0UYSIaiZT+7XaVvI5+ohyAJ8/QE2zh8qmNiO0G9tCgd0e4gfqWtlQVk91kydsl7lSkOy0dYRznA2Xw4rLacXlsJHotAbXbbic1lD57Fa5POxU6VNhDEgQ9yHyZyFE32MxmzoNUDs6rTX1LV6qmjxUB8O6urmt03ob1U0eNh9ooL7FS31L913mYLS+XY6OgPa7jVHm6QkxZMQHB68lGndTi4+RAWvHo8+FcaTFxcXR1NQU6WIIIcRJU0qFnpU9LL3789zttNY0tfmocxvBXOf2UtfiCa0b2zzB7V7KmgJsW1NKY+uRo8wdVjOZifbQjVg6L6fHx5AUa4R6ktOGVa7pljAWQghhUEoRb7cSb7eS24P920eZuz0+yhs6RpZXNLQZo86Dy1+W1vHhplbafIGwx4mLseByGtd1h0acOzvCuv21JGfHaHOHLbq6zCWMu6G15v777+f9999HKcWDDz7I3LlzOXjwIHPnzqWhoQGfz8cTTzzBmWeeyS233MKqVatQSnHzzTdzzz33RPorCCHEaeG0WShItVCQ2v057/b7lJcHb75S6+4YXV4bbG23byupaqbW7Qnb4m5nt5o6XR7W6dIwZ/BysS6XjxnbbZa+2wLvs2H8n+9sYvOBhh7v7/f7MZuP/j+lMdkJ/OyKsT063htvvMG6dev48ssvqaqqYurUqZx77rm89NJLXHzxxfz4xz/G7/fjdrtZt24dZWVlbNy4EYC6uroel1sIIQaCjvuUWxmR0bMrPHz+AHXBrvGaZi81zcZo8xq3xxh93umysX01bmqajx7g8TEWkkLXcocP7ORO1367nKdv1HmfDeNIW7ZsGfPmzcNsNpORkcF5553HypUrmTp1KjfffDNer5err76aiRMnMmTIEHbv3s23v/1tLr/8ci666KJIF18IIfo9i9kUupa6pzy+AHUtHmqD4V3rNu6oVtfcKcTdxqC27eVN1Lo9uD3+sMeKi7Gw8T8v7q2vc1R9Nox72oJtd7quMz733HNZunQp//znP7npppu49957+eY3v8mXX37Jhx9+yJ/+9CdeffVVnnnmmVNeFiGEEF3ZLD27q1pnrV5/x41Zmr2h0Pb6w5/jPhX6bBhH2jnnnMOTTz7J/PnzqampYenSpTz66KPs3buXnJwcbrvtNtra2lizZg2XXXYZNpuNr33ta4wcOZIbbrgh0sUXQgjRQ3armaxEB1mJR38IyakkYdyNa665huXLlzNhwgSUUjzyyCNkZmby3HPP8eijj2K1WomLi+P555+nrKyMBQsWEAgY/4v6r//6rwiXXgghRH/SozBWSl0C/AEwA09rrX992Ov3ArcCPqASuFlrvbeXy3patF9jrJTi0Ucf5dFHH+3y+vz585k/f/4R71uzZs1pKZ8QQojoc8xx3kopM/A4cCkwBpinlBpz2G5rgSKt9XjgNeCR3i6oEEIIEa16ctHVNGCn1nq31toDLAKu6ryD1nqx1todXF0B5PRuMYUQQojo1ZNu6kHA/k7rpcAZR9n/FuD9cC8opW4HbgfIyMiguLi4y+uJiYk0Njb2oEhH8vv9J/zeaHay9dLa2nrEn1M0aGpqisrvdbKkXsKTeglP6iW8E6mXXh3ApZS6ASgCzgv3utb6KeApgKKiIj1z5swur2/ZsuWEL0+SRyiGd7L1YrfbmTRpUi+WqG9ov42f6ErqJTypl/CkXsI7kXrpSRiXQZfblOYEt3WhlPoK8GPgPK1123GVQgghhBjAenLOeCUwXClVoJSyAV8H3u68g1JqEvAkcKXWuqL3iymEEEJEr2OGsdbaB9wNfAhsAV7VWm9SSv1cKXVlcLdHgTjg70qpdUqpt7s5nBBCCCEO06Nzxlrr94D3Dtv2007LX+nlckU9n8+HxSL3XBFCCNGzbuoB5+qrr2bKlCmMHTuWp556CoAPPviAyZMnM2HCBC644ALAGDG3YMECCgsLGT9+PK+//joAcXEdD/F+7bXXuOmmmwC46aabuOOOOzjjjDO4//77+eKLL5gxYwaTJk3izDPPZNu2bYAxAvr73/8+48aNY/z48fzv//4vn3zyCVdffXXouB999BHXXHPNaagNIYQQp1rfbZq9/wAc2tDj3R1+H5iP8XUyC+HSXx99H+CZZ54hOTmZlpYWpk6dylVXXcVtt93G0qVLKSgooKamBoBf/OIXJCYmsmGDUc7a2tpjHru0tJTPPvsMs9lMQ0MDn376KRaLhY8//pgf/ehHvP766zz11FOUlJSwbt06LBYLNTU1JCUl8a1vfYvKykrS0tL461//ys0333zsihFCCNHn9d0wjqD/+Z//4c033wRg//79PPXUU5x77rkUFBQAkJycDMDHH3/MokWLQu9LSko65rHnzJkTeu5yfX098+fPZ8eOHSil8Hq9oePecccdoW7s9s+78cYb+dvf/saCBQtYvnw5zz//fC99YyGEEJHUd8O4By3Yzlp66Trj4uJiPv74Y5YvX47T6WTmzJlMnDiRrVu39vgYSnU8jLq1tbXLa7GxsaHln/zkJ8yaNYs333yTkpKSY16XtmDBAq644grsdjtz5syRc85CCBEl5JzxYerr60lKSsLpdLJ161ZWrFhBa2srS5cuZc+ePQChbuoLL7yQxx9/PPTe9m7qjIwMtmzZQiAQCLWwu/usQYMGAfDss8+Gtl944YU8+eST+Hy+Lp+XnZ1NdnY2Dz/8MAsWLOi9Ly2EECKiJIwPc8kll+Dz+Rg9ejQPPPAA06dPJy0tjaeeeoqvfvWrTJgwgblz5wLw4IMPUltby7hx45gwYQKLFy8G4Ne//jWzZ8/mzDPPJCsrq9vPuv/++/nhD3/IpEmTQsELcOutt5KXl8f48eOZMGECL730Uui166+/ntzcXEaPHn2KakAIIcTpJv2ch4mJieH998PeWptLL720y3pcXBzPPffcEftde+21XHvttUds79z6BZgxYwbbt28PrT/88MMAWCwWfvvb3/Lb3/72iGMsW7aM22677ZjfQwghRP8hYdyPTJkyhdjYWB577LFIF0UIIUQvkjDuR1avXh3pIgghhDgF5JyxEEIIEWESxkIIIUSESRgLIYQQESZhLIQQQkSYhLEQQggRYRLGJ6Hz05kOV1JSwrhx405jaYQQQvRXEsZCCCFEhPXZ64z/+4v/ZmtNzx/O4Pf7Q09D6s6o5FH8YNoPun39gQceIDc3l7vuuguAhx56CIvFwuLFi6mtrcXr9fLwww9z1VVX9bhcYDws4s4772TVqlWhu2vNmjWLTZs2sWDBAjweD4FAgNdff53s7Gyuu+46SktL8fv9/OQnPwndflMIIUR06rNhHAlz587lu9/9biiMX331VT788EMWLlxIQkICVVVVTJ8+nSuvvLLLk5mO5fHHH0cpxYYNG9i6dSsXXXQR27dv509/+hPf+c53uP766/F4PPj9ft577z2ys7P55z//CRgPkxBCCBHd+mwYH60FG05jLzxCcdKkSVRUVHDgwAEqKytJSkoiMzOTe+65h6VLl2IymSgrK6O8vJzMzMweH3fZsmV8+9vfBmDUqFEMHjyY7du3M2PGDH75y19SWlrKV7/6VYYPH05hYSHf+973+MEPfsDs2bM555xzTuo7CSGE6PvknPFh5syZw2uvvcYrr7zC3LlzefHFF6msrGT16tWsW7eOjIyMI55RfKK+8Y1v8Pbbb+NwOLjsssv45JNPGDFiBGvWrKGwsJAHH3yQn//8573yWUIIIfquPtsyjpS5c+dy2223UVVVxZIlS3j11VdJT0/HarWyePFi9u7de9zHPOecc3jxxRc5//zz2b59O/v27WPkyJHs3r2bIUOGsHDhQvbt28f69esZNWoUycnJ3HDDDbhcLp5++ulT8C2FEEL0JRLGhxk7diyNjY0MGjSIrKwsrr/+eq644goKCwspKipi1KhRx33Mb33rW9x5550UFhZisVh49tlniYmJ4dVXX+WFF17AarWSmZnJj370I1auXMl9992HyWTCarXyxBNPnIJvKYQQoi+RMA5jw4YNoeXU1FSWL18edr+mpqZuj5Gfn8/GjRsBsNvt/PWvfz1inwceeIAHHnigy7aLL76Yiy+++ESKLYQQop+Sc8ZCCCFEhEnL+CRt2LCBG2+8scu2mJgYPv/88wiVSAghRH8jYXySCgsLWbduXaSLIYQQoh+TbmohhBAiwiSMhRBCiAiTMBZCCCEiTMJYCCGEiDAJ45NwtOcZCyGEED0lYRwFfD5fpIsghBDiJPTZS5sO/epXtG3p+fOMfX4/Ncd4nnHM6FFk/uhH3b7em88zbmpq4qqrrgr7vueff57f/OY3KKUYP348L7zwAuXl5dxxxx3s3r0bgCeeeILs7Gxmz54dupPXb37zG5qamnjooYeYOXMmEydOZNmyZcybN48RI0bw8MMP4/F4SElJ4cUXXyQjI4OmpiYWLlzIqlWrUErxs5/9jPr6etavX8/vf/97AP785z+zefNmfve73x3zewkhhOh9fTaMI6E3n2dst9t58803j3jf5s2befjhh/nss89ITU2lpqYGgIULF3Leeefx5ptv4vf7aWpqora29qif4fF4WLVqFQC1tbWsWLECpRRPP/00jzzyCI899hiPPPIIiYmJoVt81tbWYrVa+eUvf8mjjz6K1Wrlr3/9K08++eTJVp8QQogT1GfD+Ggt2HD62vOMtdb86Ec/OuJ9n3zyCXPmzCE1NRWA5ORkAD755BOef/55AMxmM4mJiccM47lz54aWS0tLmTt3LgcPHsTj8VBQUABAcXExr776ami/pKQkAM4//3zeffddRo8ejdfrpbCw8DhrSwghRG/ps2EcKe3PMz506NARzzO2Wq3k5+f36HnGJ/q+ziwWC4FAILR++PtjY2NDy9/+9re59957ufLKKykuLuahhx466rFvvfVWfvWrXzFq1CgWLFhwXOUSQgjRu2QA12Hmzp3LokWLeO2115gzZw719fUn9Dzj7t53/vnn8/e//53q6mqAUDf1BRdcEHpcot/vp76+noyMDCoqKqiurqatrY133333qJ83aNAgAJ577rnQ9lmzZvH444+H1ttb22eccQb79+/npZdeYt68eT2tHiGEEKeAhPFhwj3PeNWqVRQWFvL888/3+HnG3b1v7Nix/PjHP+a8885jwoQJ3HvvvQD84Q9/YPHixRQWFjJlyhQ2b96M1Wrlpz/9KdOmTePCCy886mc/9NBDzJkzhylTpoS6wAHuu+8+amtrGTduHBMmTGDx4sWh16677jrOOuusUNe1EEKIyJBu6jB643nGR3vf/PnzmT9/fpdtGRkZvPXWW0fsu3DhQhYuXHjE9uLi4i7rV111VdhR3nFxcV1ayp0tW7aMe+65p7uvIIQQ4jSRlvEAVFdXx4gRI3A4HFxwwQWRLo4QQgx40jI+Sf3xecYul4vt27dHuhhCCCGCJIxPkjzPWAghxMnqc93UWutIF0EEyZ+FEEKcHn0qjO12O9XV1RICfYDWmurqaux2e6SLIoQQUa9PdVPn5ORQWlpKZWXlcb+3tbVVgiOMk6kXu91OTk5OL5dICCHE4XoUxkqpS4A/AGbgaa31rw97PQZ4HpgCVANztdYlx1sYq9Uauo3j8SouLmbSpEkn9N5oJvUihBB93zG7qZVSZuBx4FJgDDBPKTXmsN1uAWq11sOA3wH/3dsFFUIIIaJVT84ZTwN2aq13a609wCLg8LtLXAW031niNeACdazHGgkhhBAC6FkYDwL2d1ovDW4Lu4/W2gfUAym9UUAhhBAi2p3WAVxKqduB24OrTUqpbb14+FSgqhePFy2kXsKTeglP6iU8qZfwpF7C665eBnf3hp6EcRmQ22k9J7gt3D6lSikLkIgxkKsLrfVTwFM9+MzjppRapbUuOhXH7s+kXsKTeglP6iU8qZfwpF7CO5F66Uk39UpguFKqQCllA74OvH3YPm8D7U8+uBb4RMvFwkIIIUSPHLNlrLX2KaXuBj7EuLTpGa31JqXUz4FVWuu3gb8ALyildgI1GIEthBBCiB7o0TljrfV7wHuHbftpp+VWYE7vFu24nZLu7ygg9RKe1Et4Ui/hSb2EJ/US3nHXi5LeZCGEECKy+tS9qYUQQoiBKCrCWCl1iVJqm1Jqp1LqgUiXp69QSpUopTYopdYppVZFujyRopR6RilVoZTa2GlbslLqI6XUjuA8KZJljIRu6uUhpVRZ8DezTil1WSTLGAlKqVyl1GKl1Gal1Cal1HeC2wf0b+Yo9TKgfzNKKbtS6gul1JfBevnP4PYCpdTnwVx6JTgAuvvj9Pdu6uDtOrcDF2LckGQlME9rvTmiBesDlFIlQJHWekBfB6iUOhdoAp7XWo8LbnsEqNFa/zr4H7gkrfUPIlnO062benkIaNJa/yaSZYskpVQWkKW1XqOUigdWA1cDNzGAfzNHqZfrGMC/meDdJmO11k1KKSuwDPgOcC/whtZ6kVLqT8CXWusnujtONLSMe3K7TjGAaa2XYozy76zzLVyfw/hHZUDppl4GPK31Qa31muByI7AF4y6DA/o3c5R6GdC0oSm4ag1OGjgf4/bQ0IPfSzSEcU9u1zlQaeBfSqnVwbufiQ4ZWuuDweVDQEYkC9PH3K2UWh/sxh5QXbGHU0rlA5OAz5HfTMhh9QID/DejlDIrpdYBFcBHwC6gLnh7aOhBLkVDGIvuna21nozxxK27gt2S4jDBG9T07/M1vecJYCgwETgIPBbR0kSQUioOeB34rta6ofNrA/k3E6ZeBvxvRmvt11pPxLhD5TRg1PEeIxrCuCe36xyQtNZlwXkF8CbGj0QYyoPnwNrPhVVEuDx9gta6PPgPSwD4MwP0NxM89/c68KLW+o3g5gH/mwlXL/Kb6aC1rgMWAzMAV/D20NCDXIqGMO7J7ToHHKVUbHCQBUqpWOAiYOPR3zWgdL6F63zgrQiWpc9oD5ugaxiAv5nggJy/AFu01r/t9NKA/s10Vy8D/TejlEpTSrmCyw6MwcRbMEL52uBux/y99PvR1ADBofS/p+N2nb+MbIkiTyk1BKM1DMad1l4aqPWilHoZmInxJJVy4GfAP4BXgTxgL3Cd1npADWbqpl5mYnQ3aqAE+I9O50kHBKXU2cCnwAYgENz8I4zzowP2N3OUepnHAP7NKKXGYwzQMmM0cF/VWv88+G/wIiAZWAvcoLVu6/Y40RDGQgghRH8WDd3UQgghRL8mYSyEEEJEmISxEEIIEWESxkIIIUSESRgLIYQQESZhLIQQQkSYhLEQQggRYRLGQgghRIT9f1vuWFjyTgB3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-madrid",
   "metadata": {},
   "source": [
    "Here you would see that training and validation curves are close which is good. At first, it seems validation is good because validation error is computed at the end of epoch and training loss is computed during each epoch. So training curve should be shifted half the epoch to the left. Then you see overlapping curves at the start.\n",
    "\n",
    "> When plotting training curves, it should be shifted by half an epoch to the left.\n",
    "\n",
    "If you want to train longer you can call `fit()` method and Keras will continue the training.\n",
    "\n",
    "If you didn't satisfy with the training try tuning the hyperparameters. The first one to check is learning rate, then optimizer, then model's hyperparameters such as number of layers, number of neurons, and activation functions. You could also tune batch size (which can be set in `fit()` method as `batch_size` argument, defaults to 32) and number of epochs.\n",
    "\n",
    "Now, it's time to evaluate model on the test set to see the generalization error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "national-brooks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 66.1980 - accuracy: 0.8457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[66.19799041748047, 0.8457000255584717]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-blond",
   "metadata": {},
   "source": [
    "Generally, test set performance is slightly lower than validation set, because the hyperparameters tuned on validation set. And do not try to tune hyperparameters on the test set.\n",
    "\n",
    "#### Using the Model to make predictions\n",
    "\n",
    "Next, we can use model `predict()` method for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "finite-indie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_probs = model.predict(X_new)\n",
    "y_probs.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-johnston",
   "metadata": {},
   "source": [
    "This will give you probabilities for each class. If you only want class then you can use `np.argmax()` method with `axis=-1`.\n",
    "\n",
    "> Use  `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification (e.g. if it uses a `softmax` last-layer activation).`(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "wanted-queen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "italic-entity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-stack",
   "metadata": {},
   "source": [
    "### Building a Regression MLP Using the Sequential API\n",
    "\n",
    "Let's do some linear regression using neural networks. We'll use Sklearn's `fetch_california_housing()` dataset, and do some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detailed-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "serial-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-reconstruction",
   "metadata": {},
   "source": [
    "Now, we use Sequential API to build, train, evaluate, and use a MLP regression model for predictions. The output layer has a single neuron to predict a value and has no activation function, and the mean squared error as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "frank-management",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8459 - val_loss: 0.5081\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4718 - val_loss: 0.4741\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.4415\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3920 - val_loss: 0.4299\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.4122\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.4136\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.4104\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3594 - val_loss: 0.3991\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3781 - val_loss: 0.3995\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.3970\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3986\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.4095\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.3923\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3526 - val_loss: 0.3950\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.4004\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.4030\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3950\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3921\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3875\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3843\n",
      "162/162 [==============================] - 0s 715us/step - loss: 0.3538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.5931412],\n",
       "       [2.05368  ],\n",
       "       [1.6135161]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                   validation_data=(X_val, y_val))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-river",
   "metadata": {},
   "source": [
    "Sequential API is quit easy and common, but sometimes we need to build more complex architectures like with multiple inputs or outputs. For this purpose, Keras offers a *Functional API*.\n",
    "\n",
    "### Building Complex Models using Functional API\n",
    "\n",
    "One example of non-sequential neural network is *Wide & Deep* neural networks. It connects all or part of the inputs to the outputs, which helps to learn both deep patterns (using deep path) and simple patterns (using short path).\n",
    "\n",
    "Let's build such a neural network:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "palestinian-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-holly",
   "metadata": {},
   "source": [
    "Let's go through this code:\n",
    "\n",
    "- First, we need to create an `Input` object. Which specifies the input kind to the model.\n",
    "- Next, we create a normal `Dense` layer. Notice, we call it like a function, passing it the input. This is why this is called Functional API. \n",
    "- Then we pass output of first hidden to another.\n",
    "- Next, we create a `Concatenate` layer, we pass it the input and the output of second hidden layer. You may prefer `keras.layers.concatenate()` function.\n",
    "- Then we create an output layer, passing in result of the concatenation.\n",
    "- Lastly, we create a Keras `Model`, specifying which inputs and outputs to use.\n",
    "\n",
    "After that all steps are same.\n",
    "\n",
    "But what if you want to send a subset of the features through the wide path and different (possibly overlapping) to the deep path? Solution is to use multiple inputs. For example, suppose we want to pass five features through wide path (0 to 4), and six features to deep path (2 to 7):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "instructional-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-painting",
   "metadata": {},
   "source": [
    "The code is self-explanatory. You should name the important layers when model get complex. Now to call `fit()` you need to pass it a pair of input matrices `(X_train_A, X_train_B)`, one for each. This is true for each `X_val`, `X_test`, and `X_new` when you call `evaluate()` or `predict()`.\n",
    "\n",
    "> Alternatively, you could pass a dictionary mapping the input names to the input matrices, like `{'wide_input': X_train_A, 'deep_input': X_train_B}`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "compound-warrior",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 3.3679 - val_loss: 0.9629\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8312 - val_loss: 0.7685\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7147 - val_loss: 0.7026\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6631 - val_loss: 0.6613\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5823 - val_loss: 0.6296\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5718 - val_loss: 0.6048\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5589 - val_loss: 0.5859\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5282 - val_loss: 0.5713\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 0.5553\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5009 - val_loss: 0.5433\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4914 - val_loss: 0.5333\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4836 - val_loss: 0.5267\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4713 - val_loss: 0.5193\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4695 - val_loss: 0.5128\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4724 - val_loss: 0.5078\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4797 - val_loss: 0.5018\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4630 - val_loss: 0.4975\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4469 - val_loss: 0.4935\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4635 - val_loss: 0.4906\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4555 - val_loss: 0.4885\n",
      "162/162 [==============================] - 0s 824us/step - loss: 0.4660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.8227581],\n",
       "       [2.1859999],\n",
       "       [1.9973905]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_val_A, X_val_B = X_val[:, :5], X_val[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                   validation_data=((X_val_A, X_val_B), y_val))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-button",
   "metadata": {},
   "source": [
    "There are many use cases when you may want multiple outputs:\n",
    "\n",
    "- For object detection, you may want to locate (regression) and classify (classification) an object in a picture.\n",
    "- If you have multiple independent tasks on the same data. Sure, you could train multiple neural networks for each task but training a single might get you better performance by learning across features. For example, you can perform *multitask classification*.\n",
    "- One use case is used as regularization technique by adding some auxiliary outputs in a neural network architecture to ensure that the underlying part of the networks learn something useful.\n",
    "\n",
    "Adding extra outputs is quite easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sweet-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-lobby",
   "metadata": {},
   "source": [
    "Each output will need its own loss function. Therefor, when we compile the model, we should pass a list of losses (or a dictionary mapping output names to losses). If we pass a single loss, Keras will assume same loss for all outputs. Keras will compute all these losses and add them up at last. Since, main outputs are more important than auxiliary ones, we can add weight to main output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "controversial-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-found",
   "metadata": {},
   "source": [
    "Now when we train the model it should expect labels for each output. In this example the labels are same, so instead of passing `y_train`, we need to pass `(y_train, y_train)` (and same goes for `y_val` and `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "changed-brunei",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.5897 - main_output_loss: 1.4234 - aux_output_loss: 3.0855 - val_loss: 0.9425 - val_main_output_loss: 0.9056 - val_aux_output_loss: 1.2744\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6145 - main_output_loss: 0.5641 - aux_output_loss: 1.0685 - val_loss: 0.7008 - val_main_output_loss: 0.6720 - val_aux_output_loss: 0.9600\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5498 - main_output_loss: 0.5159 - aux_output_loss: 0.8550 - val_loss: 0.5325 - val_main_output_loss: 0.5011 - val_aux_output_loss: 0.8149\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4993 - main_output_loss: 0.4713 - aux_output_loss: 0.7507 - val_loss: 0.5094 - val_main_output_loss: 0.4838 - val_aux_output_loss: 0.7401\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4758 - main_output_loss: 0.4531 - aux_output_loss: 0.6803 - val_loss: 0.4750 - val_main_output_loss: 0.4522 - val_aux_output_loss: 0.6799\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4371 - main_output_loss: 0.4159 - aux_output_loss: 0.6278 - val_loss: 0.4819 - val_main_output_loss: 0.4632 - val_aux_output_loss: 0.6495\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4690 - main_output_loss: 0.4509 - aux_output_loss: 0.6318 - val_loss: 0.4929 - val_main_output_loss: 0.4673 - val_aux_output_loss: 0.7228\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4783 - main_output_loss: 0.4589 - aux_output_loss: 0.6535 - val_loss: 0.4596 - val_main_output_loss: 0.4407 - val_aux_output_loss: 0.6291\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4307 - main_output_loss: 0.4144 - aux_output_loss: 0.5776 - val_loss: 0.4471 - val_main_output_loss: 0.4302 - val_aux_output_loss: 0.5991\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4304 - main_output_loss: 0.4152 - aux_output_loss: 0.5678 - val_loss: 0.4348 - val_main_output_loss: 0.4192 - val_aux_output_loss: 0.5756\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3993 - main_output_loss: 0.3839 - aux_output_loss: 0.5380 - val_loss: 0.4450 - val_main_output_loss: 0.4301 - val_aux_output_loss: 0.5790\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3950 - main_output_loss: 0.3802 - aux_output_loss: 0.5280 - val_loss: 0.4259 - val_main_output_loss: 0.4112 - val_aux_output_loss: 0.5583\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3922 - main_output_loss: 0.3773 - aux_output_loss: 0.5266 - val_loss: 0.4373 - val_main_output_loss: 0.4235 - val_aux_output_loss: 0.5612\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3907 - main_output_loss: 0.3767 - aux_output_loss: 0.5168 - val_loss: 0.4234 - val_main_output_loss: 0.4099 - val_aux_output_loss: 0.5449\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3884 - main_output_loss: 0.3746 - aux_output_loss: 0.5126 - val_loss: 0.4229 - val_main_output_loss: 0.4101 - val_aux_output_loss: 0.5383\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3753 - main_output_loss: 0.3606 - aux_output_loss: 0.5080 - val_loss: 0.4317 - val_main_output_loss: 0.4193 - val_aux_output_loss: 0.5437\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3668 - main_output_loss: 0.3533 - aux_output_loss: 0.4883 - val_loss: 0.5238 - val_main_output_loss: 0.5188 - val_aux_output_loss: 0.5687\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6090 - main_output_loss: 0.6191 - aux_output_loss: 0.5179 - val_loss: 0.4064 - val_main_output_loss: 0.3939 - val_aux_output_loss: 0.5187\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3988 - main_output_loss: 0.3878 - aux_output_loss: 0.4979 - val_loss: 0.4035 - val_main_output_loss: 0.3912 - val_aux_output_loss: 0.5140\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3713 - main_output_loss: 0.3576 - aux_output_loss: 0.4950 - val_loss: 0.4053 - val_main_output_loss: 0.3930 - val_aux_output_loss: 0.5155\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-endorsement",
   "metadata": {},
   "source": [
    "When we evaluate the model, Keras will return total loss, as well as all the individual losses. Similarly, `predict()` will return predictions for each output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "pointed-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 973us/step - loss: 0.3699 - main_output_loss: 0.3597 - aux_output_loss: 0.4626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.7583452],\n",
       "        [2.434911 ],\n",
       "        [1.4337739]], dtype=float32),\n",
       " array([[1.8176169],\n",
       "        [2.5233927],\n",
       "        [1.1671491]], dtype=float32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test]\n",
    ")\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
    "y_pred_main, y_pred_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-louis",
   "metadata": {},
   "source": [
    "### Using the Subclassing API to Build Dynamic Models\n",
    "\n",
    "Both Sequential API and Functional API are declarative: you start by declaring layers and how should they connect to each other. But they are static; you cannot involve loops, varying shapes, conditional branching, and other dynamic behaviors. For this you'll use Subclassing API.\n",
    "\n",
    "Simply subclass the `Model` class, create the layers you need in the constructor, and use them to perform the computations you want in the `call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "minor-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-nightmare",
   "metadata": {},
   "source": [
    "The main difference here is you can do anything in `call()` method: for loops, if else, low-level TensorFlow operations. This makes it a great API for researchers experimenting with new ideas.\n",
    "\n",
    "> Keras models have an `output` attribute, that's why we use `main_output` name.\n",
    "\n",
    "This extra flexibility comes with a cost: your model's architecture is hidden within `call()` method, so Keras cannot easily inspect it; it cannot save it or clone it; and when you call `summary()` method it only give list of layers not how they are connected.\n",
    "\n",
    "### Saving and Restoring a Model\n",
    "\n",
    "Saving trained models made by Sequential API or Functional API is as simple as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "welcome-progressive",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.4075 - main_output_loss: 1.2423 - aux_output_loss: 2.8943 - val_loss: 0.8346 - val_main_output_loss: 0.7835 - val_aux_output_loss: 1.2940\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5991 - main_output_loss: 0.5384 - aux_output_loss: 1.1451 - val_loss: 0.5930 - val_main_output_loss: 0.5425 - val_aux_output_loss: 1.0478\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5140 - main_output_loss: 0.4667 - aux_output_loss: 0.9396 - val_loss: 0.6174 - val_main_output_loss: 0.5874 - val_aux_output_loss: 0.8868\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4906 - main_output_loss: 0.4559 - aux_output_loss: 0.8034 - val_loss: 0.5336 - val_main_output_loss: 0.5033 - val_aux_output_loss: 0.8060\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4655 - main_output_loss: 0.4330 - aux_output_loss: 0.7585 - val_loss: 0.5127 - val_main_output_loss: 0.4867 - val_aux_output_loss: 0.7467\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4517 - main_output_loss: 0.4248 - aux_output_loss: 0.6944 - val_loss: 0.5123 - val_main_output_loss: 0.4897 - val_aux_output_loss: 0.7149\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4601 - main_output_loss: 0.4355 - aux_output_loss: 0.6816 - val_loss: 0.4764 - val_main_output_loss: 0.4530 - val_aux_output_loss: 0.6877\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4240 - main_output_loss: 0.3997 - aux_output_loss: 0.6428 - val_loss: 0.4943 - val_main_output_loss: 0.4747 - val_aux_output_loss: 0.6709\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4190 - main_output_loss: 0.3956 - aux_output_loss: 0.6299 - val_loss: 0.5059 - val_main_output_loss: 0.4897 - val_aux_output_loss: 0.6517\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4348 - main_output_loss: 0.4137 - aux_output_loss: 0.6251 - val_loss: 0.5068 - val_main_output_loss: 0.4920 - val_aux_output_loss: 0.6397\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4308 - main_output_loss: 0.4083 - aux_output_loss: 0.6328 - val_loss: 0.4566 - val_main_output_loss: 0.4375 - val_aux_output_loss: 0.6293\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4143 - main_output_loss: 0.3940 - aux_output_loss: 0.5969 - val_loss: 0.4812 - val_main_output_loss: 0.4662 - val_aux_output_loss: 0.6161\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4381 - main_output_loss: 0.4205 - aux_output_loss: 0.5960 - val_loss: 0.4483 - val_main_output_loss: 0.4305 - val_aux_output_loss: 0.6084\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4046 - main_output_loss: 0.3854 - aux_output_loss: 0.5776 - val_loss: 0.4635 - val_main_output_loss: 0.4484 - val_aux_output_loss: 0.5998\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3916 - main_output_loss: 0.3725 - aux_output_loss: 0.5636 - val_loss: 0.4849 - val_main_output_loss: 0.4728 - val_aux_output_loss: 0.5930\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3930 - main_output_loss: 0.3752 - aux_output_loss: 0.5524 - val_loss: 0.4446 - val_main_output_loss: 0.4282 - val_aux_output_loss: 0.5922\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3972 - main_output_loss: 0.3784 - aux_output_loss: 0.5661 - val_loss: 0.4476 - val_main_output_loss: 0.4326 - val_aux_output_loss: 0.5826\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3876 - main_output_loss: 0.3684 - aux_output_loss: 0.5601 - val_loss: 0.4307 - val_main_output_loss: 0.4142 - val_aux_output_loss: 0.5786\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3859 - main_output_loss: 0.3685 - aux_output_loss: 0.5424 - val_loss: 0.4344 - val_main_output_loss: 0.4199 - val_aux_output_loss: 0.5650\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3773 - main_output_loss: 0.3602 - aux_output_loss: 0.5306 - val_loss: 0.4318 - val_main_output_loss: 0.4180 - val_aux_output_loss: 0.5560\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')\n",
    "\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]))\n",
    "\n",
    "model.save('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-indiana",
   "metadata": {},
   "source": [
    "Keras will use `HDF5` format to save both model's architecture (including every layer's hyperparameters) and the values of all the parameters for every layer. It also saves the optimizer.\n",
    "\n",
    "And loading the model is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "literary-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-sponsorship",
   "metadata": {},
   "source": [
    "> If you use Subclassing API, then you can save model's parameters by `save_weights()` and load them by `load_weights()`. But you have to save everything else by yourself as a Dictionary maybe.\n",
    "\n",
    "But what if your training lasts several hours? Then you should save checkpoints for every epoch, in case you system crashes this will save everything your model learned so far. This can be done using callbacks.\n",
    "\n",
    "### Using Callbacks\n",
    "\n",
    "The `fit()` method accepts a `callbacks` argument that lets you specify when to save model. For example, `ModelCheckpoint` callback saves checkpoint at regular intervals (by default after each epoch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "worse-mitchell",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.0914 - main_output_loss: 2.0098 - aux_output_loss: 2.8255 - val_loss: 0.6607 - val_main_output_loss: 0.5891 - val_aux_output_loss: 1.3050\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5833 - main_output_loss: 0.5162 - aux_output_loss: 1.1869 - val_loss: 0.5707 - val_main_output_loss: 0.5155 - val_aux_output_loss: 1.0675\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4885 - main_output_loss: 0.4361 - aux_output_loss: 0.9601 - val_loss: 0.5225 - val_main_output_loss: 0.4783 - val_aux_output_loss: 0.9201\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4763 - main_output_loss: 0.4355 - aux_output_loss: 0.8433 - val_loss: 0.5016 - val_main_output_loss: 0.4659 - val_aux_output_loss: 0.8232\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4531 - main_output_loss: 0.4197 - aux_output_loss: 0.7533 - val_loss: 0.4855 - val_main_output_loss: 0.4548 - val_aux_output_loss: 0.7616\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4444 - main_output_loss: 0.4139 - aux_output_loss: 0.7193 - val_loss: 0.4711 - val_main_output_loss: 0.4444 - val_aux_output_loss: 0.7108\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4304 - main_output_loss: 0.4033 - aux_output_loss: 0.6739 - val_loss: 0.4654 - val_main_output_loss: 0.4419 - val_aux_output_loss: 0.6769\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4263 - main_output_loss: 0.4022 - aux_output_loss: 0.6435 - val_loss: 0.4577 - val_main_output_loss: 0.4366 - val_aux_output_loss: 0.6476\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4110 - main_output_loss: 0.3886 - aux_output_loss: 0.6128 - val_loss: 0.4523 - val_main_output_loss: 0.4336 - val_aux_output_loss: 0.6200\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4039 - main_output_loss: 0.3828 - aux_output_loss: 0.5935 - val_loss: 0.4453 - val_main_output_loss: 0.4275 - val_aux_output_loss: 0.6061\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3978 - main_output_loss: 0.3782 - aux_output_loss: 0.5743 - val_loss: 0.4323 - val_main_output_loss: 0.4156 - val_aux_output_loss: 0.5823\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4019 - main_output_loss: 0.3843 - aux_output_loss: 0.5607 - val_loss: 0.4229 - val_main_output_loss: 0.4077 - val_aux_output_loss: 0.5596\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3735 - main_output_loss: 0.3570 - aux_output_loss: 0.5216 - val_loss: 0.4162 - val_main_output_loss: 0.4014 - val_aux_output_loss: 0.5500\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3750 - main_output_loss: 0.3587 - aux_output_loss: 0.5220 - val_loss: 0.4127 - val_main_output_loss: 0.3990 - val_aux_output_loss: 0.5361\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4077 - main_output_loss: 0.3946 - aux_output_loss: 0.5255 - val_loss: 0.4084 - val_main_output_loss: 0.3956 - val_aux_output_loss: 0.5233\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3672 - main_output_loss: 0.3536 - aux_output_loss: 0.4895 - val_loss: 0.4051 - val_main_output_loss: 0.3926 - val_aux_output_loss: 0.5174\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3800 - main_output_loss: 0.3652 - aux_output_loss: 0.5125 - val_loss: 0.4002 - val_main_output_loss: 0.3886 - val_aux_output_loss: 0.5046\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3722 - main_output_loss: 0.3582 - aux_output_loss: 0.4982 - val_loss: 0.3992 - val_main_output_loss: 0.3882 - val_aux_output_loss: 0.4982\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3507 - main_output_loss: 0.3384 - aux_output_loss: 0.4614 - val_loss: 0.3967 - val_main_output_loss: 0.3856 - val_aux_output_loss: 0.4969\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3561 - main_output_loss: 0.3430 - aux_output_loss: 0.4739 - val_loss: 0.3899 - val_main_output_loss: 0.3785 - val_aux_output_loss: 0.4923\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model.h5', save_best_only=True)\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]),\n",
    "                   callbacks=[checkpoint_cb])\n",
    "\n",
    "model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-punishment",
   "metadata": {},
   "source": [
    "If you use validation data then set `save_best_only=True` will save only best performance one so far. This will be best on validation set.\n",
    "\n",
    "Another way is use the `EarlyStopping` callback: it will interrupt training when there is no progress on the validation set for a number of epochs (defined by `patience` argument), and it will optionally roll back to the best model. You can combine both; this will useful in both ways if you computer crashes or stop early when there is no progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "injured-condition",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3556 - main_output_loss: 0.3432 - aux_output_loss: 0.4669 - val_loss: 0.3939 - val_main_output_loss: 0.3832 - val_aux_output_loss: 0.4903\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3546 - main_output_loss: 0.3427 - aux_output_loss: 0.4620 - val_loss: 0.3862 - val_main_output_loss: 0.3757 - val_aux_output_loss: 0.4809\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3524 - main_output_loss: 0.3407 - aux_output_loss: 0.4574 - val_loss: 0.3892 - val_main_output_loss: 0.3786 - val_aux_output_loss: 0.4843\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3518 - main_output_loss: 0.3407 - aux_output_loss: 0.4515 - val_loss: 0.4314 - val_main_output_loss: 0.4259 - val_aux_output_loss: 0.4810\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3548 - main_output_loss: 0.3441 - aux_output_loss: 0.4508 - val_loss: 0.3811 - val_main_output_loss: 0.3708 - val_aux_output_loss: 0.4732\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3474 - main_output_loss: 0.3364 - aux_output_loss: 0.4462 - val_loss: 0.3930 - val_main_output_loss: 0.3833 - val_aux_output_loss: 0.4802\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3464 - main_output_loss: 0.3356 - aux_output_loss: 0.4443 - val_loss: 0.3800 - val_main_output_loss: 0.3710 - val_aux_output_loss: 0.4605\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3451 - main_output_loss: 0.3344 - aux_output_loss: 0.4418 - val_loss: 0.3726 - val_main_output_loss: 0.3630 - val_aux_output_loss: 0.4583\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3417 - main_output_loss: 0.3313 - aux_output_loss: 0.4348 - val_loss: 0.3807 - val_main_output_loss: 0.3721 - val_aux_output_loss: 0.4584\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3400 - main_output_loss: 0.3294 - aux_output_loss: 0.4355 - val_loss: 0.3737 - val_main_output_loss: 0.3648 - val_aux_output_loss: 0.4543\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3418 - main_output_loss: 0.3314 - aux_output_loss: 0.4349 - val_loss: 0.3742 - val_main_output_loss: 0.3659 - val_aux_output_loss: 0.4491\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3388 - main_output_loss: 0.3287 - aux_output_loss: 0.4293 - val_loss: 0.3881 - val_main_output_loss: 0.3799 - val_aux_output_loss: 0.4623\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3368 - main_output_loss: 0.3269 - aux_output_loss: 0.4252 - val_loss: 0.3696 - val_main_output_loss: 0.3618 - val_aux_output_loss: 0.4405\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3374 - main_output_loss: 0.3277 - aux_output_loss: 0.4246 - val_loss: 0.3683 - val_main_output_loss: 0.3598 - val_aux_output_loss: 0.4441\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3337 - main_output_loss: 0.3239 - aux_output_loss: 0.4220 - val_loss: 0.3689 - val_main_output_loss: 0.3609 - val_aux_output_loss: 0.4406\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3331 - main_output_loss: 0.3236 - aux_output_loss: 0.4189 - val_loss: 0.3937 - val_main_output_loss: 0.3851 - val_aux_output_loss: 0.4711\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3324 - main_output_loss: 0.3230 - aux_output_loss: 0.4178 - val_loss: 0.3659 - val_main_output_loss: 0.3577 - val_aux_output_loss: 0.4394\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3322 - main_output_loss: 0.3227 - aux_output_loss: 0.4170 - val_loss: 0.3629 - val_main_output_loss: 0.3549 - val_aux_output_loss: 0.4350\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3307 - main_output_loss: 0.3216 - aux_output_loss: 0.4132 - val_loss: 0.3599 - val_main_output_loss: 0.3516 - val_aux_output_loss: 0.4339\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3301 - main_output_loss: 0.3210 - aux_output_loss: 0.4122 - val_loss: 0.3676 - val_main_output_loss: 0.3596 - val_aux_output_loss: 0.4397\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3292 - main_output_loss: 0.3203 - aux_output_loss: 0.4099 - val_loss: 0.3678 - val_main_output_loss: 0.3601 - val_aux_output_loss: 0.4367\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3280 - main_output_loss: 0.3190 - aux_output_loss: 0.4093 - val_loss: 0.3650 - val_main_output_loss: 0.3582 - val_aux_output_loss: 0.4263\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3268 - main_output_loss: 0.3179 - aux_output_loss: 0.4067 - val_loss: 0.3601 - val_main_output_loss: 0.3527 - val_aux_output_loss: 0.4264\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3281 - main_output_loss: 0.3194 - aux_output_loss: 0.4068 - val_loss: 0.3659 - val_main_output_loss: 0.3585 - val_aux_output_loss: 0.4323\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3264 - main_output_loss: 0.3176 - aux_output_loss: 0.4055 - val_loss: 0.3567 - val_main_output_loss: 0.3491 - val_aux_output_loss: 0.4256\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3249 - main_output_loss: 0.3162 - aux_output_loss: 0.4036 - val_loss: 0.3599 - val_main_output_loss: 0.3526 - val_aux_output_loss: 0.4259\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3254 - main_output_loss: 0.3168 - aux_output_loss: 0.4027 - val_loss: 0.3667 - val_main_output_loss: 0.3599 - val_aux_output_loss: 0.4279\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3243 - main_output_loss: 0.3156 - aux_output_loss: 0.4023 - val_loss: 0.3550 - val_main_output_loss: 0.3475 - val_aux_output_loss: 0.4229\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3218 - main_output_loss: 0.3132 - aux_output_loss: 0.3992 - val_loss: 0.3599 - val_main_output_loss: 0.3527 - val_aux_output_loss: 0.4245\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3216 - main_output_loss: 0.3130 - aux_output_loss: 0.3995 - val_loss: 0.3621 - val_main_output_loss: 0.3559 - val_aux_output_loss: 0.4181\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3232 - main_output_loss: 0.3150 - aux_output_loss: 0.3975 - val_loss: 0.3635 - val_main_output_loss: 0.3564 - val_aux_output_loss: 0.4269\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3224 - main_output_loss: 0.3141 - aux_output_loss: 0.3972 - val_loss: 0.3542 - val_main_output_loss: 0.3466 - val_aux_output_loss: 0.4221\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3217 - main_output_loss: 0.3134 - aux_output_loss: 0.3969 - val_loss: 0.3544 - val_main_output_loss: 0.3472 - val_aux_output_loss: 0.4193\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3211 - main_output_loss: 0.3129 - aux_output_loss: 0.3953 - val_loss: 0.3505 - val_main_output_loss: 0.3436 - val_aux_output_loss: 0.4130\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3209 - main_output_loss: 0.3125 - aux_output_loss: 0.3959 - val_loss: 0.3693 - val_main_output_loss: 0.3621 - val_aux_output_loss: 0.4337\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3195 - main_output_loss: 0.3113 - aux_output_loss: 0.3940 - val_loss: 0.3533 - val_main_output_loss: 0.3466 - val_aux_output_loss: 0.4136\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3194 - main_output_loss: 0.3112 - aux_output_loss: 0.3931 - val_loss: 0.3511 - val_main_output_loss: 0.3436 - val_aux_output_loss: 0.4179\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3200 - main_output_loss: 0.3120 - aux_output_loss: 0.3925 - val_loss: 0.3606 - val_main_output_loss: 0.3534 - val_aux_output_loss: 0.4249\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3183 - main_output_loss: 0.3103 - aux_output_loss: 0.3904 - val_loss: 0.3540 - val_main_output_loss: 0.3472 - val_aux_output_loss: 0.4149\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3181 - main_output_loss: 0.3101 - aux_output_loss: 0.3895 - val_loss: 0.3510 - val_main_output_loss: 0.3442 - val_aux_output_loss: 0.4116\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3178 - main_output_loss: 0.3098 - aux_output_loss: 0.3896 - val_loss: 0.3604 - val_main_output_loss: 0.3541 - val_aux_output_loss: 0.4179\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3154 - main_output_loss: 0.3075 - aux_output_loss: 0.3862 - val_loss: 0.3550 - val_main_output_loss: 0.3485 - val_aux_output_loss: 0.4134\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3180 - main_output_loss: 0.3102 - aux_output_loss: 0.3882 - val_loss: 0.3466 - val_main_output_loss: 0.3403 - val_aux_output_loss: 0.4036\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3156 - main_output_loss: 0.3075 - aux_output_loss: 0.3880 - val_loss: 0.3433 - val_main_output_loss: 0.3369 - val_aux_output_loss: 0.4010\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3172 - main_output_loss: 0.3094 - aux_output_loss: 0.3871 - val_loss: 0.3455 - val_main_output_loss: 0.3390 - val_aux_output_loss: 0.4040\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3157 - main_output_loss: 0.3080 - aux_output_loss: 0.3850 - val_loss: 0.3478 - val_main_output_loss: 0.3411 - val_aux_output_loss: 0.4077\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3149 - main_output_loss: 0.3072 - aux_output_loss: 0.3834 - val_loss: 0.3473 - val_main_output_loss: 0.3410 - val_aux_output_loss: 0.4036\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3154 - main_output_loss: 0.3077 - aux_output_loss: 0.3841 - val_loss: 0.3421 - val_main_output_loss: 0.3355 - val_aux_output_loss: 0.4011\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3147 - main_output_loss: 0.3070 - aux_output_loss: 0.3839 - val_loss: 0.3506 - val_main_output_loss: 0.3442 - val_aux_output_loss: 0.4079\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3145 - main_output_loss: 0.3070 - aux_output_loss: 0.3821 - val_loss: 0.3634 - val_main_output_loss: 0.3575 - val_aux_output_loss: 0.4166\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3136 - main_output_loss: 0.3060 - aux_output_loss: 0.3820 - val_loss: 0.3476 - val_main_output_loss: 0.3409 - val_aux_output_loss: 0.4074\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3158 - main_output_loss: 0.3088 - aux_output_loss: 0.3796 - val_loss: 0.3482 - val_main_output_loss: 0.3422 - val_aux_output_loss: 0.4020\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3143 - main_output_loss: 0.3069 - aux_output_loss: 0.3813 - val_loss: 0.3420 - val_main_output_loss: 0.3359 - val_aux_output_loss: 0.3965\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3126 - main_output_loss: 0.3052 - aux_output_loss: 0.3793 - val_loss: 0.3405 - val_main_output_loss: 0.3343 - val_aux_output_loss: 0.3966\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3128 - main_output_loss: 0.3054 - aux_output_loss: 0.3794 - val_loss: 0.3434 - val_main_output_loss: 0.3374 - val_aux_output_loss: 0.3975\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3125 - main_output_loss: 0.3052 - aux_output_loss: 0.3781 - val_loss: 0.3456 - val_main_output_loss: 0.3396 - val_aux_output_loss: 0.3996\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3114 - main_output_loss: 0.3041 - aux_output_loss: 0.3774 - val_loss: 0.3524 - val_main_output_loss: 0.3465 - val_aux_output_loss: 0.4058\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3122 - main_output_loss: 0.3052 - aux_output_loss: 0.3754 - val_loss: 0.3434 - val_main_output_loss: 0.3373 - val_aux_output_loss: 0.3992\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3122 - main_output_loss: 0.3049 - aux_output_loss: 0.3782 - val_loss: 0.3441 - val_main_output_loss: 0.3381 - val_aux_output_loss: 0.3987\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3128 - main_output_loss: 0.3055 - aux_output_loss: 0.3783 - val_loss: 0.3409 - val_main_output_loss: 0.3352 - val_aux_output_loss: 0.3918\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3099 - main_output_loss: 0.3027 - aux_output_loss: 0.3749 - val_loss: 0.3441 - val_main_output_loss: 0.3384 - val_aux_output_loss: 0.3948\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3096 - main_output_loss: 0.3025 - aux_output_loss: 0.3740 - val_loss: 0.3456 - val_main_output_loss: 0.3398 - val_aux_output_loss: 0.3981\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3098 - main_output_loss: 0.3025 - aux_output_loss: 0.3751 - val_loss: 0.3444 - val_main_output_loss: 0.3387 - val_aux_output_loss: 0.3959\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3120 - main_output_loss: 0.3049 - aux_output_loss: 0.3759 - val_loss: 0.3416 - val_main_output_loss: 0.3360 - val_aux_output_loss: 0.3923\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=100,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]),\n",
    "                   callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-manual",
   "metadata": {},
   "source": [
    "You can set large number of epochs since it will stop automatically after no progress.\n",
    "\n",
    "If you need extra control, you can easily build your custom callback. There is an example of displaying ratio of validation loss and training loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "colored-lingerie",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3053 - main_output_loss: 0.2985 - aux_output_loss: 0.3663 - val_loss: 0.3717 - val_main_output_loss: 0.3654 - val_aux_output_loss: 0.4289\n",
      "\n",
      "val/train: 1.22\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3062 - main_output_loss: 0.2994 - aux_output_loss: 0.3677 - val_loss: 0.3494 - val_main_output_loss: 0.3435 - val_aux_output_loss: 0.4020\n",
      "\n",
      "val/train: 1.14\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3055 - main_output_loss: 0.2986 - aux_output_loss: 0.3668 - val_loss: 0.3439 - val_main_output_loss: 0.3383 - val_aux_output_loss: 0.3939\n",
      "\n",
      "val/train: 1.13\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3053 - main_output_loss: 0.2986 - aux_output_loss: 0.3653 - val_loss: 0.3346 - val_main_output_loss: 0.3290 - val_aux_output_loss: 0.3853\n",
      "\n",
      "val/train: 1.10\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3053 - main_output_loss: 0.2984 - aux_output_loss: 0.3665 - val_loss: 0.3388 - val_main_output_loss: 0.3329 - val_aux_output_loss: 0.3919\n",
      "\n",
      "val/train: 1.11\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3050 - main_output_loss: 0.2982 - aux_output_loss: 0.3654 - val_loss: 0.3510 - val_main_output_loss: 0.3457 - val_aux_output_loss: 0.3992\n",
      "\n",
      "val/train: 1.15\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3053 - main_output_loss: 0.2985 - aux_output_loss: 0.3665 - val_loss: 0.3394 - val_main_output_loss: 0.3343 - val_aux_output_loss: 0.3854\n",
      "\n",
      "val/train: 1.11\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3052 - main_output_loss: 0.2985 - aux_output_loss: 0.3655 - val_loss: 0.3512 - val_main_output_loss: 0.3454 - val_aux_output_loss: 0.4031\n",
      "\n",
      "val/train: 1.15\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3047 - main_output_loss: 0.2981 - aux_output_loss: 0.3646 - val_loss: 0.3420 - val_main_output_loss: 0.3364 - val_aux_output_loss: 0.3924\n",
      "\n",
      "val/train: 1.12\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3050 - main_output_loss: 0.2983 - aux_output_loss: 0.3657 - val_loss: 0.3442 - val_main_output_loss: 0.3385 - val_aux_output_loss: 0.3950\n",
      "\n",
      "val/train: 1.13\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3044 - main_output_loss: 0.2978 - aux_output_loss: 0.3638 - val_loss: 0.3421 - val_main_output_loss: 0.3371 - val_aux_output_loss: 0.3872\n",
      "\n",
      "val/train: 1.12\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3027 - main_output_loss: 0.2961 - aux_output_loss: 0.3626 - val_loss: 0.3379 - val_main_output_loss: 0.3328 - val_aux_output_loss: 0.3841\n",
      "\n",
      "val/train: 1.12\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3045 - main_output_loss: 0.2980 - aux_output_loss: 0.3628 - val_loss: 0.3336 - val_main_output_loss: 0.3282 - val_aux_output_loss: 0.3821\n",
      "\n",
      "val/train: 1.10\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3030 - main_output_loss: 0.2964 - aux_output_loss: 0.3627 - val_loss: 0.3383 - val_main_output_loss: 0.3334 - val_aux_output_loss: 0.3827\n",
      "\n",
      "val/train: 1.12\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3031 - main_output_loss: 0.2967 - aux_output_loss: 0.3600 - val_loss: 0.3331 - val_main_output_loss: 0.3276 - val_aux_output_loss: 0.3825\n",
      "\n",
      "val/train: 1.10\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3021 - main_output_loss: 0.2957 - aux_output_loss: 0.3599 - val_loss: 0.3380 - val_main_output_loss: 0.3330 - val_aux_output_loss: 0.3824\n",
      "\n",
      "val/train: 1.12\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3032 - main_output_loss: 0.2967 - aux_output_loss: 0.3620 - val_loss: 0.3303 - val_main_output_loss: 0.3246 - val_aux_output_loss: 0.3810\n",
      "\n",
      "val/train: 1.09\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3023 - main_output_loss: 0.2956 - aux_output_loss: 0.3618 - val_loss: 0.3353 - val_main_output_loss: 0.3299 - val_aux_output_loss: 0.3838\n",
      "\n",
      "val/train: 1.11\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3018 - main_output_loss: 0.2953 - aux_output_loss: 0.3602 - val_loss: 0.3400 - val_main_output_loss: 0.3349 - val_aux_output_loss: 0.3856\n",
      "\n",
      "val/train: 1.13\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3023 - main_output_loss: 0.2959 - aux_output_loss: 0.3604 - val_loss: 0.3461 - val_main_output_loss: 0.3411 - val_aux_output_loss: 0.3916\n",
      "\n",
      "val/train: 1.14\n"
     ]
    }
   ],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(f\"\\nval/train: {logs['val_loss']/logs['loss']:.2f}\")\n",
    "\n",
    "ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                   validation_data=([X_val_A, X_val_B], [y_val, y_val]),\n",
    "                   callbacks=[ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-plasma",
   "metadata": {},
   "source": [
    ">`on_epoch_end()` is predefined method, there are others which you can use like `on_epoch_begin()`. There are such methods for test and predict also, if you need debugging in those cases.\n",
    "\n",
    "### Using TensorBoard for Visualization\n",
    "\n",
    "TensorBoard is a great interactive visualization tool that you can use to \n",
    "\n",
    "- view the learning curves during multiple runs,\n",
    "- visualize the computation graph,\n",
    "- analyze training statistics,\n",
    "- view images generated by your model,\n",
    "- visualize complex multidimensional data projected down to 3D, and\n",
    "- automatically clustered for you and more!\n",
    "\n",
    "To use it, you need to modify your program including a special binary file called *event files*. Each binary data is called a *summary*. TensorBoard server will monitor the log directory. We have to point TensorBoard to the root directory and configure your program so that it writes in different directory every time it runs. This way your logs won't mixed up.\n",
    "\n",
    "Let's make it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "funny-spending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.1606 - val_loss: 0.7592\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7395 - val_loss: 0.5976\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6818 - val_loss: 0.4523\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4154 - val_loss: 0.4398\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4256\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.4320\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.4173\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3994 - val_loss: 0.4129\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3867 - val_loss: 0.4118\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.4123\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.4050\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4037\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3778 - val_loss: 0.3985\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.4022\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.4026\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.3894\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.3936\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.3879\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.3940\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3834\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3902\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.3855\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.3977\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.3907\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3818\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.3801\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3643 - val_loss: 0.3906\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3890\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3444 - val_loss: 0.3767\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.3830\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                   validation_data=(X_val, y_val),\n",
    "                   callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-stick",
   "metadata": {},
   "source": [
    "It will automatically create needed folders and events as well as profile traces: that tells you much time your model takes on each part, and across all your devices, which is great for locating bottlenecks.\n",
    "\n",
    "Next you need to start the TensorBoard server, you can do this by command line:\n",
    "\n",
    "`$ tensorboard --logdir=./my_logs --port=6006`\n",
    "\n",
    "Once the server is up go to web browser and visit *https://localhost:6006*.\n",
    "\n",
    "There you can see TensorBoard web interface. Click the SCALARS tab to view learning curves.\n",
    "\n",
    "Additionally, TensorFlow offers a lower-level API in the `tf.summary` package. Which creates a `SummaryWriter` using the `create_file_writer()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-fusion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
