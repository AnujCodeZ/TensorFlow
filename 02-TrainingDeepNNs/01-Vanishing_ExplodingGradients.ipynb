{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boolean-assets",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks\n",
    "\n",
    "So far we trained shallow nets, which have few hidden layers. Now, if we have to tackle more complex problems like, object detection or speech recognition. Then we have increase our layers and neurons. But training a DNN is not that easy, there are several problems that can occur:\n",
    "\n",
    "- *Vanishing/Exploding gradients* problem: this is when gradients grow smaller and smaller, or bigger and bigger during back propagation. This will make lower layers hard to train.\n",
    "- You might not have enough data, or label the data is too costly.\n",
    "- Training may be extremely slow.\n",
    "- Model could easily overfit the data having too many parameters.\n",
    "\n",
    "So, let's tackle these problems. Welcome to Deep Learning!\n",
    "\n",
    "# Vanishing/Exploding Gradients Problem\n",
    "\n",
    "In backpropagation gradients often get smaller and smaller when getting to lower layers. Hence, updates of parameters of those layers are negligible, this is called *vanishing gradients*. On other hand sometimes gradients grow bigger and bigger and updates on parameters are too large leading algorithm to diverge. This is called *exploding gradients*, it usually occurs in RNN.\n",
    "\n",
    "The reason of vanishing gradients are activation functions used. Like in sigmoid functions when bigger values occur it tends to 0 or 1, where the gradients is almost zero or very minimum. So backpropagation keeps diluting the gradients.\n",
    "\n",
    "## Glorot and He Initialization\n",
    "\n",
    "Xavier Glorot and Yoshua Bengio proposed a paper to solve this problem by pointing out that we need the signal to flow properly in both directions (forward and back propagation): means the variance of inputs and outputs of a layer should be equal same goes for the gradients.\n",
    "\n",
    "So they propose that we should initialize weights randomly as:\n",
    "$$\n",
    "fan_{avg} = (fan_{in} + fan_{out}) / 2\n",
    "\\\\\n",
    "\\sigma^2 = \\frac1{fan_{avg}} , mean=0: for\\ normal\\ distribution\n",
    "\\\\\n",
    "r = \\sqrt{\\frac3{fan_{avg}}}, (-r, +r): for\\ uniform\\ distribution\n",
    "$$\n",
    "Here, $fan_{in}$ and $fan_{out}$ are number of inputs and neurons of a layer.\n",
    "\n",
    "This is called *Xavier Initialization* or *Glorot Initialization*. If you replace $fan_{avg}$ to $fan_{in}$ you get *LeCun Initialization*, which was proposed back in 1990. These initialization can be used with logistic activation functions.\n",
    "\n",
    "For ReLU (and its variance), there is a *He Initialization*. Where $\\sigma^2=\\frac2{fan_{in}}$.\n",
    "\n",
    "By default, Keras uses Glorot Initialization with uniform distribution. When creating a layer you can change this to He initialization by setting `kernel_initializer='he_uniform'` or `kernel_initializer='he_normal'` like this:\n",
    "\n",
    "```python\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')\n",
    "```\n",
    "\n",
    "If you want to use $fan_{avg}$ in He initialization with uniform distribution, you can use `VarianceScaling` like this:\n",
    "\n",
    "```python\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg',\n",
    "                                                distribution='uniform')\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer=he_avg_init)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-starter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
