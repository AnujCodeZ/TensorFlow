{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elder-consolidation",
   "metadata": {},
   "source": [
    "# Fine-Tuning Neural Network Hyperparameters\n",
    "\n",
    "There are many hyperparameters to tune in neural networks like number of layers, number of units in each layer, activation function in each layer, etc.\n",
    "\n",
    "One way to find a combination is to use Sklearn's `GridSearchCV` or `RandomizedSearchCV` to explore the hyperparameter space.\n",
    "\n",
    "So we need to wrap Keras model to Sklearn model.So let's create a function that build and compile a model, given a set of hyperparameters. And then convert this model using `KerasRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iraqi-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "official-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "norman-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-precipitation",
   "metadata": {},
   "source": [
    "Now we can use it like Sklearn regressor also it works as Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "crazy-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "determined-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "chief-celebrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.9551 - val_loss: 0.7994\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7938 - val_loss: 0.6934\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6655 - val_loss: 0.6336\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6336 - val_loss: 0.5937\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5824 - val_loss: 0.5666\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5831 - val_loss: 0.5708\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5299\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5180 - val_loss: 0.5185\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5104 - val_loss: 0.5069\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4914 - val_loss: 0.4950\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4925 - val_loss: 0.4879\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4834 - val_loss: 0.4820\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4873 - val_loss: 0.4757\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4701 - val_loss: 0.4707\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4698 - val_loss: 0.4653\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4560 - val_loss: 0.4606\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4516 - val_loss: 0.4582\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4500 - val_loss: 0.4567\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4455 - val_loss: 0.4517\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4438 - val_loss: 0.4486\n",
      "162/162 [==============================] - 0s 780us/step - loss: 0.4297\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=20,\n",
    "             validation_data=(X_val, y_val))\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-bride",
   "metadata": {},
   "source": [
    "Note that any extra parameter you pass to `fit()` method will get passed to the underlying Keras model. \n",
    "\n",
    "Now, we want to train it on many variants to see which combination of hyperparameters works best for validation set. We use randomized search because there are many hyperparameters:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    'n_hidden': [0, 1, 2, 3],\n",
    "    'n_neurons': np.arange(1, 100),\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RanomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Note that `RandomizedSearchCV` use K-fold cross-validation, so it does not use `X_val` and `y_val`. Since, Sklearn regression `score()` is opposite to the Keras `evaluate()`.\n",
    "\n",
    "This training will take several hours, depending on the hardware, size of dataset, complexity of model, and values of `n_iter` and `cv`. After this done you can check best hyperparameters by `rnd_search_cv.best_params_`, score by `rnd_search_cv.best_score_`, and model by `rnd_search_cv.best_estimator_.model`.\n",
    "\n",
    "There are many techniques to explore search space much more efficiently than randomly. The main idea is: when a region is turns out to be good, then it should be more explored. This is called \"zooming\".\n",
    "\n",
    "## Number of Hidden Layers\n",
    "\n",
    "Usually you can begin with single hidden layer for many problems. But for complex problems, deep networks have a much higher *parameter efficiency*. As in Deep Networks, lower hidden layers model lower structures (line, orientation), then higher hidden layers combine those to learn complex patterns. Thus adding more layers is beneficial.\n",
    "\n",
    "DNN also helps in generalize to new datasets. Instead of randomly initialize you could use pretrained lower layers of similar tasks. This is called *Transfer Learning*.\n",
    "\n",
    "So you can get good results with two-three hidden layers. For more complex layers you can increase number of layers until model overfit. You will rarely train a large model from scratch: it is more common to use pretrained layers of state-of-the-art models.\n",
    "\n",
    "## Number of  Neurons per Hidden Layer\n",
    "\n",
    "It is common to start with bigger number and decrease gradually over layers. If your model is very deep first increase then decrease the number of neurons. \n",
    "\n",
    "You can start with more layers and neurons than your model need, then perform regularization and early stopping techniques to prevent overfitting: this is called \"stretch pants\" approach.\n",
    "\n",
    "> In general you will get more better performance by increasing number of layers instead of the number of neurons.\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    "One way to find start with small $10^{-5}$ and increase gradually up to 0.1 over some iterations. You'll see loss first decrease and then increasing, choose minimum point and start training with that.\n",
    "\n",
    "After that you can decrease learning rate as epoch increases to converge better, this is called *learning schedule*.\n",
    "\n",
    "## Optimizer\n",
    "\n",
    "Generally we use Adam as default optimizer, or you can choose according to your datasets. Also tuning it's hyperparameters are important. We'll see advanced optimizer on further tutorials.\n",
    "\n",
    "## Batch Size\n",
    "\n",
    "These are set in 2 to the power as memories are assigned in that number. From research papers we can say that you could use large batch size with learning rate warm up (small to large), and if results are not good then use smaller 32-64 batch size instead.\n",
    "\n",
    "> Learning rate is sensitive to other hyperparameters, so if you change any other hyperparameter make sure to tune learning rate also.\n",
    "\n",
    "For more best practices you can check this excellent paper [A disciplined approach to neural network hyper-parameters](https://homl.info/1cycle).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-circumstances",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
