{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hired-slovenia",
   "metadata": {},
   "source": [
    "# Biological to Artificial Neuron\n",
    "\n",
    "## Introduction\n",
    "\n",
    "*Artificial Neural Networks* (ANNs) is a Machine Learning Model inspired by the networks of biological neurons found in our brains.\n",
    "\n",
    "ANNs are the very core of Deep Learning. They are versatile, powerful, and scalable. They are able to tackle complex problems like classifying large number of images (Google Images), speech recognition services (\"OK Google\"), recommending movies (Netflix), etc.\n",
    "\n",
    "We'll use popular Keras API: this is a beautifully designed and simple high-level API for building, training, evaluating, and running neural networks. \n",
    "\n",
    "## History of ANNs\n",
    "\n",
    "ANNs first introduced back in 1943 by Neurophysiologist Warren McCulloch and the mathematician Walter Pitts in their paper \"A Logical Calculus of Ideas Immanent in Nervous Activity\".\n",
    "\n",
    "In 1960, idea of making intelligent machines seems impossible (for quite a while), ANNs entered a long winter.\n",
    "\n",
    "In early 1980, new architectures were build, interest rises in *connectionism* (the study of neural networks), progress was slow. And around 1990, more powerful Machine Learning Algorithms was built like SVMs. So they put on hold again.\n",
    "\n",
    "But now we see that ANNs are rising and this time they keep rising. Here are few reasons why:\n",
    "\n",
    "- Huge quantity of data available.\n",
    "- Increase in Computing powers, and thanks to Gaming industry to give us GPU.\n",
    "- Training algorithms have been improved.\n",
    "- More funding in building amazing products and research.\n",
    "\n",
    "## Logical Computations with Neurons\n",
    "\n",
    "McCulloch and Pitts proposed a very simple model of the biological neuron, which later became known as *artificial neuron*: it has one or more binary inputs and one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active. \n",
    "\n",
    "You can compute complex logical expressions by changing the inputs or combining them.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "The *Perceptron* is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on slightly different artificial neuron called a *threshold logic unit* (TLU), or sometimes a *linear threshold unit* (LTU).\n",
    "\n",
    "The inputs and output are numbers, and each connection holds some value called *weights*. The TLU computes a weighted sum of its inputs:\n",
    "$$\n",
    "z = w_1x_1 + w_2x_2 + ... + w_nx_n = X^TW\n",
    "$$\n",
    " Then applies a *step function* and results an output:\n",
    "$$\n",
    "h_w(X) = step(z), where\\ z = X^TW\n",
    "$$\n",
    "The most common step function used in Perceptron is the *Heaviside step function*:\n",
    "$$\n",
    "heaviside(z) = \\{{0, if\\ z < 0 \\\\ 1, if z >= 0}\n",
    "$$\n",
    "Sometimes a sign function:\n",
    "$$\n",
    "sign(z) = \\{{-1, if\\ z < 0 \\\\ 0, if\\ z = 0 \\\\ 1, if\\ z > 0}\n",
    "$$\n",
    "A single TLU can be use for simple linear binary classification. Training a TLU means finding the right values of weights.\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs.\n",
    "\n",
    "When all the neurons in a layer are connected to every neuron in previous layer, the layer is called *fully connected layer* or *dense layer*.\n",
    "\n",
    "The inputs of Perceptron are simply passthrough neurons called *input neurons*: they output whatever they are fed. All the input neurons form the *input layer*. Moreover, an extra bias feature is added: represented as a *bias neuron*, always outputs 1. \n",
    "\n",
    "Computing the outputs of a fully connected layer:\n",
    "$$\n",
    "h_{W, b} = \\phi(XW + b)\n",
    "$$\n",
    " In this equation,\n",
    "\n",
    "- **X**: inputs matrix\n",
    "- **W**: weights matrix\n",
    "- **b**: bias\n",
    "- $\\phi$: it represents *activation function*\n",
    "\n",
    "The training of Perceptron was largely inspired by *Hebb's rule*: which is summarized as \"cells that fire together, wire together\"; that is connection weights between two neurons tends to increase when they activates together.\n",
    "\n",
    "The Perceptron trains with slight different variant that takes into account of error made by the network. It reinforces the connection when it makes an error. The rule is:\n",
    "$$\n",
    "w_{i,j}^{(next\\ step)} = w_{i, j} + \\eta(y_j - \\hat y_j)x_i\n",
    "$$\n",
    "In this equation:\n",
    "\n",
    "- $w_{i,j}$ is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron.\n",
    "- $x_i$ is the $i^{th}$ input value of current training instance.\n",
    "- $y_j$ is the target output of the $j^{th}$ output neuron.\n",
    "- $\\hat y_j$ is the output of the $j^{th}$ output neuron.\n",
    "- $\\eta$ is the learning rate.\n",
    "\n",
    "The decision boundary of the TLUs are linear, so Perceptron are incapable of learning complex patterns. Overall this algorithm is called *Perceptron convergence theorem*.\n",
    "\n",
    "You can use Sklearn's `Perceptron` to do this. However, it is same as the SGD classifier of Sklearn with hyperparameters `loss=\"perceptron\", learning rate=\"constant\"`.\n",
    "\n",
    "Perceptrons do not output a class probability as Logistic regression. Rather, they output a hard threshold.\n",
    "\n",
    "In 1969 monograph *Perceptrons*, there are weakness of Perceptrons like it cannot solve XOR problem. \n",
    "\n",
    "It turns out limitations of Perceptron can be replaced by stacking multiple Perceptrons. The resulting ANN is called *Multilayer Perceptron* (MLP). An MLP can solve XOR problem easily.\n",
    "\n",
    "\n",
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "likely-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "differential-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length and width\n",
    "y = (iris.target == 0).astype(np.int) # Iris setosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "realistic-explanation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specified-royal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-defensive",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
