{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-vinyl",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting through Regularization\n",
    "\n",
    "Deep neural networks have many parameters to fit that give them incredible freedom and flexibility. But, it tends to overfit dataset because of this, so we need regularization techniques.\n",
    "\n",
    "We already studied early stopping and Batch Normalization (which is not for overfitting).\n",
    "\n",
    "## $l_1$ and $l_2$ Regularization\n",
    "\n",
    "You can use $l_2$ regularization to constrain connection weights or use $l_1$ if you want sparse model.\n",
    "\n",
    "```python\n",
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                          kernel_initialization='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "```\n",
    "\n",
    "This computes regularization loss at every steps and then add it to the final loss.\n",
    "\n",
    "`keras.regularizers.l1()` for $l1$ regularization. `keras.regularizers.l1_l2()` if you want to use both.\n",
    "\n",
    "If you want to use same activation, regularization, and initialization to every layer, then you can do it with Python's `functools.partial()` function:\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense, \n",
    "                           activation='elu',\n",
    "                           kernel_initialization='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax',\n",
    "                    kernel_initializer='glorot_uniform')\n",
    "])\n",
    "```\n",
    "\n",
    "## Dropout\n",
    "\n",
    "*Dropout* is one of the most popular regularization and default in recent deep neural networks.\n",
    "\n",
    "It is a fairly simple algorithm: at every training step, every neurons (excluding output neurons) has a probability $p$ of being temporarily \"dropped out\", meaning it will be ignored during this training step, but it may be active during the next step. $p$ is a hyperparameter called *dropout rate*, usually between 10% to 50%: closer to 40-50% in CNNs, and 20-30% in RNNs. After training it will be removed.\n",
    "\n",
    "It works because neurons now not depend on the neighbor neurons, it will learn on their own and it will focus on every input neurons makes it less sensitive to slight changes in input neurons. Thus performing a great generalization.\n",
    "\n",
    "Another power of dropout is that a new network is generated at each step. So if you train for 10,000 epochs, you have 10,000 different networks. These networks are not independent as they share many of their weights. This results in ensemble of many neural networks.\n",
    "\n",
    "One technical detail is when applying dropout, remaining neurons have extra input weights (in average) as it should be, which leads to total much more weights than we want. To solve this multiply each input connection weights by $(1 - p)$ *keep probability* after training or alternatively divide each neuron's output by keep probability during training.\n",
    "\n",
    "To implement dropout in Keras, you can use `keras.layers.Dropout`:\n",
    "\n",
    "```python\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(100, activation='elu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "> Since dropout is active only during training, comparing training loss with validation loss can be misleading. So make sure to evaluate training loss after training without dropout.\n",
    "\n",
    "Dropout does tends to significantly slow down convergence, but it results a much better model.\n",
    "\n",
    "> If you want to regularize a self-normalizing network based on SELU activation function, you can use *alpha dropout*: this preserves mean and variance of its inputs.\n",
    "\n",
    "## Monte Carlo Dropout\n",
    "\n",
    "*MC Dropout* is a powerful technique which can boost the performance of any trained dropout. Let's look at implementation:\n",
    "\n",
    "```python\n",
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                    for sample in range(100)]) # [100, 10000, 10]\n",
    "y_probas = y_probas.mean(axis=0) # [10000, 10]\n",
    "```\n",
    "\n",
    "We set `training=True` for dropout to be active. Because of this all the predictions are different, so we stack up 100 different predictions and average them all. Then we get single prediction. \n",
    "\n",
    "When model predicts single prediction with dropout off it shows best probability. But by MC Dropout technique result is more general and sensitive. So when you want to make a sensitive model (like medical or financial) MC Dropout would be helpful.\n",
    "\n",
    "Moreover, model's accuracy also increased by MC Dropout without retraining it.\n",
    "\n",
    "> The number of MC samples (100 in this example) is a hyperparameter. Higher would be more accurate the predictions and uncertainty will be. If you double it prediction time doubles. So its a trade-off between latency and accuracy.\n",
    "\n",
    "If your model consists other special layers like `BatchNormalization` layers. Then you should not force `training=True`, instead use `MCDropout` layer given below:\n",
    "\n",
    "```python\n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "```\n",
    "\n",
    "## Max-Norm Regularization\n",
    "\n",
    "*Max-norm regularization* constrains the weights $w$ of the incoming connections such that $||w||_2 < r$, where $r$ is the max-norm hyperparameter. \n",
    "\n",
    "It is typically implemented by computing $||w||_2$ after each training step and rescaling $w$ if needed by $w \\gets w\\frac{r}{||w||_2}$.\n",
    "\n",
    "Reducing $r$ increases regularization.\n",
    "\n",
    "```python\n",
    "keras.layers.Dense(100, activation='elu',\n",
    "                  kernel_constraint=keras.constraints.max_norm(1.))\n",
    "```\n",
    "\n",
    "You can also constrain bias term by setting `bias_constraint`.\n",
    "\n",
    "The`max_norm()` has an `axis` argument defaults to 0. But when using convolutional layers make sure what axis you want to use (usually `axis=[0, 1, 2]`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-antique",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
